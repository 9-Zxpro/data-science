<div class="carousel-track-container" aria-live="polite" style="height: 684px; width: 556px;"><ul class="carousel-track" style="width: 165228px; transform: translateX(-1176px);">
              
              
            <li class="carousel-slide" data-ssplayer-slide-index="0" style="width: 556px; height: 684px;" tabindex="-1" aria-hidden="true"><img class="carousel-lazy-element carousel-slide-is-portrait carousel-element-loaded carousel-element-load-success" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/1/1739580232671?e=1743033600&amp;v=beta&amp;t=NxC1sOhKb28G9bPSeOmjEMJyQjtLAZ6nUcmS2SVuqB4" alt="AIAI Intensive Intensive PythonPython Applications Applications Minal Pandey" src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/1/1739580232671?e=1743033600&amp;v=beta&amp;t=NxC1sOhKb28G9bPSeOmjEMJyQjtLAZ6nUcmS2SVuqB4"></li><li class="carousel-slide" data-ssplayer-slide-index="1" style="width: 556px; height: 684px;" tabindex="-1" aria-hidden="true"><img class="carousel-lazy-element carousel-slide-is-portrait carousel-element-loaded carousel-element-load-success" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/2/1739580232671?e=1743033600&amp;v=beta&amp;t=KglOIoEoWEjNvFrLlm5TElhREIckVPucjMkc19Xlyt8" alt="1 Getting Started with Generative AI There are a plethora of options for building generative AI (GenAI) applications. The landscape is, quite frankly, overwhelming to navigate, and many of the tools that satisfy one criterion may fall short in another. GenAI applications evolve so quickly that within weeks of this book being published, some of the new AI companies might no longer exist. Therefore, this chapter focuses on long-lived, high- level concepts related to technologies that are used to create GenAI applications. You will learn ways from which your next web development project might benefit. This chapter will examine not just what these ways are but how they work, which will give you a broaderunderstanding and perspective of GenAI. This should help you decide when to use GenAI and how, as well as make the applications you create generally more accurate. By the end of this chapter, you will have a good understanding of the benefits that individual AI/ML stack components bring to a development project, how they relate to each other, and why GenAI technologies are a revolution in software—both in terms of the data handled and desired functionalities." src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/2/1739580232671?e=1743033600&amp;v=beta&amp;t=KglOIoEoWEjNvFrLlm5TElhREIckVPucjMkc19Xlyt8"></li><li class="carousel-slide carousel-slide-active" data-ssplayer-slide-index="2" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait carousel-element-loaded carousel-element-load-success" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/3/1739580232671?e=1743033600&amp;v=beta&amp;t=nrcBMBRZcGI9Ahav_E8fcINpn5Z8NXW-YSxZXURrQlo" alt="2 Getting Started with Generative AI • VS Code or an IDE of your choice • An OpenAI API key This chapter gives an introduction to GenAI and provides a quick overview of the following topics: • Definitions for common terminology • • A GenAI stack of choice Python and GenAI • The OpenAI API • An introduction to MongoDB Vector Search • Important features of GenAI • Why use GenAI? • The ethics and risks of GenAI This book has sample code for a basic Python application. To recreate it, it is recommended that you have the following: • The latest version of Python • A local development environment on your device for your application server • A MongoDB Atlas cloud account to host your database. You can register for one at https:// For the true beginner, let’s start with defining some key terms: AI, ML, and GenAI. You will come across these terms repeatedly in this book, so it helps to have a strong conceptual foundation of these terms: • Artificial intelligence (AI) refers to the ability of machines to perform tasks that would normally require human intelligence. This includes tasks such as perception, reasoning, learning, and decision making. The journey of AI has evolved significantly from early speculative ideas to the sophisticated technologies of today. Figure 1.1 shows a timeline of the development of AI. Technical requirements Defining the terminology www.mongodb.com/cloud/atlas/register" src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/3/1739580232671?e=1743033600&amp;v=beta&amp;t=nrcBMBRZcGI9Ahav_E8fcINpn5Z8NXW-YSxZXURrQlo"></li><li class="carousel-slide" data-ssplayer-slide-index="3" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait carousel-element-loaded carousel-element-load-success" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/4/1739580232671?e=1743033600&amp;v=beta&amp;t=mcp9G0KhFwyjI3DOBbzAEdaallIhSfgXpGsghY2xO7Q" alt="Figure 1.1: A timeline of AI The generative AI stack 3 A stack combines tools, libraries, software, and solutions to create a unified and integrated approach. The GenAI stack includes programming languages, LLM providers, frameworks, databases, and deployment solutions. Though the GenAI stack is relatively new, it already has many variations and options for engineers to choose from. Let’s discuss what you need to build a functional GenAI application. The bare minimum requirements are the following, as also shown in Figure 1.2: • An operating system: Usually, this is Unix/Linux based. • A storage layer: An SQL or NoSQL database. This book uses MongoDB. • A vector database capable of storing embeddings: This book uses MongoDB, which stores its embeddings within your data or content, rather than in a separate database. • A web server: Apache and Nginx are quite popular. • Machine learning (ML) is a subset of AI that involves the use of algorithms to automatically learn from data and improve over time. Essentially, it’s a way for machines to learn and adapt without being explicitly programmed. Most often used in fields that require advanced analysis of thousands of data points, ML is most useful in medical diagnostics, market analysis, and military intelligence. Effectively, ML identifies hidden or complex patterns in data that would be impossible for a human to see and then can make suggestions for the next steps or actions. • Generative AI (GenAI) is the ability to create text, images, audio, video, and other content in response to a user prompt. It powers chatbots, virtual assistants, language translators, and other similar services. These systems use algorithms trained on vast amounts of data, such as text and images from the internet, to learn patterns and relationships. This enables them to generate new content that is similar but not identical to the underlying training data. For instance, large language models (LLMs) use training data to learn patterns in written language. GenAI can then use these models to emulate a human writing style. The generative AI stack Arti&nbsp;cial Intelligence Timeline Computational model for neural networks architecture Turing test to de&nbsp; ne machine intelligence The term ar ti&nbsp;cial intelligence coined The &nbsp;rst computer— perceptron, which learned from trial and error—was developed Expert systems that could emulate the decision- making abilites of a human brain AI becomes a wide-spread industry with applications in healthcare, simulators, and mission management aids IBM develops the Deep Blue chess playing computer, which can defeat an expert human player Personal assistants use speech recognition to interact with humans Big data analysis leads to more complex systems, generative AI, and LLMs such as ChatGPT 1950 1950 1950 1950 1950 1950 1950 1950 1950" src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/4/1739580232671?e=1743033600&amp;v=beta&amp;t=mcp9G0KhFwyjI3DOBbzAEdaallIhSfgXpGsghY2xO7Q"></li><li class="carousel-slide" data-ssplayer-slide-index="4" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait carousel-element-loaded carousel-element-load-success" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/5/1739580232671?e=1743033600&amp;v=beta&amp;t=yJ4PEnC_X_Q2_fAtp2OymNSXBY315spTfDsoJmg0pko" alt="4 Getting Started with Generative AI Figure 1.2: A basic GenAI stack If you want to learn more about the AI stack, you can find detailed information at com/resources/basics/ai-stack. • A development environment: This could be Node.js/JavaScript, .NET, Java, or Python. This book uses Python throughout the examples with a bit of JavaScript where needed. Python was conceived in the late 1980s by Guido van Rossum and officially released in 1991. Over the decades, Python has evolved into a versatile language, beloved by developers for its clean syntax and robust functionality. It has a clean syntax that is easy to understand, making it an ideal choice for beginner developers. Although it is not entirely clear why, fairly early on, the Python ecosystem began introducing more libraries and frameworks that were tailored to ML and data science. Libraries and frameworks such as TensorFlow, Keras, PyTorch, and scikit-learn provided powerful tools for developers in these fields. Analysts who were less technical were still able to get started with Python with relative ease. Due toits interoperability, Python seamlessly integrated with other programming languages and technologies, making it easier to integrate with data pipelines and web applications. www.mongodb. Python and GenAI Storage layer Vector database Operating system Web server Developmen t environment" src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/5/1739580232671?e=1743033600&amp;v=beta&amp;t=yJ4PEnC_X_Q2_fAtp2OymNSXBY315spTfDsoJmg0pko"></li><li class="carousel-slide" data-ssplayer-slide-index="5" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait carousel-element-loaded carousel-element-load-success" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/6/1739580232671?e=1743033600&amp;v=beta&amp;t=uqDlqrnbt4DdT_ckA4FnsPlXO9fgyfI8K52hMMAIsD8" alt="The generative AI stack 5 GenAI, with its demands for high computational power and sophisticated algorithms, finds a perfect partner in Python. Here are some examples that readily come to mind: • Libraries such as Pandas and NumPy allow efficient manipulation and analysis of large datasets, a fundamental step in training generative models • Frameworks such as TensorFlow and PyTorch offer pre-built components to design and train complex neural networks • Tools such as Matplotlib and Seaborn enable detailed visualization of data and model outputs, aiding in understanding and refining AI models • Frameworks such as Flask and FastAPI make deploying your GenAI models as scalable web services straightforward Python has a rich ecosystem that is easy to use and allows you to quickly get started, making it an ideal programming language for GenAI projects. Now, let’s talk more about the other pieces of technology you’ll be using throughout the rest of the book. The first, and most important, tool of this book is the OpenAI API. In the following chapters, you’ll learn more about each component of the GenAI stack—and the most critical to be familiar with is OpenAI. While we’ll cover other LLM providers, the one used in our examples and code repository will be OpenAI. The OpenAI API, launched in mid-2020, provides developers with access to their powerful models, allowing integration of advanced NLP capabilities into applications. Through this API, developers gain access to some of the most advanced AI models in existence, such as GPT-4. These models are trained on vast datasets and possess unparalleled capabilities in natural language understanding and response generation. Moreover, OpenAI’s infrastructure is built to scale. As your project grows and demands more computational power, OpenAI ensures that you can scale effortlessly without worrying about the underlying hardware or system architecture. OpenAI’s models excel at NLP tasks, including text generation, summarization, translation, and sentiment analysis. This can be invaluable for creating content, chatbots, virtual assistants, and more. Much of the data from the internet and internal conversations and documentation is unstructured. OpenAI, as a company, has used that data to train an LLM, and then offered that LLM as a service, making it possible for you to create interactive GenAI applications without hosting or training your own LLM. You’ll learn more about LLMs in Chapter 3, Large Language Models. OpenAI API" src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/6/1739580232671?e=1743033600&amp;v=beta&amp;t=uqDlqrnbt4DdT_ckA4FnsPlXO9fgyfI8K52hMMAIsD8"></li><li class="carousel-slide" data-ssplayer-slide-index="6" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait carousel-element-loaded carousel-element-load-success" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/7/1739580232671?e=1743033600&amp;v=beta&amp;t=OJGAa1ocQw__-rb9kX8Mnyw-PQUivUca2w61QDqSzSY" alt="6 Getting Started with Generative AI Figure 1.3: Example of a vector MongoDB with Vector Search Much has been said about how MongoDB serves the use case of unstructured data but that the world’s data is fundamentally relational. It can be argued that no data is meaningful until humans deem it so, and that the relationships and structure of that data are determined by humans as well. For example, several years ago, a researcher at a leading space exploration company made this memorable comment in a meeting: “We scraped text content from websites and PDF documents primarily, and we realized it didn’t really make sense to try and cram that data into a table.” MongoDB thrives with the messy, unstructured content that characterizes the real world—.txt files, Markdown, PDFs, HTML, and so on. MongoDB is flexible enough to have the structure that engineers deem is best suited for purpose, and because of that flexibility, it is a great fit for GenAI use cases. For that reason, it is much easier to use a document database for GenAI than it is to use a SQL database. Another reason to use MongoDB is for its vector search capabilities. Vector search ensures that when you store a phrase in MongoDB, it converts that data into an array. This is called a vector. Vectors are numerical representations of data and their context, as shown in Figure 1.3. The number of these dimensions is referred to as an embedding, and the more of them you have, the better off you are. After you’ve created embeddings for a piece of data, a mathematical process will identify which vectors are closest or nearest to each other, and you can then infer that the data is related. This allows you to return related words instead of only exact matches. For instance, if you are looking for pets, you could find cats, dogs, parakeets, and hamsters—even though those terms are not the exact word pets. Vectors are what allow you to receive results that are related in meaning or context or are alike, without being an exact match. MongoDB stores your data embeddings alongside the data itself. Storing the embeddings together makes the consequent queries faster. It is easiest to visualize vector search via an example with explanations of how it works along the way. You will learn more about vector search in Chapter 8, Implementing Vector Search in AI Applications. “quick brown fox” = [0.743, 0.720, -0.325, 0.195, 0.835, -0.945, ...] Vector" src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/7/1739580232671?e=1743033600&amp;v=beta&amp;t=OJGAa1ocQw__-rb9kX8Mnyw-PQUivUca2w61QDqSzSY"></li><li class="carousel-slide" data-ssplayer-slide-index="7" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/8/1739580232671?e=1743033600&amp;v=beta&amp;t=ANYNndFPIz2mOecT-YO03_nuOayYdBZ5fsTM-y5cd3s" alt="Important features of generative AI 7 Important features of generative AI When asked to list the most important capability of GenAI applications, ChatGPT, which is arguably the most popular GenAI application in existence, said the following: That response took 1.5 seconds to generate, and most people would agree with it. GenAI applications can create content for you and your users with lightning speed. Whether it’s text, video, images, artwork, or even Java code, GenAI is able to easily draft foundational content that can then be edited by professionals. But there are other key features of GenAI applications that merit calling out as well: • Language translation: With remarkable proficiency, GenAI can translate languages in real time, preserving context and nuance, and facilitating seamless communication across linguistic barriers. • Personalization: In the realm of marketing and customer service, GenAI can tailor experiences and content to individual users. When given proper context, it can analyze preferences and behaviors to deliver personalized recommendations, emails, and customer interactions. • Simulation and modeling: In scientific research and engineering, GenAI can simulate complex systems and phenomena. It aids in predicting molecular behaviors, climate patterns, and even economic trends by generating realistic models based on vast datasets. • Data augmentation: For ML, GenAI can produce synthetic data to augment training sets. This is invaluable in scenarios where real data is scarce or biased, allowing for the creation of diverse and balanced datasets to improve model performance. This is incredibly useful for testing purposes, particularly in software testing. And perhaps most importantly, it can accept prompting in natural language (such as in English) to do these tasks. This makes performing tasks you previously found difficult incredibly easy. You may use GenAI to accomplish multiple and varied tasks in a day, such as reviewing a pull request, guiding you through some tasks for Golang, and generating illustrations for the interior artwork of a book. Content Creation: Generative AI can craft text, images, music, and even videos. It can pen articles, generate realistic images from textual descriptions, compose music, and create video content, opening endless possibilities for creative industries."></li><li class="carousel-slide" data-ssplayer-slide-index="8" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/9/1739580232671?e=1743033600&amp;v=beta&amp;t=4wS26HVvdE68lCtiRZ6jT-SJ7RzL38019TlHrAj4qUY" alt="8 Getting Started with Generative AI Why use generative AI? Each of the preceding abilities is compelling and important, and when used correctly and in combination, revolutionary. Put simply, there is no industry where GenAI cannot play a role. By rapidly aggregating and summarizing a wide range of content and simplifying searching, GenAI improves the user experience of finding ideas and building knowledge. It can help gather new information, summarize it, and recraft it into content. It can help speed up or even automate administrative tasks, and exponentially increase output. But beyond all of that, the experience of using GenAI is an order of magnitude better than what is available today. Consider, for example, a customer service bot. Many of you will be familiar with thisflow: The ethics and risks of GenAI Despite those benefits, there are risks and concerns about the use of AI. In some fields, the outcry against AI is substantial and has merit. Art generated by AI, for example, flooded the internet’s marketplaces, displacing artists and illustrators who make their living off their craft. There are questions about whether using AI to write a book gives a person the right to call themselves an author. There are no clear-cut answers here; from our own experience, the authors of this book believe that GenAI accelerates, rather than replaces, the existing paradigms of work done today. But that may not always remain true. As AI improves, it may be more likely to replace the humans who are using it. The risks of GenAI are considerable, and some of them are not well understood. Even the ones that are well understood, such as hallucinations, are difficult to identify for users, and harder still to combat. You can read more about the challenges of GenAI in Chapter 11, Common Failures of Generative AI, along with recommendations on how to mitigate them in Chapter 12, Correcting and Optimizing Your Generative AI Application. 1. The customer first encounters a long menu of options: . When the customer has a question that does not neatly fit into any category, they may press anyway. 2. Upon pressing 4, they are then routed to a support page that does not have the answer they seek. They click a button that says, No, this did not answer my question. They search the knowledge base themselves, perhaps never finding the answer and reaching out via phone. 3. Imagine being able to type what you wanted and the bot responding in a natural way—not routing you to a page but just giving you the answer. Imagine even further that the user can then chat with the bot to say they want to modify the address on their order, and the bot is able to do that from within the chat window, having a multi-step dialogue with the user to confirm and record their new information. It is a wholly new, more pleasing experience for the customer! If you want to talk to sales or support, press 1. For billing, press 2. For administration, press 3. For orders, press 4. 4"></li><li class="carousel-slide" data-ssplayer-slide-index="9" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/10/1739580232671?e=1743033600&amp;v=beta&amp;t=1wiWENIttGBeHPk4qv65V16J4wd1ZAqukzsZL3PFfYE" alt="Summary 9 Summary This chapter laid the background for the GenAI application, from describing the role of each component to their strengths. You learned some key definitions and were introduced to the basics of the AI stack. By now, you also know why Python is a great choice for building GenAI applications and why you will be using the OpenAI API and MongoDB with Vector Search to build your GenAI application. Finally, you also saw some significant use cases for GenAI and learned why you should use GenAI in the first place, while also remaining mindful of the ethics and risks of using it. Since you’re reading this, I’ll assume that the case was compelling—that you’re still interested and ready to explore. In the next chapter, you will get a fast-paced, concise, and actionable overview of the building blocks of GenAI applications in more detail and learn how to get started."></li><li class="carousel-slide" data-ssplayer-slide-index="10" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/11/1739580232671?e=1743033600&amp;v=beta&amp;t=VcgGH9-cnQjjt-Zxt2aargcjwU0dfsnznlMMjs4lQwU" alt="11"></li><li class="carousel-slide" data-ssplayer-slide-index="11" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/12/1739580232671?e=1743033600&amp;v=beta&amp;t=CjfHKgNn5mKaIutThkVjRoBv3If0sX_iRVnD69ndyLU" alt="2 Building Blocks of Intelligent Applications In the rapidly evolving landscape of software development, a new class of applications is emerging: intelligent applications. Intelligent applications are a superset of traditional full stack applications. These applications use artificial intelligence (AI) to deliver highly personalized, context-aware experiences that go beyond the capabilities of traditional software. Intelligent applications understand complex, unstructured data and use this understanding to make decisions and create natural, adaptive interactions. The goal of this chapter is to provide you with an overview of the logical and technical building blocks of intelligent applications. The chapter explores how intelligent applications extend the capability of traditional full-stack applications, the core structures that define them, and how these components function to create dynamic, context-aware experiences. By the end of this chapter, you willunderstand how these components fit together to form an intelligent application. This chapter covers the following topics: • The building blocks of intelligent applications • LLMs as reasoning engines for intelligent applications • Vector embedding models and vector databases as semantic long-term memory • Model hosting infrastructure"></li><li class="carousel-slide" data-ssplayer-slide-index="12" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/13/1739580232671?e=1743033600&amp;v=beta&amp;t=_1Azx62d-DUUtF6qq1Na8Bp3l0-DWf7nj8BtPXkwL9I" alt="12 Building Blocks of Intelligent Applications Technical requirements This chapter is theoretical. It covers the logical components of intelligent applications and how they fit together. This chapter assumes fundamental knowledge of traditional full stack application development components, such as servers, clients, databases, and APIs. Defining intelligent applications Traditional applications typically consist of a client-side user interface, a server-side backend, and a database for data storage and retrieval. They perform tasks following a strict set of instructions. Intelligent applications require a client, server, and database as well, but they augment the traditional stack with AI components. Intelligent applications stand out by understanding complex, unstructured data to enable natural, adaptive interactions and decision-making. Intelligent applications can engage in open-ended interactions, generate novel content, and make autonomous decisions. Examples of intelligent applications include the following: • Chatbots that provide natural language responses based on external data using retrieval- augmented generation (RAG). For example, Perplexity.ai (https://www.perplexity.ai/) is an AI-powered search engine and chatbot that provides users with AI-generated answers to their queries based on sources retrieved from the web. • Content generators that let you use natural language prompts to create media such as images, video, and audio. There are a variety of intelligent content generators focusing on different media types, such as Suno (https://suno.com/) for text-to-song, Midjourney (https://www.midjourney.com/home) for text-to-image, and Runway (https://runwayml.com/) for text-to-video. • Recommendation systems that use customer data to provide personalized suggestions based on their preferences and history. These suggestions can be augmented with natural language to further personalize the customer experience. An example of this is Spotify’s AI DJ (https://support.spotify.com/us/article/dj/), which creates a personalized radio station, including LLM-generated DJ interludes, based on your listening history. These examples are a few early glances at the new categories of intelligent applications that developers have only started to build. In the next section, you will learn more about the core components of intelligent applications."></li><li class="carousel-slide" data-ssplayer-slide-index="13" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/14/1739580232671?e=1743033600&amp;v=beta&amp;t=JWzGM8mfKAvH6R6cw8XtmTyN6yfNnhbLZ9u-b78hOnE" alt="LLMs – reasoning engines for intelligent apps 13 The building blocks of intelligent applications At the heart of intelligent applications are two key building blocks: LLMs are the key technology of intelligent applications, unlocking whole new classes of AI-powered systems. These models are trained on vast amounts of text data to understand language, generate human-like text, answer questions, and engage in dialogue. LLMs undergo continuous improvement with the release of new models. featuring billions or trillions of parameters and enhanced reasoning, memory, and multi-modal capabilities. • The reasoning engine: The reasoning engine is the brain of an intelligent application, responsible for understanding user input, generating appropriate responses, and making decisions based on available information. The reasoning engine is typically powered by large language models (LLMs)—AI models that perform text completion. LLMs can understand user intent, generate human-like responses, and perform complex cognitive tasks. • Semantic memory: Semantic memory refers to the application’s ability to store and retrieve information in a way that preserves its meaning and relationships, enabling the reasoning engine to access relevant context as needed. Semantic memory consists of two core components: &nbsp; AI vector embedding model: AI vector embedding models represent the semantic meaning of unstructured data, such as text or images, in large arrays of numbers. &nbsp; Vector database: Vector databases efficiently store and retrieve vectors to support semantic search and context retrieval. The reasoning engine can retrieve and store relevant information from the semantic memory, using unstructured data to inform its outputs. The LLMs and embedding models that power intelligent applications have different hardware requirements than traditional applications, especially at scale. Intelligent applications require specialized model hosting infrastructure that can handle the unique hardware and scalability requirements of AI workloads. Intelligent applications also incorporate continuous learning, safety monitoring, and human feedback to ensure quality and integrity. LLMs are the vital organ for intelligent applications. The next section will provide a deeper understanding of the role of LLMs in intelligent applications. LLMs – reasoning engines for intelligent apps"></li><li class="carousel-slide" data-ssplayer-slide-index="14" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/15/1739580232671?e=1743033600&amp;v=beta&amp;t=9SsoWon-k9nSrHrEffh0fFdJwHSPW-3WMib5uJX2DRo" alt="14 Building Blocks of Intelligent Applications Diverse capabilities of LLMs While fundamentally just language models, LLMs have shown surprising emergent capabilities (https://arxiv.org/pdf/2307.06435). As of writing in spring 2024, state-of-the-art language models are capable of performing tasks of the following categories: Use cases for LLM reasoning engines LLMs have emerged as a powerful general-purpose technology for AI systems, analogous to the central processing unit (CPU) in traditional computing. Much like CPUs, LLMs serve as general- purpose computational engines that can be programmed for many tasks and play a similar role in language-based reasoning and generation. The general-purpose nature of LLMs lets developers use their capabilities for a wide range of reasoning tasks. A crop of techniques to leverage the diverse abilities of LLMs have emerged, such as: • Text generation and completion: Given a prompt, LLMs can generate coherent continuations, making them useful for tasks such as content creation, text summarization, and code completion. • Prompt engineering: Using carefully crafted prompts, developers can steer LLMs to perform a wide range of language tasks. A key advantage of prompt engineering is its iterative nature. Since prompts are fundamentally just text, it’s easy to rapidly experiment with different prompts and see the results. Advanced prompt engineering techniques, such as chain-of-thought prompting (which encourages the model to break down its reasoning into a series of steps) and multi-shot prompting (which provides the model with example input/output pairs), can further enhance the quality and reliability of LLM-generated text. • Fine-tuning: Fine-tuning involves starting with a pre-trained general-purpose model and further training it on a smaller dataset relevant to the target task. This can yield better results than prompt engineering alone, but it comes with certain caveats, such as being more expensive and time-consuming. You should only fine-tune after exhausting what you can achieve through prompt engineering. • Retrieval augmentation: Retrieval augmentation interfaces LLMs with external knowledge, allowing them to draw on up-to-date, domain-specific information. In this approach, relevant information is retrieved from a knowledge base and injected into the prompt, enabling the LLM to generate contextually relevant outputs. Retrieval augmentation mitigates the limitations of the static pre-training of LLMs, keeping their knowledge updated and reducing the likelihood of the model hallucinating incorrect information. With these techniques, you can use LLMs for a diverse array of tasks. The next section explores current use cases for LLMs."></li><li class="carousel-slide" data-ssplayer-slide-index="15" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/16/1739580232671?e=1743033600&amp;v=beta&amp;t=X_ABa21uTm28SGTTM498lctzVFRrMYIC2OfTGB8lblQ" alt="LLMs – reasoning engines for intelligent apps 15 Multi-modal language models hold particular promise for expanding the capabilities of language models. Multi-modal models can process and generate images, speech, and video in addition to text, and have become an important component of intelligent applications. Examples of new application categories made possible with multi-modal models include the following:• Creating content based on multiple input types, such as a chatbot where users can provide both images and text as inputs. • Advanced data analysis, such as a medical diagnosis tool that analyzes X-rays along with medical records. • Real-time translation, taking audio or images of one language and translating it to another language. Such examples highlight how multi-modal language models can enhance the possible use cases for language models. • Open-ended dialogue and chat: LLMs can engage in back-and-forth conversations, maintaining context and handling open-ended user queries and follow-up questions. This capability is foundational for chatbots, virtual assistants, tutoring systems, and similar applications. • Question answering: LLMs can provide direct answers to user questions, perform research, and synthesize information to address queries. • Classification and sentiment analysis: LLMs can classify text into predefined categories and assess sentiment, emotion, and opinion. This enables applications such as content moderation and customer feedback analysis. • Data transformation and extraction: LLMs can map unstructured text into structured formats and extract key information, such as named entities, relationships, and events. This makes LLMs valuable for tasks such as data mining, knowledge graph construction, and robotic process automation (RPA). As LLMs continue to grow in scale and sophistication, new capabilities are constantly emerging, often in surprising ways that were not directly intended by the original training objective. For example, the ability of GPT-3 to generate functioning code was an unexpected discovery. With advancements in the field of LLMs, we can expect to see more impressive and versatile capabilities emerge, further expanding the potential of intelligent applications. Multi-modal language models"></li><li class="carousel-slide" data-ssplayer-slide-index="16" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/17/1739580232671?e=1743033600&amp;v=beta&amp;t=xcXH-Vau58prt4D9YFvHflyKWxpMWvMS5iNoNJtrJ3U" alt="16 Building Blocks of Intelligent Applications Embedding models Embedding models are AI models that map text and other data types, such as images and audio, into high-dimensional vector representations. These vector representations capture the semantic meaning of the input data, allowing for efficient similarity comparisons and semantic search, typically using cosine similarity as the distance metric. Embedding models encode semantic meaning into a machine-interpretable format. By representing similar concepts as nearby points in the vector space, embedding models let us measure thesemantic similarity between pieces of unstructured data and perform semantic search across a large corpus. A paradigm shift in AI development The rise of LLMs represents a paradigm shift in the development of AI-powered applications. Previously, many reasoning tasks required specially trained models, which were time-intensive and computationally expensive to create. Developing these models often necessitated dedicated machine learning (ML) engineering teams with specialized expertise. In contrast, the general-purpose nature of LLMs allows most software engineers to leverage their capabilities through simple API calls and prompt engineering. While there is still an art and science to optimizing LLM-based workflows for production deployability, the process is significantly faster and more accessible compared to traditional ML approaches. This shift has dramatically reduced the total cost of ownership and development timelines for AI-powered applications. NLP tasks that previously could take months of work by a sophisticated ML engineering team can now be achieved by a single software engineer with access to an LLM API and some prompt engineering skills. Moreover, LLMs have unlocked entirely new classes of applications that were previously not possible or practical to develop. The ability of LLMs to understand and generate human-like text, engage in open-ended dialogue, and perform complex reasoning tasks has opened up a wide range of possibilities for intelligent applications across industries. You’ll learn more about LLMs in Chapter 3, Large Language Models, which discusses their history and how they operate. Embedding models and vector databases – semantic long-term memory In addition to the reasoning capabilities provided by LLMs, intelligent applications require semantic long-term memory for storing and retrieving information. Semantic memory typically consists of two core components—AI vector embedding models and vector databases. Vector embedding models represent the semantic meaning of unstructured data, such as text or images, in large arrays of numbers. Vector databases efficiently store and retrieve these vectors to support semantic search and context retrieval. These components work together to enable the reasoning engine to access relevant context and information as needed."></li><li class="carousel-slide" data-ssplayer-slide-index="17" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/18/1739580232671?e=1743033600&amp;v=beta&amp;t=O8vrjKXzAFegXOSZepzTZOIfYRnSI3abLkM1V7KmN4I" alt="Embedding models and vector databases – semantic long-term memory 17 Vector databases are specialized data stores optimized for storing and searching high-dimensional vectors. They provide fast, approximate nearest neighbor (ANN) search capabilities that allow intelligent applications to quickly store and retrieve relevant information based on spatial proximity. ANN search is necessary because performing exact similarity calculations against every vector in the database becomes computationally expensive as the database grows in size. Vector databasesuse algorithms, such as hierarchical navigable small worlds (HNSW), to efficiently find approximate nearest neighbors, making vector search feasible at scale. In addition to ANN search, vector databases typically support filtering and exact search on metadata associated with the vectors. The exact functionality and performance of these features vary across different vector database products. Vector databases provide an intelligent application with low-latency retrieval of relevant information given a query. Using the semantic meaning of the content for search, vector databases align with the way LLMs reason about information, enabling the application to apply the same unstructured data format for long-term memory as it does for reasoning. Pre-trained embedding models are widely available and can be fine-tuned for specific domains or use cases. Compared to LLMs, embedding models tend to be more affordable and can run on limited hardware, making them accessible to a wider range of applications. Some common applications of embedding models include the following: • Semantic search and retrieval: Embedding models can be used as a component in larger AI systems to retrieve relevant context for LLMs, especially in RAG architectures. RAG is a particularly important use case for the intelligent applications discussed in this book and will be covered in more detail in Chapter 8, Implementing Vector Search in AI Applications. • Recommendation systems: By representing items and user preferences as embeddings, recommendation systems can identify similar items and generate personalized recommendations. • Clustering and topic modeling: Embedding models can help discover latent topics and themes in large datasets, which can be useful for analyzing user interactions with intelligent applications, such as identifying frequently asked questions in a chatbot. • Anomaly detection: By identifying outlier vectors that are semantically distant from the norm, embedding models can be used for anomaly detection in various domains. • Analyzing relationships between entities: Embedding models can uncover hidden relationships and connections between entities based on their semantic similarity. You will explore the technical details and practical considerations of embedding models in Chapter 4, Embedding Models. Vector databases"></li><li class="carousel-slide" data-ssplayer-slide-index="18" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/19/1739580232671?e=1743033600&amp;v=beta&amp;t=1jdnq8HGd_vCwGIUj-VC24XGgcfMUuMG_HIiBBIb7O4" alt="18 Building Blocks of Intelligent Applications The term self-hosting models refers to the practice of deploying and running AI models, such as LLMs, on an organization’s own infrastructure and hardware resources. In this approach, the organization is responsible for setting up and maintaining the necessary computational resources, software environment, and infrastructure required to load and execute the models. Self-hosting AI models requires a significant upfront investment in specialized hardware, which can be cost-prohibitive for many organizations. Managing the model infrastructure also imposes an operational burden that requires ML expertise, which many software teams lack. This can divert the focus from the core application and business logic. Scaling self-hosted models to ensure availability can be challenging, as models can be large and take time to load into memory. Organizations may need to provision significant excess capacity to handle peak loads. Additionally, maintaining and updating models is a complex task, as models can go stale over time and require retraining or fine-tuning. With the active research in the field, new models and techniques constantly emerge, making it difficult for organizations to keep up. Model hosting providers The challenges associated with self-hosting have made model hosting providers a popular choice for intelligent application development. Model hosting providers are cloud-based services that offer a platform for deploying, running, and managing AI models, such as LLMs, on their infrastructure. These providers handle the complexities of setting up, maintaining, and scaling the infrastructure required to load and execute the models. In applications that use RAG, the vector database plays a crucial role. The application generates a query embedding, which is used to retrieve relevant context from the vector database. Multiple relevant chunks are then provided as context to the LLM, which uses this information to generate informed and relevant responses. You will learn about the technical details and practical considerations of vector databases in Chapter 5, Vector Databases. To implement AI models in your intelligent application, you must host them on computers, either in a data center or the cloud. This process is known as model hosting. Hosting AI models for applications presents a different set of requirements compared to hosting traditional software. Running AI models at scale requires powerful graphics processing units (GPUs) and configuring the software environment to load and execute the model efficiently. The key challenges in model hosting include high computational requirements and hardware costs, limited availability of GPU resources, complexity in managing and scaling the hosting infrastructure, and potential vendor lock-in or limited flexibility when using proprietary solutions. As a result, hardware and cost constraints must be factored into the application design process more than ever. Self-hosting models Model hosting"></li><li class="carousel-slide" data-ssplayer-slide-index="19" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/20/1739580232671?e=1743033600&amp;v=beta&amp;t=Pz_dq9HijSYysQye_GKQcpaSo7MAWiNl9kDI73LlINE" alt="Your (soon-to-be) intelligent app 19 With LLMs, embedding models, vector databases, and model hosting, you have the key building blocks for creating intelligent applications. While the specific architecture will vary depending on your use case, a common pattern emerges: • LLMs for reasoning and generation • Embeddings and vector search for retrieval and memory • Model hosting to serve these components at scale This AI stack is integrated with traditional application components, such as backend services, APIs, frontend user interfaces, databases, and data pipelines. Additionally, intelligent applications often include components for AI-specific concerns, such as prompt management and optimization, data preparation and embedding generation, and AI safety, testing, and monitoring. The rest of this section walks through an example architecture for a RAG-powered chatbot, showcasing how these components work together. The subsequent chapters will dive deeper into the end-to-end process of building production-grade intelligent applications. Model hosting providers offer several benefits: • Outsourced hardware and infrastructure management: Model hosting providers handle provisioning, scaling, availability, security, and other infrastructure concerns, allowing application teams to focus on their core product. • Cost efficiency and flexible pricing: With model hosting providers, organizations pay only for what they use and can scale resources up and down as needed, reducing upfront investment. • Access to a wide range of models: Providers curate and host many state-of-the-art models, continuously integrating the latest research. They often add additional features and optimizations to the raw models. • Support and expertise: Providers can offer consultation on model selection, prompt engineering, application architecture, and assistance with fine-tuning, data preparation, evaluation, and other aspects of AI development. • Rapid prototyping and experimentation: Model hosting providers enable developers to quickly test different models and approaches, adapting to new developments in the fast-moving AI/ML space. Scalability and reliability: Providers build robust, highly available, and auto-scaling infrastructure to meet the demands of production-scale intelligent applications. • Examples of model hosting providers include those from model developers such as OpenAI, Anthropic, and Cohere, as well as offerings from cloud providers such as AWS Bedrock, Google Vertex AI, and Azure AI Studio. Your (soon-to-be) intelligent app"></li><li class="carousel-slide" data-ssplayer-slide-index="20" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/21/1739580232671?e=1743033600&amp;v=beta&amp;t=33zNDE0i23tjDp_rBlIEE01UMnFfG1jzEi2q48Cl5Y8" alt="20 Building Blocks of Intelligent Applications Sample application – RAG chatbot Consider a simple chatbot application that leverages RAG that lets users talk to some documentation. There are seven key components of this application: Note This simple example application does not include evaluation or observability modules. • Chatbot UI: A website with a simple chatbot UI that communicates with the web server • Web server: A Python Flask server to manage conversations between the user and the LLM • Data ingestion extract, transform, load (ETL) pipeline: A Python script that ingests data from the data sources • Embedding model: The OpenAI text-embedding-3-small model, hosted by OpenAI • LLM: The OpenAI gpt-4-turbo model, hosted by OpenAI • Vector store: MongoDB Atlas Vector Search • MongoDB Atlas: A database-as-a-service for persisting conversations In this architecture, there are two key data flows: • Chat interaction: The user communicates with the chatbot with RAG • Data ingestion: Bringing data from its original sources into the vector database In the chat interaction, the chatbot UI communicates with the chatbot web server, which in turn interacts with the LLM, embedding model, and vector store. This occurs for every message that the user sends to the chatbot. Figure 2.1 shows the data flow for the chatbot application:"></li><li class="carousel-slide" data-ssplayer-slide-index="21" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/22/1739580232671?e=1743033600&amp;v=beta&amp;t=n1vm_tYIpvNY7QSxjCFZorqHQGjGL9j7bXuyqrbXf2w" alt="Figure 2.1: An example of a basic RAG chatbot conversation data flow Your (soon-to-be) intelligent app 21 The data flow illustrated in Figure 2.1 can be described as follows: 1. 2. 3. The user sends a message to the chatbot from the web UI. The web UI creates a request to the server with the user’s message. The web server sends a request to the embedding model API to create a vector embedding for the user query. The embedding model API responds with the corresponding vector embedding. The web server performs a vector search in the vector database using the query vector embedding. The vector store responds with the matching vector search results. 4. 5. The server constructs a message that the LLM will respond to. This message consists of a system prompt and a new message that includes the user’s original message and the content retrieved from the vector search. The LLM then responds to the user message. The server saves the conversation state to the database.6. 7. The server returns the LLM-generated message to the user in a response to the original request from the web UI. A data ingestion pipeline prepares and enriches data, generates embeddings using the embedding model, and populates the vector store and traditional database. This pipeline runs as a batch job every 24 hours. Figure 2.2 shows an example of a data ingestion pipeline: User 1. User input Web UI text-embedding-3-small 3. Generate embedding for user message OpenAI Model Hosting 2. Request with message 7. Respond with generated message Flask Web Server gpt-4-turbo 6. Save conversation to database 4. Vector search with user message embedding conversations (traditional DB) MongoDB Atlas embedded_content (Vector DB) 5. Generate response based on user message and search results"></li><li class="carousel-slide" data-ssplayer-slide-index="22" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/23/1739580232671?e=1743033600&amp;v=beta&amp;t=YSOTvLWHtnWzrQsyrvPsR7eaKNCy750pbxNwK2ZJ6tI" alt="22 Building Blocks of Intelligent Applications Figure 2.2: An example of a RAG chatbot data ingestion ETL pipeline Let’s look at the data flow shown in Figure 2.2: 1. 2. The data ingestion ETL pipeline pulls in data from various data sources. The ETL pipeline cleans the data into a consistent format. It also breaks the data into chunks of data. The ETL pipeline calls the embedding model API to generate a vector embedding for each data chunk. The ETL pipeline stores the chunks along with their vector embeddings in a vector database. The vector database indexes the embeddings for use with vector search. 3. 4. 5. While a simple architecture like this can be used to build compelling prototypes, transitioning from prototype to production and continuously iterating on the application requires addressing many additional considerations: • Data ingestion strategy: Acquiring, cleaning, and preparing the data that will be ingested into the vector store or database for retrieval. Advanced retrieval patterns: Incorporating techniques for efficient and accurate retrieval of relevant information from the vector store or database, such as combining semantic search with traditional filtering, AI-based reranking, and query mutation. • • Evaluation and testing: Adding modules for evaluating model outputs, testing end-to-end application flows, and monitoring for potential biases or errors. • Scalability and performance optimization: Implementing optimizations such as caching, load balancing, and efficient resource management to handle increasing workloads and ensure consistent responsiveness. Data sources 1. Pull in data ETL pipeline 2. Clean and chunk data 3. Generate embeddings for data chunks OpenAI model hosting text-embedding-3-small 4. Store embeddings and data MongoDB Atlas embedded_content (Vector DB) 5. Index embeddings with Atlas Vector Search"></li><li class="carousel-slide" data-ssplayer-slide-index="23" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/24/1739580232671?e=1743033600&amp;v=beta&amp;t=8Vxavd48v6_qvKX_kg64n5omiAlZEXOu9lBGxgCVYX0" alt="Summary 23 • Security and privacy: Securing the application to ensure that users can only interact with data that they have permission to, so that user data is handled in accordance with relevant policies, standards, and laws. • User experience and interaction design: Incorporating new generative AI interfaces and interaction patterns, such as streaming responses, answer confidence, and source citation. • Continuous improvement and model updates: Building processes and systems to update AI models safely and reliably and hyperparameters in the intelligent application. Intelligent applications represent a new paradigm in software development, combining AI with traditional application components to deliver highly personalized, context-aware experiences. This chapter details the core components of intelligent applications, highlighting the pivotal role of LLMs as reasoning engines. LLMs serve as versatile computational tools capable of performing diverse tasks, including chat, summarization, and classification, due to their general-purpose design. Complementing these reasoning engines are embedding models and vector databases, which function as the semantic memory of intelligent applications. These components enable the reasoning engineto retrieve pertinent context and information as needed. Additionally, the hosting of AI models demands dedicated infrastructure, as their unique hardware requirements differ significantly from traditional software needs. Using building blocks such as LLMs, embedding models, vector databases, and model hosting infrastructure, developers can create applications that understand complex, unstructured data, engage in open-ended interactions, generate novel content, and make autonomous decisions. Building these intelligent applications demands a new set of tools, approaches, and best practices. The next chapter will examine how LLMs work and the role they play in building intelligent applications. The rise of intelligent applications has significant implications for how software is made. Developing these intelligent applications requires an extension of traditional development skills. The AI engineer must possess an understanding of prompt engineering, vector search, and evaluation, as well as familiarity with the latest AI techniques and architectures. While a complete understanding of the underlying neural networks is not necessary, basic knowledge of natural language processing (NLP) is helpful. Intelligent application development also introduces new challenges and considerations, such as data management and integration with AI components, testing and debugging of AI-driven functionality, and addressing the ethical, safety, and security implications of AI outputs. The compute-heavy nature of AI workloads also necessitates focusing on scalability and cost optimization. Developers building traditional software generally do not need to face such concerns. To address these challenges, software development teams must adapt their processes and adopt novel approaches and best practices. This entails implementing AI governance, bridging the gap between software and ML/AI teams, and adjusting the development lifecycle for intelligent app needs. Implications of intelligent applications for software engineering Summary"></li><li class="carousel-slide" data-ssplayer-slide-index="24" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/25/1739580232671?e=1743033600&amp;v=beta&amp;t=KcOOw34dO4aDV4GVue5yFo_DWKjC5OYYhKyELBLyxtE" alt="25"></li><li class="carousel-slide" data-ssplayer-slide-index="25" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/26/1739580232671?e=1743033600&amp;v=beta&amp;t=F95fbHS76NbCxWNEi67YU7rXcMqWuRDsznaUw3UxFQM" alt="Part 1 Foundations of AI: LLMs, Embedding Models, Vector Databases, and Application Design This set of chapters provides in-depth and practical knowledge on the techniques and principles underpinning AI-intensive applications. You will progress quickly from fundamental concepts to real-world use cases and learn best practices for building your AI solution. This part of the book includes the following chapters: • Chapter 3, Large Language Models • Chapter 4, Embedding Models • Chapter 5, Vector Databases • Chapter 6, AI/ML Application Design"></li><li class="carousel-slide" data-ssplayer-slide-index="26" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/27/1739580232671?e=1743033600&amp;v=beta&amp;t=pxdfTDw9_yvBcIWxkSCWgdFaugscfZkHHJsegoe7eII" alt="27"></li><li class="carousel-slide" data-ssplayer-slide-index="27" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/28/1739580232671?e=1743033600&amp;v=beta&amp;t=bDul2xJRMThD9J8gfaobI_Ksog0E9oJy9B3l0Qlo_Q8" alt="3 Large Language Models Language models are computational algorithms designed to process, understand, and generate natural language. The study, research, and development of these algorithms is known as natural language processing (NLP). NLP predates the field of machine learning (ML) and can be traced back to the 1950s and the development of the first computers. While the first language models relied heavily on rule-based approaches, NLP shifted in the 1980s toward statistical methods and began to converge with ML. The increase in computational power and text corpora led to the development of deep learning and neural network-based language models in the early 21st century, which have seen significant progress over the last decade. Language models have a variety of applications in NLP for understanding and generating natural languages as well as more formal languages, such as programming and database query languages. Their use cases include tasks such as text labeling and sentiment analysis, translation, summarization, information extraction, and question answering. With the advent of large language models (LLMs), applications have further expanded to develop conversational chat systems and personal assistants, software development agents, and general problem-solvers. In this chapter, you’ll deep dive into the essential concepts and implementation of LLMs. This chapter will cover the following topics: • Language modeling with n-gram models to provide a probabilistic viewpoint • Artificial neural networks (ANNs), their architecture, and training paradigm • The application of ANNs to the language modeling domain • The Transformer architecture • LLMs in practice"></li><li class="carousel-slide" data-ssplayer-slide-index="28" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/29/1739580232671?e=1743033600&amp;v=beta&amp;t=xJ0l0IAoCfbvGZl8NvLKniXW7Eask3NXRKDLmt5tnPE" alt="28 Large Language Models Technical requirements This chapter is largely theoretical, with a short code snippet in Python to illustrate the tiktoken tokenizer library. To follow along, you will need access to a computer with Python version 3.8 or later. To make the most of this chapter, you will need proficiency with Python and the pip package manager. You will also need a basic knowledge of probabilities, calculus, and software development concepts such as APIs.Probabilistic framework When building AI-intensive applications that interact with LLMs, you will likely come across API parameters relating to probabilities of tokens. To understand how LLMs relate to the concept of probabilities, this section introduces the probabilistic framework underpinning language models. Language modeling is typically done with a probabilistic view in mind, rather than in absolute and deterministic terms. This allows the algorithms to deal with the uncertainty and ambiguity often found in natural language. To build an intuitive understanding of probabilistic language modeling, consider the following start of a sentence, for which you want to predict the next word: This is obviously an ambiguous task with many possible answers. The article the is a very common and generic word in the English language, and the possibilities are endless. Any noun, such as house, dog, spoon, etc. could be a valid possible continuation of the sentence. Even adjectives such as big, green, and lazy are likely candidates. Conversely, there are words rarely seen after an article, including verbs, such as eat, see, and learn. To deal with this kind of uncertainty, consider instead a slightly different question: “What is the probability of each word to come next?” The answer to this question is no longer a single word, but instead a large lookup table, assigning each word in the vocabulary a number, which represents the probability of this word following the. If this lookup table is representative of the English language, one would expect nouns and adjectives to have a higher probability than verbs. Table 3.1 shows what such a table could look like, using made-up values for the Probability column. You will see shortly how these probabilities can be calculated from a text corpus: The"></li><li class="carousel-slide" data-ssplayer-slide-index="29" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/30/1739580232671?e=1743033600&amp;v=beta&amp;t=DSK72D_sIbc1ecn-ArLSQjTV4M4EN9QfiEFzElpz6Cs" alt="Table 3.1: A partial lookup table for words following the word the Probabilistic framework 29 Previous word … the the the … the the the … the the the …. Next word … house dog spoon … big green lazy … eat see learn .. Probability … 0.012% 0.013% 0.007% … 0.002% 0.001% 0.001% … 0.000% 0.000% 0.000% … How does this sentence continue? What does the probability distribution look like now? If you are familiar with this sentence1, you’ll agree that at this point, the probability for the word lazy will stand out above all others. Your internal language model can’t help but autocomplete the entire sentence, and the words lazy dog will just pop into your head. In this simple example, one (but not the only) way to decide which word comes next is to scan through this lookup table and find the word with the highest probability. This method, known as greedy selection, would suggest that the word dog is the most probable continuation of the sentence. However, it’s important to note that there are many possibilities, each with a different probability. For instance, the word house is also a close second in terms of probabilities, indicating that it could also be a likely continuation of the sentence. To capture the flexibility and expressiveness of natural language, language models operate in terms of probabilities, and the process of training a language model means assigning probabilities for each word continuing the sentence thus far. Assume you have gone through the process of selecting the next word several times, and find yourself further along in the sentence: The quick brown fox jumps over the 1 This sentence is a pangram. A pangram contains every letter of the alphabet at least once. The sentence has been used in various contexts, such as typing practice and testing the display of text in computers."></li><li class="carousel-slide" data-ssplayer-slide-index="30" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/31/1739580232671?e=1743033600&amp;v=beta&amp;t=aIgv19Z8juZoOQDjeKs6jomH5-Kk7beubJpP_UBfHUE" alt="30 Large Language Models One of the first formalisms of a language model is an n-gram model, a simple statistical language model, first published in 1948 in Claude Shannon’s famous paper A Mathematical Theory of Communication (https://people.math.harvard.edu/~ctm/home/text/others/shannon/ entropy/entropy.pdf). An n-gram language model can be described as a giant lookup table, where the model considers the last n-1 words to predict the next. For n=2, you get a so-called bigram model, looking back only one word, as shown in Table 3.1. As the sentence in the previous example illustrated, such a simple bigram model is limited and fails to capture the nuances of natural language. However, before exploring what happens when n is scaled up to larger values, let’s briefly discuss how you would train a bigram model, which is to say, how to calculate the probabilities for each pair of words in the table: 1. Take a large corpus of text, such as the collection of all Wikipedia pages in English. 2. Scan through this text and count the occurrences of single words as well as observed pairs of words. Record all counts in a lookup table. Calculate the probability of word 3. 4. following word as follows: divide the count for the word pair by the count of the single word . For example, to calculate the probability of seeing the word dog following the word the, divide the pair count by the single word count in the following way: Here, the term is pronounced as “probability of x given y.” In other words, the probability of seeing the word dog given we’ve just seen the word the is the count of seeing the words in combination (the numerator) divided by all counts of seeing the by itself (the denominator). Thus, the training process of an n-gram language model only requires a single pass over the text, counting all occurring n-grams and (n-1)-grams, and storing the numbers in a table. In practice, several tricks improve the quality of n-gram models, such as including special <start> and <end> markers at the beginning and end of each sentence, splitting words into smaller sub-words, such as playing into play and -ing, and many other improvements. You will review some of these techniques later in the Tokenization section, and they apply to modern LLMs as well. But why is that? Aren’t you in the same situation as before, asking what follows next after the? The key difference here is that you have more context; you see more of the sentence, which demonstrates that considering only the preceding word is not sufficient to build a good predictor of the next word. Yet this basic concept marks the very beginning of language models and can be viewed as a distant ancestor of the likes of ChatGPT and other modern LLMs. n-gram language models"></li><li class="carousel-slide" data-ssplayer-slide-index="31" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/32/1739580232671?e=1743033600&amp;v=beta&amp;t=97fgcYEw5cULHAxVGcDaEicrH8IYyAKHh_c-xa7wbkA" alt="Table 3.2: A possible entry in the lookup table for an 8-gram Probabilistic framework 31 Let’s now revisit the choice of n. As you have seen, a low value, such as n=2, doesn’t yield a very good language model. Is it just a matter of scaling up n until you reach the level of desired quality? A larger n value can capture more context and leads to a more predictive model. For n=8, the model can look back at the last seven words. The lookup table, as shown in Table 3.2, would contain a row that captures the example sentence: However, increasing n to large values has several challenges, which make this approach infeasible in practice. The size of the lookup table grows exponentially with a larger n. The Oxford English Dictionary contains approximately 273,000 English words (https://en.wikipedia.org/wiki/List_of_dictionaries_ by_number_of_words), which allows for 74.5 billion possible combinations of two words (though many of these combinations would never be seen in a text). Increasing the n-gram model to n=8, the possible combinations of eight words grows to the astronomical number of . Storing an entry in the table for each combination would be impossible as this number far exceeds all available hard drive storage space in the world, especially since the world’s collective data is estimated to reach 175 zettabytes = bytes by 2025 (https://www. networkworld.com/article/966746/idc-expect-175-zettabytes-of-data- worldwide-by-2025.html). Of course, most of these word combinations would never be encountered, and you could choose to omit unseen n-grams in the table. This challenge, known as the sparsity problem, highlights the real issue of n-gram models. As you grow n, the probability of encountering any one n-gram shrinks exponentially. Most combinations of n words would never be encountered for any realistic size of training dataset. When processing text that is not part of the training corpus, the model would assign zero probability for unseen n-grams. The model would not be able to make meaningful predictions in this case, and this problem would be exacerbated the larger n became. In summary, while n-grams have their uses for certain narrow applications and educational purposes, the language models of today have evolved beyond purely statistical approaches. LLMs use machine learning techniques to deal with some of the issues pointed out above, which you’ll learn about in the next section. Previous 7 words … the quick brown fox jumps over the …. Next word … lazy .. Probability … 99.381% …"></li><li class="carousel-slide" data-ssplayer-slide-index="32" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/33/1739580232671?e=1743033600&amp;v=beta&amp;t=IutNIEWCSEMFNEe6k-uLgydp0HNwfLYrCHoEkSllmXI" alt="32 Large Language Models Machine learning for language modelling Before diving into language modeling approaches using ML, this section first introduces some general ML concepts and gives a high-level overview of different neural network architectures. At its core, ML is a field concerned with developing and studying algorithms that learn from data. Rather than executing hardcoded rules, the system is expected to learn by example, looking atprovided inputs and desired outcomes (often referred to as targets in ML literature) and adjusting its behavior during the training process to change its outputs to closely resemble the user-provided targets. ML algorithms are roughly differentiated into three groups: One class of supervised learning algorithms is artificial neural networks (ANNs). All modern LLMs are variations of the basic ANN architecture. When you make an API call to a model such as GPT-4, your question flows through an ANN to produce the answer. These models have evolved in size and complexity over decades, but the core principles and building blocks remain the same. The neural architectures found in human brains may have inspired the original design of ANNs, but ANNs are significantly different from their biological counterparts. ANNs are made of many smaller units called neurons, which are interconnected with each other in various patterns, depending on the network architecture. Each neuron is a small processing unit, receiving numeric signals from other neurons and passing a (modified) signal to its successor neurons, analogous to biological neurons. ANNs have tunable parameters, referred to as weights, which sit on the connections between two neurons and can influence the signal passing between them. One of the most basic ANN architectures is the so-called feed-forward network (FFN), depicted in Figure 3.1. In this architecture, neurons are arranged in layers, starting with an input layer, followed by one or more hidden layers, and finally an output layer. The layer size, which refers to the number of neurons per layer, can vary. Input and output layer sizes are determined by the specific problem domain. For example, you may want to learn a mapping from a two-dimensional input (say, the body mass index and age of a person) to a one-dimensional output (say, the daily resting calories burnt). The size of hidden layers is often chosen arbitrarily through experimentation in a process called hyper-parameter tuning. In FFNs, every neuron in one layer connects to all neurons in the following layer, leading to a many- to-many relationship between two consecutive layers. Figure 3.1 shows an FFN architecture with one input layer (Layer 1), two hidden layers (Layers 2 and 3), and one output layer (Layer 4): • Supervised learning • Unsupervised learning • Reinforcement learning Each of these groups has different learning objectives and problem formulations. For language modeling, you can mainly consider supervised (and related self-supervised) algorithms. Artificial neural networks"></li><li class="carousel-slide" data-ssplayer-slide-index="33" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/34/1739580232671?e=1743033600&amp;v=beta&amp;t=MdJTocFKfu609r96wHgaDopLIpnShvYql5Dnhfn7AKQ" alt="Figure 3.1: A feed-forward neural network architecture Figure 3.2: Activation of a single neuron with two inputs Machine learning for language modelling 33 Inputs Inputs Layer 1 Input layer Layer 2 Hidden layers Layer 3 Layer 4 Output layer Outputs Zooming in on the functioning of a single neuron, Figure 3.2 shows a neuron with inputs from two ). The inputs are first multiplied with their corresponding weight and then summed up. The resulting and the result forms the output of the other neurons (denoted and ). The connections to the neuron contain the weights (denoted and sum is passed through a non-linear activation function neuron (shown as ). In mathematical terms, this is expressed as follows: While the specifics of activation functions are out of scope for this chapter, suffice it to say that non-linearity is important for the network to be able to learn complex patterns in the data. sum act z z w2 w1 z 1 2 3"></li><li class="carousel-slide" data-ssplayer-slide-index="34" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/35/1739580232671?e=1743033600&amp;v=beta&amp;t=hJj15PLgxPY4t8_Ela_EAxZgORXzWyKZ0nO15-L9c1w" alt="34 Large Language Models During a forward pass through the network, you present the input data (for example, the BMI and the age of a person) at the input layer, calculate the activations of all neurons of the layer, pass these activations to the next layer, and so forth, until the output layer produces an outcome (which, in this example, you can interpret as the model’s prediction for calories burnt by a person). It may seem surprising that the simple activation functions governing individual neurons in a neural network can lead to complex pattern recognition capabilities. This phenomenon is rooted in the universal approximation theorem, which proves that a neural network with enough hidden layers and neurons can approximate any continuous function to any desired degree of accuracy. You now know how data flows forward in an ANN from input to output layer. For an untrained model, this is only the first of three phases. In the next section, you’ll learn about the other twophases required to train an ANN: loss calculation and a backward pass. So far, this chapter has described the forward pass of a network, that is, how a response for a given input is calculated. Since the initial weights of an ANN are chosen randomly, the output of an untrained network is also random and nonsensical. The weights need to be adjusted during the training process. The goal of training a neural network is to make its outputs match the provided targets for any given input. Thus, for supervised learning, a training dataset consists of input/target pairs of known correct responses. In the example of predicting the calories burnt given a person’s BMI and age, the training dataset would consist of many measurements of people’s BMI and age (the inputs) and theirmeasured calories burnt (the targets). The more measurements the dataset contains, the better the model can learn patterns from the relationship between inputs and targets. The training process for an ANN can be broken down into three phases, as illustrated in Figure 3.3: 1. 2. 3. Forward pass: Calculating the outputs from the inputs. Loss calculation: Calculating an error signal between the outputs and the desired targets. Backward pass and weight adjustment: Propagating the error back through the model and adjusting each of the weights. Training an artificial neural network"></li><li class="carousel-slide" data-ssplayer-slide-index="35" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/36/1739580232671?e=1743033600&amp;v=beta&amp;t=5mCl3mGi55t6Ii_msm2GLIRF4jSuVFTXdWskI-e3phs" alt="Figure 3.3: The three phases of training an ANN Machine learning for language modelling 35 This process is repeated over multiple passes of the dataset, until the weight parameters no longer meaningfully change. At this point, the model is said to have converged and is ready for inference. Training starts with a forward pass of the data, passing in the inputs and recording the network’s output. As this output may differ from the correct target (especially for an untrained network with random weights), it is possible to calculate a metric called loss, which is a scalar number that reflects the difference between actual and desired output. The loss is required to execute the backward pass. This step will adjust all weights of the network in such a way that the network will produce an output closer to the target for the given input. The activation for each neuron is a well-formed differentiable expression with sums, products, and a differentiable activation function. This means that the derivative of a weight with respect to the loss can be calculated by the rules of calculus to determine how each weight parameter needs to be adjusted to minimize the loss. This gradient calculation is then propagated backward to previous layers using the chain rule of calculus, all the way to the input layer. Having calculated the gradients for each weight in this way, the weights can then be updated. Controlled by a parameter called the learning rate, the weights can be moved a small step toward the direction of minimizing the loss. While it’s possible to execute this loop of forward and backward passes for every single entry in the training set one by one, in practice the training set is split into small batches. A batch may contain tens, hundreds, or even thousands of data points. The batch size is another hyper-parameter chosen experimentally through hyper-parameter tuning before the actual training process. Batching up the data in such a manner serves these purposes: 1. Forward pass 3. Backward pass and weight adjustment 2. Loss calculation"></li><li class="carousel-slide" data-ssplayer-slide-index="36" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/37/1739580232671?e=1743033600&amp;v=beta&amp;t=Ee5mt77L0dFm3VHcqb82m2VgGFvZB4BRwFP8WC-igI0" alt="36 Large Language Models The first processing step to convert text to numeric inputs is called tokenization. During this phase, words are split into common sub-words, characters, and punctuation marks, making up the vocabulary of tokens. Each token is then assigned a unique integer ID. When interacting with LLMs, especially when dealing with self-hosted open-source models, the choice of tokenizer is important and must match exactly the one used during the training of the model. Luckily, many common open-source tokenizers exist. Even commercial LLM providers, such as OpenAI, have open-sourced their tokenizer libraries to make it easier to interact with their models. Bindings of OpenAI’s tiktoken library are available for many popular programming languages, including Python, C#, Java, Go, and Rust. The previous section showed how ANNs can learn mappings of numerical inputs to numerical outputs. Language, however, is inherently non-numeric: a sentence is a sequence of discrete words from a large vocabulary. Building a neural network-based word predictor poses the following challenges: • The inputs to the model are discrete words. Since ANNs operate on numeric inputs and outputs, a suitable mapping from words to numbers and vice versa is required. • The inputs are further sequential. Unlike bigrams, the model should be able to take more than one word into account when predicting the next word. • The output of the language model needs to be a probability distribution over all possible next words. To form a proper distribution, the outputs need to be normalized to be non-negative and sum up to one. The following sections will explain these challenges and review how they are addressed in modern language models. • It leads to higher efficiency as batches can be processed in parallel, especially on specialized hardware, such as graphical processing units (GPUs). The error gradients backpropagated through the network are averaged across each batch. This leads to more stable training as single outliers in the data have less impact on the weight changes. • Training continues until the model no longer improves on unseen validation data. After training, the trained model can then be applied to previously unseen inputs. For example, the model can be integrated into a fitness tracking app, where it predicts burnt calories based on a person’s BMI and age with the expectation that it will not only work for measurements in the training data but also generalize to new data points as well. This application of a trained model to new data is known as inference. This training procedure is at the core of every neural network, including LLMs. As neural networks operate on numeric data, the next section will show how language can be represented numerically to make it compatible with the use of ANNs. ANNs for natural language processing Tokenization"></li><li class="carousel-slide" data-ssplayer-slide-index="37" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/38/1739580232671?e=1743033600&amp;v=beta&amp;t=ckVuStlON3AvJEDXe-yZUYEYbxdq1K22sB6tP2tPLQE" alt="ANNs for natural language processing 37 Running this code produces the following output: The following code example demonstrates the use of the tiktoken Python library. After installing the package with pip install tiktoken, you can create an encoder object and encode any text, which will return a list of token IDs. The following code snippet tokenizes the sentence tiktoken is a popular tokenizer! and decodes each token ID back into its byte string: Embedding is the process of mapping data into a high-dimensional vector space. This concept is not just relevant to the training of language models but will also play an important role for vector databases to retrieve semantically similar items, which we’ll discuss later in Chapter 5, Vector Databases. Embeddings can be created for arbitrary data entities: words, sentences, entire documents, images, or even more abstract concepts, such as users or products in the context of building recommender systems. You can see that the word tiktoken was split into three tokens, t, ik, and token, likely because the word itself is not common enough to warrant its own token in the vocabulary. Also of note is that whitespace is often encoded as part of a token, at the beginning, such as in “ is.” When interacting with proprietary models via APIs, tokenization typically happens automatically and server-side. This means that you can submit prompts in text form without having to tokenize theinputs yourself. However, tiktoken and similar libraries are still useful tools when building AI-powered applications. For example, you can use them to calculate the number of tokens of a request, as API calls are usually charged by the number of submitted and returned tokens. Additionally, language models have an upper token limit for their inputs, known as their context size. Requests that are too large may fail or get truncated, which impacts the model’s response. For the purposes of developing applications with LLMs, it is sufficient to know about tokenization when it comes to the preprocessing of text. However, this is only the first step in making neural networks understand textual inputs. Even though the token IDs are numeric, the assignment from the token to its ID happens arbitrarily. Neural networks interpret their inputs geometrically and are not well suited to processing large integer numbers. In the second step, called embedding, these integers are converted into high-dimensional floating-point vectors, also known as embedding vectors or simply embeddings. import tiktoken # use the gpt-4 tokenizer 'cl100k_base' encoder = tiktoken.get_encoding(&quot;cl100k_base&quot;) token_ids = encoder.encode(&quot;tiktoken is a popular tokenizer!&quot;) print(&quot;Token IDs&quot;, token_ids) tokens = [encoder.decode_single_token_bytes(t) for t in token_ids] print(&quot;Tokens&quot;, tokens) Token IDs [83, 1609, 5963, 374, 264, 5526, 47058, 0] Tokens [b't', b'ik', b'token', b' is', b' a', b' popular', b' tokenizer', b'!'] Embedding"></li><li class="carousel-slide" data-ssplayer-slide-index="38" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/39/1739580232671?e=1743033600&amp;v=beta&amp;t=U39nMkkTjLk2R_IAxzknlEObMbk5GlVh7m_h4RQzy6M" alt="38 Large Language Models Figure 3.4: Visual representation of tokens embedded in a three-dimensional vector space The purpose of embeddings is twofold: • They are fixed-length floating-point representations of their corresponding entities, ideally suited to be processed by neural networks. • Embeddings are coordinates in a vector space. With the right choice (or, rather, training), embeddings can represent semantic similarities of data entities through their geometric proximity. This enables the use of geometric algorithms, such as clustering or nearest neighbor search, to operate on the semantic meaning of the embedded data. Embeddings are a fundamental concept at the core of language models and vector search. To understand how tokens can be embedded, let’s assume a small vector space with only three dimensions, as illustrated in Figure 3.4. To map a token into this space, a random point in this space is assigned to each token. Here, the token is represented by its integer ID, and the random point in this space is indicated by its x, y, and z coordinates. The mapping is done with the help of an embedding matrix consisting of n rows and d columns, initialized with random floating-point numbers. Here, n is the size of the vocabulary and d is the embedding dimensionality (in this example, d equals 3). To retrieve the coordinates for a token, the token ID is used as a row index into the embedding matrix, returning a d-dimensional vector. For example, the token fox may be assigned the following coordinates: [-0.241, 1.356, -0.7882]. Just like the weights of a neural network are assigned randomly before training, the values of the embedding matrix are also chosen randomly. Furthermore, and this is a crucial step in the training of LLMs, the embedding matrix values are treated as additional learnable parameters of the neural network. By allowing the gradients to flow all the way back into the embedding layer, the model can update the positions of the token coordinates during training in such a way that it helps the prediction task. … fox the dog quick brown over lazy jumps"></li><li class="carousel-slide" data-ssplayer-slide-index="39" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/40/1739580232671?e=1743033600&amp;v=beta&amp;t=2peNm5IFbj09S0haxc4kFEZKt0tVONCrmNLOdlQuNZ4" alt="ANNs for natural language processing 39 As you have seen in the n-gram language models section, the model needs to output a probability distribution over the next tokens, that is, one numeric value for each token in the vocabulary. By choosing an output layer size matching the vocabulary size, the neural network will give you the right output shape, but these numbers can theoretically be any real number, including negative or very large positive numbers. To form a proper probability distribution, the outputs must meet two additional conditions: • The outputs must be non-negative. • The outputs must sum up to 1.0. A special activation function called softmax has been designed for this exact purpose and is used for the output layer when probabilities are expected. The mathematical formulation of the softmax function is as follows: Intuitively, the application of the exponential function in the numerator maps the range from negative for all x). By dividing by the sum of all to positive infinity to that of non-negative numbers ( exponents, you normalize the values to ensure that the sum of outputs exactly adds up to 1. The targets for training the model also need to contain vectors of the same length (one value per token). Since the next word at each step in the token sequence is known, you can encode the correct token with one-hot encoding. You can assign a value of 1.0 to the correct token’s position in the vector and 0.0 to all other positions, as shown in Figure 3.5. This ensures that during the backward pass, the probability of seeing the correct next token is increased while all other probabilities are decreased. Studies on fully trained embedding layers of LLMs reveal that the model moves semantically similar tokens close together. In the earlier example, you might find a cluster of nouns (fox, dog) or a cluster of adjectives (quick, lazy, brown). However, with only three dimensions, similarity is limited to only three attributes by which tokens can be compared. LLMs use vector spaces with much larger dimensionality, often in the order of hundreds or even thousands of dimensions. In such a high- dimensional space, tokens can relate to each other (and be close to each other geometrically) in many ways. Some of the dimensions may have interpretable meanings, such as the sentiment of a word. However, most of them are likely to make sense only to the model internally. In this section, you have seen how text is prepared for neural network training by splitting it into tokens and assigning token IDs, which can be used as an index to find the corresponding embedding vector in the embedding matrix. These vectors have geometric meaning and can be updated as part of the training phase. Next, you’ll learn how the outputs of the neural network can be interpreted as probabilities of choosing the next token. Predicting probability distributions"></li><li class="carousel-slide" data-ssplayer-slide-index="40" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/41/1739580232671?e=1743033600&amp;v=beta&amp;t=TGomL3vmtur50AR6eb7MOOG7ujTqE_NQi5HdPjSf414" alt="40 Large Language Models Figure 3.5: Example output probabilities as predicted by the model and targets for the token fox To produce good next-token predictions, a language model needs to be able to consider a sizeable context, reaching back many words or even sentences. To demonstrate this, consider the following text: A solitary tiger stealthily stalks its prey in the dense jungle. The underbrush whispers as it attacks, concealing its advance toward an unsuspecting fawn. The second sentence in this example contains two pronouns, it and its (shown in bold above), both referring to the tiger from the previous sentence, many words apart. But without seeing the first sentence, you’d likely assume that it refers to the underbrush instead, which would have led to a very different sentence ending, such as this one: The underbrush whispers as it sways gently in the soft breeze. This shows long-range context matters for language modeling and next-token prediction. You can construct examples of arbitrary length where the pronoun resolution relies on the context provided many sentences earlier. These temporal dependencies and ambiguities are inherent to natural language, so a good language model needs to process long sequences of words. However, the FFN architecture introduced earlier is stateless and does not possess any memory of previously seen inputs. It is not suitable for sequential tasks, where future tokens depend on andrefer to previous tokens. With tokenization, embedding, and softmax activation, you can convert language into a numeric format that an ANN can understand. Further, the ANN can interpret the numeric outputs of the model as a discrete probability distribution over the next token. The final missing piece to model language with ANNs is the processing of sequences, which are discussed next. Dealing with sequential data Outputs Targets brown fox jumps over quick the … 0.02 0.08 0.30 0.20 0.05 0.10 0.25 brown fox jumps over quick the … 0.0 0.0 0.0 0.0 0.0 0.0 1.0"></li><li class="carousel-slide" data-ssplayer-slide-index="41" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/42/1739580232671?e=1743033600&amp;v=beta&amp;t=d0fy9x0g2SC3aWU9j0WRgrDDHnXw63kvwRHnn-uMQxc" alt="Figure 3.6: Recurrent connections give RNNs an internal state Dealing with sequential data 41 The training of RNNs remains like that of FFNs, where the RNN can be unrolled across time steps, and conceptually transformed into an FFN (albeit with many more layers and additional inputs corresponding to the internal states). One class of ANNs that deals with sequential data is called recurrent neural networks (RNNs). Unlike FFNs, RNNs include connections from a neuron to itself and its neighboring neurons within the same layer. These recurrent connections give the model an internal state, where previous activations can flow in a circular fashion and remain in the network when processing the next input, as depicted in Figure 3.6: Sequence learning is a fundamental problem in ML, not just for NLP but many other areas, such as time series prediction, speech recognition, video understanding, robot control, etc. In some cases, the inputs are sequential, in others the outputs are sequential, or even both. Different modifications to the FFN architecture have been proposed to tackle this problem. Recurrent neural networks The quick brown The The, quick great? quick? sky? rabbit? fox? car? bus? brown? way?"></li><li class="carousel-slide" data-ssplayer-slide-index="42" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/43/1739580232671?e=1743033600&amp;v=beta&amp;t=Td8FfU-bx_RMYOSfzRDnTCQsoo0VmpfJTkxEoX1I8N8" alt="42 Large Language Models However, one limitation of RNNs is that the gradients quickly diminish with each iteration through a recurrent connection. The network tends to forget activations that go back more than a few time steps, an issue known as the vanishing gradient problem. To address this problem, further architectural changes have been proposed, including long short- term memory (LSTM) and gated recurrent unit (GRU) networks. In these models, cells consisting of multiple neurons are introduced, which can trap the gradient signal inside over thousands of time steps, thus alleviating the vanishing gradient problem. LSTMs have been applied successfully to many sequence problem domains, including robotics, speech and handwriting recognition, language translation, and playing video games. However, the training of recurrent networks happens sequentially along the time dimension, meaning that each time step requires a separate forward and backward pass through the network. This slows down training significantly, particularly for long sequences. RNNs have another limitation. Though the network can, in principle, remember previous activations due to its recurrent connections, this internal state needs to be carried forward for each time step. The model does not have direct access to the global context and its previous inputs explicitly. Both limitations were addressed by a breakthrough discovery in 2017, which is discussed in the next section. In 2017, Google published a new network architecture aimed to address some of the drawbacks of recurrent networks. This now famous paper, titled Attention Is All You Need (https://arxiv. org/abs/1706.03762), introduced the Transformer architecture, which departed from the idea of recurrent connections and instead relied on an attention mechanism to consider previous tokens in an otherwise stateless neural network. This publication marked a significant shift in the field of ML and NLP and paved the way for almost all modern LLMs as variations of the original transformer. Their advantages over recurrent networks—including the ability to process sequences in parallel, reduced computational complexity for long sequences, and superior handling of long-range dependencies—are key reasons why transformer architectures have become ubiquitous in the domain of NLP and beyond. At a high level, the original transformer model consists of two components: an encoder and a decoder. This architecture was proposed for the purpose of language translation, a sequence-to-sequence learning task with an input sequence of tokens in the source language processed by the encoder, and an output sequence of tokens in the target language processed by the decoder. While some LLMs still use this encoder/decoder structure, other families of models nowadays use simplified architectures building only on the encoder (such as BERT language models and variants) or the decoder (the GPT family). Generative models, including OpenAI’s GPT series, Meta’s Llama, Anthropic’s Claude, and Google’s PaLM models, all frame language modeling as next-tokenprediction, where the learning task is sequence-to-single token, as compared to sequence-to-sequence in the encoder/decoder structure. This allows for a simpler architecture, doing away with the encoder and only using the decoder part of a transformer. Transformer architecture"></li><li class="carousel-slide" data-ssplayer-slide-index="43" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/44/1739580232671?e=1743033600&amp;v=beta&amp;t=61k32gHzs3V0pWB6rHZO9ypqnGkSAnKG2sgdLkx9phE" alt="Figure 3.7: Attention maps for two sentence variations ending in hungry versus tasty Dealing with sequential data 43 Figure 3.7 shows attention maps for two sentences where only the last word differs. Darker color shades indicate higher attention weights. A transformer model would learn to pay more attention to tokens related to hungry, such as cat, in the first example, and to tokens related to tasty, such as food, in the second example. This attention mechanism is key to transformers. The landmark paper on Transformer architecture demonstrated that this mechanism alone could solve the sequential data problem without introducing recurrent connections into the architecture. Both the encoder and decoder of a transformer consist of many layers of so-called transformer blocks. Unlike FFNs, where each layer is simply a fully connected layer of neurons with the next, a transformer block has an additional attention layer preceding the fully connected layer. The attention layer’s purpose is to learn which tokens in the sequence seen so far are most relevant when processing the current token. It assigns high attention weights to words that are highly relevant in the current context, and low attention weights to generic or irrelevant words, as you can see in Figure 3.7: A&nbsp;ention map for the word “hungry” is A&nbsp;ention map for the word “tasty” The The The The cat cat cat cat eats eats eats eats food food food food because because because because it it it it is is is tasty tasty hungry hungry"></li><li class="carousel-slide" data-ssplayer-slide-index="44" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/45/1739580232671?e=1743033600&amp;v=beta&amp;t=TIorMiIwoofQkPDa_MCaQ0eLRCXVWUkHuNdCOfqczdI" alt="44 Large Language Models LLMs in practice So far, this chapter has mainly discussed the theoretical foundations of LLMs. Let’s close this chapter with an overview of the LLM landscape as it stands today, discussing some considerations for choosing an appropriate LLM as well as different techniques to tailor the model’s responses to your needs. Generative AI and LLMs are a rapidly changing field, with new models, frameworks, and research papers on the topic released frequently. Most of the know-how to train an LLM is publicly available, yet at the time of writing, the cost of training a state-of-the-art LLM from scratch is still in the order of tens to hundreds of millions of US dollars, due to the large amount of GPU compute resources needed. This cost puts training your own model out of reach of individuals and most smaller companies, who will have to rely on pre-trained LLMs. The most competent models as of the time of writing, namely OpenAI’s GPT-4o (https://openai. com/) and Anthropic’s Claude 3.5 Sonnet (https://www.anthropic.com/), remain closed source but can be accessed via APIs on a per-token cost model. Open-source models, such as Meta’s Llama 3 (https://llama.meta.com/), are still behind on common benchmarks, but the gap is closing quickly. Depending on your use case and throughput requirements, it may be more cost-effective to self-host an open-source model or choose one of the many providers that offer model-hostingservices. Other considerations when choosing between open and closed models include security and compliance, technical support, and vendor lock-in. Commercial LLM offerings often come with technical support and moderation endpoints to filter illegal requests and harmful or objectionable content and provide detailed documentation for their APIs and models. Open models, in contrast, provide more flexibility and customization, as well as transparency and interoperability with other models, and avoid potential vendor lock-in. LLMs accept inputs in the form of text prompts (or simply prompts), which can be questions, statements, or requests that guide the model’s response. While the best LLMs are very capable and efficient in answering a wide range of different requests, chances are that a simple prompt may not lead to acceptable results for your application. Your use case may require special domain knowledge or responses in an uncommon (natural or programming) language that is under- represented in the original training dataset, or you may work with proprietary non-public data. This will not prevent you from integrating LLMs into your applications. There are several strategies available to deal with this scenario: • Different prompting strategies • Fine-tuning an LLM on custom data • Retrieval-augmented generation (RAG) The evolving field of LLMs Prompting, fine-tuning, and RAG"></li><li class="carousel-slide" data-ssplayer-slide-index="45" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/46/1739580232671?e=1743033600&amp;v=beta&amp;t=EIYo4kXaPFp0rv6lqzQrvBdLfkphPc3VN02g6gjTdLE" alt="Summary 45 Prompting an LLM is more of an art than a hard science, which has led to an entirely new “prompt engineer” role in software development. Common techniques include zero- and few-shot prompting and chain-of-thought prompting. For more advanced prompting techniques, you can refer to the Prompt Engineering Guide at https://www.promptingguide.ai/. You’ll learn more about different prompting strategies in Chapter 9, LLM Output Evaluation. For an even more custom-tailored response, pre-trained LLMs can be further trained on your own specific data through a process known as fine-tuning. Fine-tuning allows for adjustment of thelanguage and style of the response, as well as injecting domain knowledge into the LLM. However, the process can be expensive depending on the dataset size. Fine-tuned models need to be evaluated carefully,as adjusting the weights may lead to overfitting, which can impact the model responses on previous tasks. Retrieval-augmented generation (RAG) is another strategy to inject outside knowledge of proprietary data into an LLM. Here, an external knowledge base (for example, a vector database, which you will learn about in Chapter 5, Vector Databases) is first queried with each request, and relevant information from the external data source is then included in the LLM prompt. While this alleviates some of the downsides of fine-tuning, one limiting factor is the length of the prompt (the context size) that the LLM can process in a single request. It is thus important to filter out irrelevant information to keep the prompt size manageable.This chapter covered the main components of a modern transformer-based LLM and a quick overview of the LLM landscape as it stands today. It detailed how text can be transformed into numeric data to be processed by ANNs. To summarize, sentences of a large text corpus are tokenized and assigned integer token IDs. Token IDs index into an embedding matrix, turning the integers into real-valued embedding vectors of fixed length. To create the targets for supervised training, the inputs are shifted by one token to the right, so that the target at each position becomes the token that follows in the sequence. Sequential data can be learned with recurrent neural networks, but these have been superseded by transformers, which use an attention mechanism to learn which previous tokens are most relevant to predict the next. At every step in the sequence, the model predicts probabilities for each token in the vocabulary, which can be used to generate the next token. The training dataset, consisting of inputs and targets, is split into smaller batches. With repeated forward and backward passes through the network, gradient calculation, and weight adjustments, the network learns to adjust the probabilities for each token given the context of previous tokens. You learned how these mechanisms have been put into practice by modern-day LLMs. You also got a brief introduction to some methods that can help you make the most of your language model. In the next chapter, you will take this knowledge forward with an understanding of embedding models and their crucial role in machine learning. Summary"></li><li class="carousel-slide" data-ssplayer-slide-index="46" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/47/1739580232671?e=1743033600&amp;v=beta&amp;t=_yjA9sEOjkX1GKp_uf7eVa5ApZzREQBYMo-OSvT3mZI" alt="47"></li><li class="carousel-slide" data-ssplayer-slide-index="47" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/48/1739580232671?e=1743033600&amp;v=beta&amp;t=3C_xMp2ivLn6ssruDm38I7LvfoH-67a8VkCcMPI0Mo8" alt="4 Embedding Models Embedding models are powerful machine learning techniques that simplify high-dimensional data into lower-dimensional space, while preserving essential features. Crucial in natural language processing (NLP), they transform sparse word representations into dense vectors, capturing semantic similarities between words. Embedding models also process images, audio, video, and structured data, enhancing applications in recommendation systems, anomaly detection, and clustering. Here is an example of an embedding model in action. Suppose the full plot in a database of movies has been previously embedded using OpenAI’s text-embedding-ada-002 embedding model. Your goal is to find all movies and animations for Guardians of the Galaxy, but not by traditional phonetic or lexical matching (where you would type some of the words in the title). Instead, you will search by semantic means, say, the phrase Awkward team of space defenders. You will then use the same embedding model again to embed this phrase and query the embedded movie plots. Table 4.1 shows an excerpt of the resulting embedding: Dimension 1 2 3 ... 1535 1536 Value 0.00262913 0.031449784 0.0020321296 ... -0.01821267 0.0014683881 Table 4.1: Excerpt of embedding"></li><li class="carousel-slide" data-ssplayer-slide-index="48" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/49/1739580232671?e=1743033600&amp;v=beta&amp;t=i-3zO4MpNuRyTirkRokyYy0BgIzBD0a2WJUP21edJHM" alt="48 Embedding Models To successfully execute the example in this chapter, you will need a MongoDB Atlas Vector Index created on the MongoDB Atlas cluster. The index name must be text_vector_index, created on the embeddings.text collection as follows: This chapter will help you understand embedding models in depth. You’ll also implement an example using the Python language and the langchain-openai library. This chapter will cover the following topics: • Differentiation between embedding models and LLMs • Types of embedding models • How to choose an embedding model • Vector representations To follow the examples in this chapter, you will need the following prerequisites: • A MongoDB Atlas cluster. An Atlas M0 free cluster should be sufficient as you will store a small set of documents and create only one vector index. • An OpenAI account and API key with access to the text-embedding-3-large model. • A Python 3 working environment. You will also need to have installed Python libraries for MongoDB, LangChain, and OpenAI. You can install these libraries in your Python 3 environment as follows: Technical requirements { &quot;fields&quot;: [ } { &quot;numDimensions&quot;: 1024, &quot;path&quot;: &quot;embedding&quot;, &quot;similarity&quot;: &quot;cosine&quot;, &quot;type&quot;: &quot;vector&quot; } ] %pip3 install --upgrade --quiet pymongo pythondns langchain langchain- community langchain-mongodb langchain-openai"></li><li class="carousel-slide" data-ssplayer-slide-index="49" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/50/1739580232671?e=1743033600&amp;v=beta&amp;t=oPxCay0BNLfJvOBAwLZtt_d4MnTo3rRdBDUz1oUpeKE" alt="Figure 4.1: An embedding model example for a library use case What is an embedding model? 49 What is an embedding model? Embedding models are a type of tool used in machine learning and artificial intelligence that simplifies large and complex data into a more manageable form. This process, known as embedding, involves reducing the data’s dimensions. Imagine going from a detailed world map with highways, railroads, rivers, trails, and so on, to a simpler, summarized version with only country boundaries and capital cities. This not only makes computation faster and less resource-intensive, but also helps identify and understand relationships within the data. Because embedding models streamline the processing and analyzing of largedatasets, they are particularly useful in areas of language (text) processing, image and sound recognition, and recommendation systems. Consider a vast library where each book stands for one point in high dimensions. Embedding models can help reorganize the library to improve ease of navigation, such as by grouping the books onrelated topics closer together and reducing the library’s overall size. Figure 4.1 illustrates this concept: This conversion or reduction from a high-dimensional or original representation to a lower-dimensional representation created the basis for advancements in NLP, computer vision, and more. Book on Topic B Closely related Book on Topic A High-dimensional library Lower-dimensional library Embedding model Book on Topic C Less related"></li><li class="carousel-slide" data-ssplayer-slide-index="50" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/51/1739580232671?e=1743033600&amp;v=beta&amp;t=-AVnsviODC_aAzWf2MVKue8fdXk7_hen0LhH3BH_r5k" alt="50 Embedding Models Figure 4.2: 2D visualization of contrastive loss and positive and negative sampling How do embedding models differ from LLMs? Embedding models are specialized algorithms that reduce high-dimensional data (such as text, images, or sound) into a low-dimensional space of dense vectors. On the other hand, LLMs are effective artificial neural networks pre-trained on gigantic corpora of textual data. While both are rooted in neural networks, they employ distinct methodologies. LLMs are designed for generating coherent and contextually relevant text. LLMs leverage massive amounts of data to understand and predict language patterns. Their basic building blocks include transformer architectures, attention mechanisms, and large-scale pre-training followed by fine-tuning. In contrast, embedding models focus on mapping words, phrases, or even entire sentences into dense vector spaces where semantic relationships are preserved. They often use techniques such ascontrastive loss, which helps in distinguishing between similar and dissimilar pairs during training. Positive and negative sampling is another technique employed by embedding models. Positive samples aresimilar items (such as synonyms or related sentences), while negative samples are dissimilar items (such as unrelated words or sentences). Figure 4.2 visualizes an example of contrastive loss and positive and negative sampling in 2D space. This sampling aids the model in learning meaningful representations by minimizing the distance between positive pairs and maximizing the distance between negative pairs in the vector space. To summarize, while LLMs excel in language generation tasks, embedding models are optimized for capturing and leveraging semantic similarities. Both enhance NLP by enabling machines to grasp and produce human language more effectively. Now, let’s look at an example of each. king [0.106, 0.265, ...] man [0.175, 0.765, ...] Contrastive loss: similar and dissimilar pairs during training queen [0.281, 0.435, ...] woman [0.293, 0.675, ...] cat [0.53, 0.575, ...] king [0.106, 0.265, ...] queen [0.281, 0.435, ...] lynx [0.53, 0.575, ...] Positive and negative sampling: Positive samples are similar. Negative samples are dissimilar man [0.175 0.765, ...] woman [0.293, 0.675, ...]"></li><li class="carousel-slide" data-ssplayer-slide-index="51" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/52/1739580232671?e=1743033600&amp;v=beta&amp;t=rge88frWIi6ICa_oj-tr_fLRI_PD9gOT3B6qeWczQHM" alt="What is an embedding model? 51 Embedding models are used in scenarios where the goal is to capture and leverage the relationships within data. They are the ideal choice for the following tasks: • Semantic similarity: Finding or recommending items (such as documents or products) that are like a given item. • Clustering: Grouping entities based on their semantic properties. • Information retrieval: Enhancing search functionalities by understanding the semantic content of queries. LLMs are the go-to for tasks that require text understanding, generation, or both, such as the following: • Content creation: Generating text that is coherent, contextually relevant, and stylistically appropriate. For example, generating a synopsis from the full plot of a movie. Conversational AI: Building chatbots and virtual assistants that can understand and engage in human-like dialogue, such as answering questions about employment policies and employee benefits. • • Language translation: The extensive training on language-diverse datasets allows LLMs to handle idiomatic expressions, cultural nuances, and specialized terminology. Embedding models and LLMs both play crucial roles in AI. Embedding models capture and manipulate semantic properties compactly, while LLMs excel in generating and interpreting text. Using both, and selecting the right embedding models based on your goals, can unlock AI’s full potential in your projects. Word-level models, including Global Vectors for Word Representation (GloVe) and Bidirectional Encoder Representations from Transformers (BERT), capture broader textual meanings. Specialized models such as fastText adapt to linguistic challenges. All of these reflect the evolving landscape of embedding models. Word2vec (developed by Google) transforms words into vectors and discerns semantic relationships, such as “king” is to “man” as “queen” is to “woman.” It’s useful for sentiment analysis, translation, and content recommendations, enhancing natural language understanding for machines. GPT-4 (developed by OpenAI) is an LLM that is characterized by its ability to generate human-like text based on the input it receives. GPT-4 excels in a range of language-based tasks, includingconversation, content generation, summarization, and translation. Its architecture allows it to comprehend the intricate details and nuances of language, enabling it to perform tasks that require a deep understanding of context, humor, irony, and cultural references. Types of embedding models When to use embedding models versus LLMs"></li><li class="carousel-slide" data-ssplayer-slide-index="52" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/53/1739580232671?e=1743033600&amp;v=beta&amp;t=BjnQfrOQ1ADKbUu9oUsrodqWHM-vsU-ZlFQJ56_csMg" alt="52 Embedding Models In this section, you will explore many types of embedding models: word, sentence, document, contextual, specialized, non-text, and multi-modal. Word embeddings Word embedding models capture semantic meanings based on context within extensive text corpora. One common approach involves a neural network that learns word associations either by predicting a word from its surrounding context or vice versa. Another method combines matrix factorization with context window techniques to generate embeddings by summarizing word co- occurrence frequencies in large matrices. A further enhancement treats each word as a collection of character n-grams (a sequence of n adjacent symbols in a particular order), which helps to better handle prefixes, suffixes, and rare words. Word2vec and GloVe are examples of these models. Word2vec was the first attempt of embedding models to learn the representation of words as vectors based on their contextual similarities. Developed by a team from Google, it uses two architectures: Continuous Bag of Words (CBOW), which predicts a word given a context, and skip-gram, which predicts a context for a given word. Word2vec has been seen to capture the relationship in the syntax of words, evidenced by its ability to deduce meanings from arithmetic operations performed with word vectors. GloVe, developed at Stanford University, merges the benefits of two leading word representation approaches: global matrix factorization with co-occurrence statistics and context window methods. By constructing a co-occurrence matrix from the corpus and applying dimensionality reduction techniques, GloVe captures both global statistics and local context, which is invaluable for tasks that require a deep understanding of word relationships. Sentence and document embeddings Sentence and document embedding models capture the overall semantic meaning of text blocks by considering word context and arrangement. A common approach aggregates word vectors into a coherent vector for the whole text unit. These models are useful in document similarity, information retrieval, and text summarization, such as synopses versus full movie plots. Notable models include Doc2vec and BERT. Building on Word2vec, Doc2vec, which is also known as Paragraph Vector, encapsulates whole sentences or documents as vectors. Introducing a document ID token that allows the model to learn document-level embeddings alongside word embeddings aids significantly in tasks such as document classification and similarity comparison. Google’s BERT employs context-aware embeddings, reading the entire sequence of words concurrently, unlike its predecessors that processed text linearly. This approach enables BERT to understand aword’s context from all surrounding words, resulting in more dynamic and nuanced embeddings and setting new standards across various NLP tasks."></li><li class="carousel-slide" data-ssplayer-slide-index="53" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/54/1739580232671?e=1743033600&amp;v=beta&amp;t=rHhk78bQ7Fb9qTo4z_AO8tviHr6ph-Zwcw9kDnEW1gA" alt="What is an embedding model? 53 Contextual embeddings Contextual embedding models are designed to produce word vectors that vary according to the context of use in a sentence. These models use deep learning architectures by examining the whole sentence, or at times the surrounding sentences. The contextual model produces dynamic embeddings that capture nuances based on a word’s particular context and linguistic environment. A model architecture of this kind uses a bi-directional framework to process text both forward and in reverse, thereby capturing fine semantic and syntactic dependencies within the preceding and following contexts. They are useful in sentiment analysis (such as to interpret the tone of the text in an IT support ticket) and question- answering tasks where the exact meaning of words for interpretation is necessary. ELMo and GPT are two examples. Embeddings from Language Models (ELMo) introduced dynamic, context-dependent embeddings, producing variable embeddings based on a word’s linguistic context. This approach greatly enhances performance on downstream NLP tasks by providing a richer language understanding. OpenAI’s GPT series leverages transformer technology to offer embeddings pre-trained on extensive text corpora and fine-tuned for specific tasks. GPT’s success underscores the efficacy of combining large-scale language models with transformer architectures in NLP. Specialized embeddings Specialized embedding models capture specific linguistic properties, such as places, people, tone, and mood, in vector space. Some are language- or dialect-specific, while others analyze sentiment and emotional dimensions. Applications include legal document analysis, support ticket triage, sentiment analysis in marketing, and multilingual content management. fastText is an example of a specialized embedding model. Developed by Facebook’s AI Research lab, fastText enhances Word2vec by treating words as bags of character n-grams, which provesparticularly helpful for handling out-of-vocabulary (OOV) words. OOV words are words not seen during training and thus lack pre-learned vector representations, posing challenges for traditional models. fastText enables embeddings for OOV words through the summation of their sub-word embeddings. This makes it especially suitable for handling rare words and morphologically complex languages, which are languages with rich and varied word structures that use extensive prefixes, suffixes, and inflections to convey different grammatical meanings, such as Finnish, Turkish, and Arabic. Other non-text embedding models Embedding models go beyond converting only text to vector representations. Images, audio, video, and even JSON data itself can be represented in vector form: • Images: Models such as Visual Geometry Group (VGG) and Residual Network (ResNet) set benchmarks for the translation of raw images into dense vectors. These models capture important visual features, such as edges, textures, and color gradients, which are vital to many computer vision tasks, including image classification and object recognition. VGG works well at recognizing visual patterns, while ResNet improves accuracy in complex image-processing tasks, such as image segmentation or photo tagging."></li><li class="carousel-slide" data-ssplayer-slide-index="54" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/55/1739580232671?e=1743033600&amp;v=beta&amp;t=2OfyjJFSGHKI48A_sRLN1RQkJUSre3EBn0QIg3qnXQI" alt="54 Embedding Models • Audio: OpenL3 and VGGish are models for audio. OpenL3 is a model adapted from the L3-Net architecture that is used in audio event detection and environmental sound classification to embed audio into a temporal and spectral context-rich space. VGGish is born out of the VGG architecture for images, and so follows the same principle of converting sound waves into patterns of small, compact vectors. This simplifies tasks such as recognition of speech and music genres.• Video: 3D Convolutional Neural Networks (3D CNNs or 3D ConvNets) and Inflated 3D (I3D) expand the capabilities of image embeddings in perceiving the temporal dynamics paramount to both action recognition and for video content analysis. 3D ConvNets apply convolutional filters in three dimensions (height, width, time) capturing spatial and temporal dependencies in volumetric data, making them particularly effective for spatiotemporal data, such as video analysis, medical imaging, and 3D object recognition. I3D uses a spatiotemporal architecture that combines the outputs of two 3D ConvNets: one processes RGB frames, while the other handles optical flow predictions between consecutive frames. I3D models are useful for sports analytics and surveillance systems. • Graph data: Node2vec and DeepWalk capture connectivity patterns of nodes within a graph and are applied in the domains of social network analysis, fraud detection, and recommendation systems. Node2vec learns continuous vector representations for nodes by performing biased random walks on the graph. This captures the diverse node relationships and community structures, improving the performance of tasks such as node classification and link prediction. DeepWalk treats random walks as sequences of nodes like sentences in NLP by capturing the structural relationships between nodes and encodes them into continuous vector representations, which can be used for node classification and clustering. • JSON data: There are even JSON data embedding models, such as Tree-LSTM, which is a variation of the traditional long short-term memory (LSTM) networks, adapted specifically to handle data with a hierarchical tree structure, such as JSON. Unlike standard LSTM units that process data sequentially, Tree-LSTM operates over tree-structured data by incorporating states from multiple child nodes into a parent node, effectively capturing the dependencies in nested structures. This makes it particularly suitable for tasks such as semantic parsing and sentiment analysis, where understanding the hierarchical relationships within data can significantly improve performance. json2vec is an implementation of this kind of embedding model. After single-mode models, you can explore multi-modal models. These analyze multiple data types simultaneously and are crucial for applications such as autonomous driving, where merging data from sensors, cameras, and LiDAR builds a comprehensive view of the driving environment."></li><li class="carousel-slide" data-ssplayer-slide-index="55" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/56/1739580232671?e=1743033600&amp;v=beta&amp;t=qdhBGS3PrDMNyk4G2BF0fe4r9S47jkRfiYyFrsGg2w8" alt="Choosing embedding models 55 Multi-modal models Multi-modal embedding models process and integrate information from many types of data sources into a unified embedding space. This approach is incredibly useful when different modalities complement or reinforce each other and together can lead to better AI applications. Multi-modal models are excellent for in-depth comprehension of multisensory input content, such as the tasks of multi-media search engines, automated content moderation, and interactive AI systems that can engage the user via visual and verbal interaction. Here are a few examples: • CLIP: A well-known multi-modal model by OpenAI. It learns how to correlate visual images with textual descriptions in such a way that it can recognize images it has never seen during training, based on natural language queries. • LXMERT: A model that focuses on processing both visual and text inputs. It can improve the performance of tasks such as answering questions with a visual aspect, which includes object detection. ViLBERT: Vision-and-Language BERT (ViLBERT) extends the BERT architecture to process both visual and textual inputs simultaneously by using a two-stream model where one stream handles visual features extracted from images using a pre-trained convolutional neural network (CNN or ConvNet), and the other processes textual data with cross-attention layers facilitating interaction between the two modalities. ViLBERT is used for tasks such as visual question answering and visual commonsense reasoning, where understanding image-text relationships is essential. • • VisualBERT: Integrates visual and textual information by combining image features with contextualized word embeddings from a BERT-like architecture. It is commonly used for tasks such as image-text retrieval and image captioning, where aligning and understanding both visual and textual information are essential. You have now explored word, image, and multi-modal embeddings. Next, you’ll learn how to choose embedding models based on your application’s needs. Choosing embedding models Embedding models impact an application’s performance, its ability to understand language and other forms of data, and ultimately, a project’s success. The following sections look at the parameters for choosing the right embedding model that aligns with the task requirements, characteristics of your dataset, and computational resources. This section explains vector dimensionality and model leaderboards as additional information to consider when choosing embedding models. For a quick overview of this section, you can consult Table 4.2."></li><li class="carousel-slide" data-ssplayer-slide-index="56" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/57/1739580232671?e=1743033600&amp;v=beta&amp;t=0VDbo4Pn98VQM8S-k-A2Jk3vLcRMR72zqsS67T5Jiqk" alt="56 Embedding Models Task requirements Each type of task may benefit from different embedding models based on how they process and represent text data. For instance, tasks such as text classification and sentiment analysis often require a deep understanding of semantic relationships at the word level. Word2vec or GloVe are particularly beneficial in these cases, as they provide robust word-level embeddings that capture semantic meanings. For more complex linguistic tasks such as named entity recognition (NER) and part-of-speech (POS) tagging, the ability to understand the context in which a word is used becomes critical. Here, models such as BERT or ELMo show their strengths as they generate embeddings that vary dynamically based on the surrounding text, providing a richer and more precise understanding of each word’s role within a sentence. This deep contextual awareness is essential for accurately identifying entities and tagging parts of speech, as it allows the model to differentiate between words with multiplemeanings based on their usage. Advanced models such as BERT, GPT, and Doc2vec are ideal for tasks requiring nuanced language understanding, such as question answering, machine translation, document similarity, and clustering. These models handle complex dependencies within text, making them suitable for analyzing entire documents. Doc2vec excels in comparing thematic similarities between documents, like finding similar news or sports articles.Dataset characteristics When choosing an embedding model, consider the dataset’s size and characteristics. For morphologically rich languages or datasets with many OOV words, models such as fastText, which capture sub-word information, are advantageous. They handle new or rare words effectively. For texts with polysemous words (words with multiple meanings), contextual embeddings such as ELMo or BERT are essential, as they provide dynamic, context-specific representations. The dataset size influences the choice of embedding model. Larger datasets benefit from complex models such as BERT, GPT, and OpenAI’s text-embedding-3-large, which capture deep linguistic nuances but require substantial computational power. Smaller datasets might benefit from simpler models such as text-embedding-3-small, offering robust performance with less computational demand. This ensures even modest datasets can yield significant insights with the appropriate model. Computational resources Computational cost is crucial when selecting an embedding model due to varying resource demands. Larger models such as GPT-4 require extensive computational power, making them less accessible to smaller organizations or projects with limited budgets. Choosing a lightweight model or fine-tuning one for specific tasks can reduce computational needs, speed up development, and improve response times. Efficient models are essential for real-timetasks such as translation, speech recognition, and instant recommendations in gaming, media streaming, and e-commerce."></li><li class="carousel-slide" data-ssplayer-slide-index="57" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/58/1739580232671?e=1743033600&amp;v=beta&amp;t=drwFBvfLOU-5P-pU01lbKAqTIyZGpX9gK7XL7vliJbY" alt="Figure 4.3: Penultimate layer of a neural network Choosing embedding models 57 Some level of iterative experimentation helps identify the most suitable models. Staying updated on the latest developments is critical, as newer models frequently supersede older ones. Model leaderboards can help track advancements in the field and are covered later in this section. The size of a vector in an embedding model affects its ability to capture data complexity. Large vectors encode more information, allowing finer distinctions, but require more computation. Small vectors are more efficient but might miss subtle nuances. Choosing a vector size involves balancing detailed representation with practical constraints like memory and speed. Why do vector dimensions matter? Knowing the relationship between a vector, its size, and the second-last layer of a neural network is crucial for understanding the quality of the model’s output. The penultimate or second-last layer often serves as a feature extractor, where the dimensions of the output vector represent the learned features of the input data, as visualized in Figure 4.3. The size of this vector directly influences the granularity of the representation. Vector representations Input Hidden layers Output"></li><li class="carousel-slide" data-ssplayer-slide-index="58" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/59/1739580232671?e=1743033600&amp;v=beta&amp;t=uHCLuRmRiEeRA2Ob2Q8lIVdgPwHQNuvBaIhm4UAzbUQ" alt="58 Embedding Models Figure 4.4: Typical data flow for embedding source data into the vector store and query vectors To obtain these vectors, the output layer (the last layer) of the neural network is removed, and the output from the preceding layer—the penultimate or second-last layer—is captured. Typically, the final layer outputs the model’s prediction, prompting the use of the output from the layer just before it. The data that is fed into the network’s predictive layer is known as vector embedding. The dimensionality of a vector embedding aligns with the size of the penultimate layer of the underlying neural network of the model being used, making it synonymous with the vector’s size or length. Dimensionalities such as 384 (by SBERT’s all-MiniLM-L6-v2), 768 (by SBERT’s all-mpnet- base-v2), 1,536 (by OpenAI’s text-embedding-ada-002), and 2,048 (from ResNet-50 by Microsoft Research) are common. Larger vectors are becoming available now, such as 3,072 by OpenAI’s text- embedding-3-large. What does a vector embedding mean, and how is it typically used? Vector embeddings are the output of an embedding model, expressed as an array of floating-point numbers that typically range from –1.0 to +1.0. Each position in the array represents a dimension. Vector embeddings play a key role in context-retrieval use cases, such as semantic search in chatbots. Data is embedded and stored in a vector database upfront, and queries must use the sameembedding model for accurate results. Each embedding model produces unique embeddings based on its training data, making them specific to the model’s domain and not interchangeable. For example, the embedding obtained from a model trained on full documents of legal text will differ from one trained on healthcare data for patient history. You may recall the example of trying to find movies for Guardians of the Galaxy at the beginning of this chapter. You now understand why you had to embed the search string (which is also called the query vector) using the same embedding model. This workflow, common in AI applications, is explained in Figure 4.4: Audio Images Documents Transform into embedding Vector representation Dense vectors Results Nearest neighbour Vector representation Transform into embedding Query"></li><li class="carousel-slide" data-ssplayer-slide-index="59" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/60/1739580232671?e=1743033600&amp;v=beta&amp;t=s_y-in-Uw7ysjJ3wmagcz_7R6h5pw36Npza2C7XOI3Q" alt="Choosing embedding models 59 GloVe BERT GPT Word2vec Embedding model Embedding quality and ease of use High-quality, contextually rich embeddings. Available on TensorFlow and others, but limited availability online. Robust embeddings, especially for less frequent words. Available on TensorFlow and others, but limited availability online. Contextualized embeddings that are rich and adaptable. Available online. High-quality embeddings that excel in generative and language understanding tasks. Available online. The workflow shows the Transform into embedding step twice: one for embedding existing data into a vector database (on the left) and another for real-time embedding of the query (on the right). Both steps must use the same embedding model. Table 4.2 provides a quick overview of some of the embedding models covered in this chapter, focusing on their quality and ease of use. Each model’s description includes the quality of embeddings based on factors such as accuracy in downstream tasks and the richness of semantic representation, ease of use, documentation quality, and computational requirements. With such a variety of existing models and new models constantly evolving, how can you stay up to date? Embedding model leaderboards, such as those offered by platforms like Hugging Face, help gauge the performance of various models across numerous tasks. They provide transparent and competitive rankings of models based on criteria, such as accuracy and efficiency. By measuring models against standardized datasets and benchmark tasks, these leaderboards pinpoint state-of- the- art models and their trade-offs. The Massive Text Embedding Benchmark (MTEB) leaderboard from Hugging Face is a critical resource. It offers a comprehensive overview of the performance benchmarks of text embeddingmodels. To see which models are setting the standard, visit the Hugging Face MTEB leaderboard: https:// huggingface.co/spaces/mteb/leaderboard. You can also consult other leaderboards as you select the components of your AI/ML application architecture. Hugging Face hosts the Open LLM leaderboard (https://huggingface.co/ spaces/open-llm-leaderboard/open_llm_leaderboard) and language- specific leaderboards, such as the Open Portuguese LLM leaderboard, the Open Ko-LLM leaderboard (Korean), and the Spanish Embeddings leaderboard. There are even industry-specific leaderboards, such as the Open Medical-LLM leaderboard. Embedding models overview Embedding model leaderboards"></li><li class="carousel-slide" data-ssplayer-slide-index="60" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/61/1739580232671?e=1743033600&amp;v=beta&amp;t=eGx-Bx9Tn2KyvMM_h1m8EMpWXRAKZZ7b5R7iuzFML30" alt="60 Embedding Models Table 4.2: Embedding quality and ease of use in various embedding models fastText Embedding model Doc2vec Embedding quality and ease of use Suitable for document-level tasks; embeddings reflect broader context than word-level models. Captures OOV words effectively. Open source and remarkably lightweight. Works on standard hardware and can produce models small enough for mobile devices. High-quality embeddings for sophisticated NLP tasks, capturing nuanced context. Replaced OpenAI’s text-embedding-ada-002. Can produce smaller vectors while maintaining high embedding quality. Good-quality embeddings for standard NLP tasks, balancing performance and computational requirements. While this comparison should serve as a guide to selecting the most suitable embedding model for specific needs, the MTEB leaderboard mentioned previously, as well as online documentation, should always be consulted given the fast-moving development in this space. No, you don’t need an embedding model always. Not all situations call for the intricate details of an embedding model to represent data in the required vector form. For some applications, more straightforward vectorization methods are entirely adequate. In some cases, complex public embedding models or bespoke models are unnecessary. Tasks with narrow focus, clear rules, or structured data can thrive on simple vector representations. This approach suits straightforward clustering, precise similarity measurements, and situations with limited computing power. For instance, one-hot encoding is a straightforward technique that turns categorical data into binary vectors, fitting perfectly for cases where categories are nominal without any intrinsic order. Similarly, term frequency-inverse document frequency (TF-IDF) vectors adeptly convey text significance for information retrieval and ranking tasks by highlighting the relevance of terms within documents in relation to the whole corpus. These alternatives may lack the semantic depth of embedding models but provide computational efficiency and simplicity for tasks where intricate context isn’t required. Opting for simple vector representations enhances transparency, reduces computational demands or advanced scientific skill, and is ideal for swift performance or resource-limited environments, such as embedded systems or mobile devices. With your understanding of embedding models established, you can now move on to a practical demonstration using Python, LangChain, MongoDB Atlas, and OpenAI. text- embedding- 3- large text- embedding-3- small Do you always need an embedding model?"></li><li class="carousel-slide" data-ssplayer-slide-index="61" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/62/1739580232671?e=1743033600&amp;v=beta&amp;t=V4oxx56FwlN9gZjdx8P8jYX5unukN3-tVB0ThDJNLHg" alt="Choosing embedding models 61 Executing code from LangChain Now that you have explored the diverse types of embedding models, you will see what it is like to use them with working code. The following Python script (named semantic_search.py) uses the langchain-openai library to embed textual data with OpenAI’s text-embedding-3-large model, tailored to produce 1,024 dimensional vectors versus 3,072: embedding_model = OpenAIEmbeddings( model=&quot;text-embedding-3-large&quot;, dimensions=1024, disallowed_special=() ) embeddings = embedding_model.embed_documents(texts) import os, pprint, time from langchain_mongodb import MongoDBAtlasVectorSearch from langchain_openai import OpenAIEmbeddings from pymongo import MongoClient os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;YOUR-OPENAI-API-KEY&quot; MONGODB_ATLAS_CONNECTION_STRING = &quot;YOUR-MONGODB_ATLAS-CONNSTRING&quot; client = MongoClient(MONGODB_ATLAS_CONNECTION_STRING, tls=True, tlsAllowInvalidCertificates=True) db_name = &quot;embeddings&quot; collection_name = &quot;text&quot; coll = client[db_name][collection_name] vector_search_index = &quot;text_vector_index&quot; coll.delete_many({}) texts = [] texts.append(&quot;A martial artist agrees to spy on a reclusive crime lord using his invitation to a tournament there as cover.&quot;) texts.append(&quot;A group of intergalactic criminals are forced to work together to stop a fanatical warrior from taking control of the universe.&quot;) texts.append(&quot;When a boy wishes to be big at a magic wish machine, he wakes up the next morning and finds himself in an adult body.&quot;)"></li><li class="carousel-slide" data-ssplayer-slide-index="62" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/63/1739580232671?e=1743033600&amp;v=beta&amp;t=j65I8XmLS5vp5TdVoCP9bIEI_gqaxk9IiYmi3s4e4jA" alt="62 Embedding Models docs = [] for i in range(len(texts)): docs.append( { &quot;text&quot;: texts[i], &quot;embedding&quot;: embeddings[i] coll.insert_many(docs) print(&quot;Documents embedded and inserted successfully.&quot;) time.sleep(3) # allow vector store (Atlas) to undergo indexing semantic_queries = [] semantic_queries.append(&quot;Secret agent captures underworld boss.&quot;) semantic_queries.append(&quot;Awkward team of space defenders.&quot;) semantic_queries.append(&quot;A magical tale of growing up.&quot;) vector_search = MongoDBAtlasVectorSearch( } ) collection= coll, embedding= OpenAIEmbeddings( model=&quot;text-embedding-3-large&quot;, dimensions=1024, disallowed_special=()), ) index_name= vector_search_index for q in semantic_queries: results = vector_search.similarity_search_with_score( query = q, k = 3 ) p r i n t ( &quot; S E M A N T I C Q U E R Y : &quot; + q ) p r i n t ( &quot; R A N K E D R E S U L T S : &quot; ) p p r i n t . p p r i n t ( r e s u l t s ) p r i n t ( &quot; &quot; )"></li><li class="carousel-slide" data-ssplayer-slide-index="63" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/64/1739580232671?e=1743033600&amp;v=beta&amp;t=SUUeS6l8xL9ckO5yw1C5hjXLakm_AWgk_AGsrGlIj6Q" alt="Choosing embedding models 63 The console output will be as follows: (myenv) % python3 semantic_search.py 0 1 2 Documents embedded and inserted successfully. SEMANTIC QUERY: Secret agent captures underworld boss. RANKED RESULTS: [(Document(metadata={'_id': '66aada5537ef2109b3058ccb'}, page_content='A martial artist agrees to spy on a reclusive crime lord using his invitation to a tournament there as cover.'), 0.770392894744873), (Document(metadata={'_id': '66aada5537ef2109b3058ccc'}, page_content='A group of intergalactic criminals are forced to work together to stop a fanatical warrior from taking control of the universe.'), 0.6555435657501221), (Document(metadata={'_id': '66aada5537ef2109b3058ccd'}, page_content='When a boy wishes to be big at a magic wish machine, he wakes up the next morning and finds himself in an adult body.'), SEMANTIC QUERY: Awkward team of space defenders. RANKED RESULTS: [(Document(metadata={'_id': '66aada5537ef2109b3058ccc'}, page_content='A group of intergalactic criminals are forced to work together to stop a fanatical warrior from taking control of the universe.'), 0.5847723484039307)] 0.7871642112731934), (Document(metadata={'_id': '66aada5537ef2109b3058ccb'}, page_content='A martial artist agrees to spy on a reclusive crime lord using his invitation to a tournament there as cover.'), 0.6236412525177002), (Document(metadata={'_id': '66aada5537ef2109b3058ccd'}, page_content='When a boy wishes to be big at a magic wish machine, he wakes up the next morning and finds himself in an adult body.'), SEMANTIC QUERY: A magical tale of growing up. RANKED RESULTS: [(Document(metadata={'_id': '66aada5537ef2109b3058ccd'}, page_content='When a boy wishes to be big at a magic wish machine, he wakes up the next morning and finds himself in an adult body.'), 0.5492569208145142)] 0.7488957047462463), (Document(metadata={'_id': '66aada5537ef2109b3058ccb'}, page_content='A martial artist agrees to spy on a reclusive crime lord using his invitation to a tournament there as cover.'), 0.5904781222343445),"></li><li class="carousel-slide" data-ssplayer-slide-index="64" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/65/1739580232671?e=1743033600&amp;v=beta&amp;t=yXw8F02__kesceUBgQCtBtf9G_lY3HcfL4fW-Hz2CXk" alt="64 Embedding Models (Document(metadata={'_id': '66aada5537ef2109b3058ccc'}, page_content='A group of intergalactic criminals are forced to work together to stop a fanatical warrior from taking control of the universe.'), 0.5809941291809082)] The example sets up the environment, authenticating to OpenAI with API keys, and connecting to MongoDB Atlas. Plots for three movies are then embedded and stored in MongoDB Atlas (the vector store) and different vector searches are then executed to demonstrate semantic search with ranked results. This chapter covered the realm of embedding models, which are essential tools in AI/ML applications. They facilitate the transformation of high-dimensional data into a more manageable, lower-dimensional space. This process, known as embedding, significantly boosts computational efficiency and enhances the ability to describe and quantify relationships within data. Selecting the right embedding models for different types of data, such as text, audio, video, images, and structured data, is essential for expanding the reach of use cases and different workloads. The chapter also highlighted the importance of consulting leaderboards to gauge the effectiveness across the vast list of available models and the delicate balance necessary when choosing vectorsizes, emphasizing the trade-offs between detail, efficiency, performance, and cost. While embedding models provide deep, contextual insights, simpler vectorization methods might be adequate for certain tasks. The next chapter will delve into aspects of vector databases, examining the role of vector search in AI/ML applications with use cases. Selecting the most appropriate embedding models and vector size is not merely a technical decision, but a strategic one that aligns with the unique characteristics, technical and organizational constraints, and objectives of your project. Maintaining computational efficiency and cost is another cornerstone of effectively using embedding models. As some models can be resource-intensive and have higher response times and higher cost, optimizing the computational aspects without sacrificing the quality of the output is essential. Designing your system to use different embedding models depending on the task at hand will yield a more resilient application architecture. It’s imperative to regularly evaluate your embedding model to ensure your AI/ML application continues to perform as expected. This involves routinely checking performance metrics and making necessary adjustments. Tweaking your model usage could mean altering vector sizes to avoid overfitting— where the model is too finely tuned to training data and performs poorly on unseen data. It is essential to monitor vector search response times versus the embedding models being used and vector sizes, as these impact the user experience of AI-driven applications. Also consider the costs of maintaining and updating embedding models, including monetary, time, and resource expenses for re-embedding data. Planning for these helps make informed decisions on when updates are needed and balancing performance, cost-efficiency, and technological advancement.Summary Best practices"></li><li class="carousel-slide" data-ssplayer-slide-index="65" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/66/1739580232671?e=1743033600&amp;v=beta&amp;t=TnOZgRnA7IPmn6De0Gsm1gS3It66vCsFzFus4xPLKvQ" alt="5 Vector Databases Sometimes, data is rich with information and has a well-defined structure. If you know what you want, then this data is straightforward to work with in a modern database system. However, you often don’t know exactly what you need. Without specific search terms or phrases, you may not receive optimal search results. For example, you might not know the brand or name of your picky pet’s favorite food. In such complex cases, traditional information search and retrieval methods can fall short. Modern AI research has given rise to a new class of methods that can encode the underlying semantic meaning of something instead of just its raw data. For example, AI models can understand that when you ask for , you’re asking for the latest John Wick film, which stars Keanu Reeves, who was also the star of The Matrix films. To achieve this result, these methods convert their inputs into a numerical format called a vector embedding. Vector databases provide a means to efficiently store, organize, and search these vector representations. This makes vector databases valuable tools for retrieval tasks, which are common in AI applications. In this chapter, you will learn about vector search, the key concepts and algorithms associated with it, and the significance of vector databases. By the end of this chapter, you will understand the workings of graph connectivity and its application in architecture patterns such as RAG. You will also understand the best practices for building vector search systems. the new action movie with that one actor who was also in the movie with green falling numbers"></li><li class="carousel-slide" data-ssplayer-slide-index="66" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/67/1739580232671?e=1743033600&amp;v=beta&amp;t=bDWtfqXGyjcX-S3UBQ6pZ-JsF2JoHo1_JdnLO1mjq2c" alt="66 Vector Databases This chapter will cover the following topics: • Vector embeddings and similarity • Nearest neighbor vector search • • • The need for vector databases Case studies and real-world applications Vector search best practices While not required, it may help to have some familiarity with graph data structures and operations. You may also want to know about the embedding models that are used to create vectors, which are discussed in more detail in Chapter 4, Embedding Models. What is a vector embedding? At the most basic level, a vector is a list of numbers plus an implicit structure that determines how those numbers are defined and how you can compare them. The number of elements in a vector is the vector’s dimension. Dimensions represent different aspects of the thing that they describe. You might think of a list of properties that describe a car and list them out in a structured way such that the order is always [year, make, model, color, mileage]. These properties form a vector space that can describe any car for which these properties hold. For example, you could describe a specific car with these values as [2000, Honda, Accord, Gold, 122000]. This is a useful model for building intuition on how vectors can encode information. However, each element may not always correspond to a concrete idea with a numerable set of possible values. The vectors used in AI applications are more abstract and have significantly more dimensions. In a way, they smear concrete ideas across many dimensions and standardize to a single set of possible values for every dimension. For example, vectors from OpenAI’s text-embedding-ada-002 model always have 1,536 elements, and each element is a floating-point number between -1 and 1. The vectors used in AI applications are the output of embedding models. These are machine learning (ML) models that are pre-trained to convert inputs, typically a string of text tokens, into vectors that encode the semantic meaning of the input. For humans, the many dimensions of these vectors are basically impossible to decipher. However, the embedding model learns an implicit meaning for every dimension during training and can reliably encode that meaning for its inputs. The exact structure of the vectors varies between embedding models, but a specific model always outputs vectors of the same size. To use a vector, it’s imperative to know which model created it. Technical requirements"></li><li class="carousel-slide" data-ssplayer-slide-index="67" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/68/1739580232671?e=1743033600&amp;v=beta&amp;t=Z99yLg3z3vTWXhb94BZFAV7hgZgwJnWF6TJ9yb-6pQc" alt="Figure 5.1: A comparison of vector measurements What is a vector embedding? 67 Vector similarity Beyond storing high-dimensional vector data, vector databases also support various operations that let you query and search for the vectors. The most common operation is nearest neighbor search, which returns a list of stored vectors that are most similar to an input query vector. Common search interfaces are familiar territory. For instance, e-commerce searches often prioritize products relevant to your query, even if they aren’t exact matches. Nearest neighbor search uses the semantic nature of embedding model vectors to make finding similar vectors the same as finding relevant results. But what does it mean for two vectors to be similar? In short, similar vectors are close together, which you can measure as a distance. There are many ways to define distance, including some that become more relevant in higher dimensions. It’s not possible to visualize how distance works for high-dimensional vectors but it’s straightforward to see how the ideas work for small vectors and then scale them up. If you think back to geometry class, you’ll remember that you can find the distance between two use the coordinate vectors using the distance formula. For example, 2D coordinates such as distance formula . It also works for 3D coordinates, where the formula has another component for the extra dimension: . This pattern generalizes to any number of dimensions and is referred to as the Euclidean distance between two n-dimensional points. In theory, you can also use Euclidean distance to measure distances between high-dimensional vectors such as those used in AI applications. Practically, however, the usefulness of Euclidean distance breaks down as you continue to increase the number of dimensions. This pattern of intuitions and tools that work in small dimensions breaking down at higher dimensions is common and often referred to as the curse of dimensionality. Instead of Euclidean distance, most applications use a different distance metric called cosine similarity. Unlike Euclidean distance, which measures the space between the tips of two vectors, cosine similarity uses a different formula that measures the size of the angle between two vectors that share a common base. It effectively determines whether two vectors are identical, completely unrelated, or (most likely) somewhere in between in a mathematically precise way, as shown in Figure 5.1. Similar vectors point in almost the same direction, unrelated vectors are orthogonal, and opposite vectors point in opposite directions. (x, y) distance(a, b) = sqrt((a_x - b_x)**2 + (a_y - b_y)**2) sqrt((a_x - b_x)**2 + (a_y - b_y)**2 + (a_z - b_z)**2) Similar Unrelated A Opposite A B B A B"></li><li class="carousel-slide" data-ssplayer-slide-index="68" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/69/1739580232671?e=1743033600&amp;v=beta&amp;t=ZNj0tRmMzdm0THwjO0PK8MVe5S81FwswZw7_IOQPf3g" alt="68 Vector Databases Cosine similarity equips you with a tool to measure the distance between two vectors. Due to the nature of how vector embeddings carry semantic information, it’s also a tool to measure how related or relevant two vectors are to one another. If you extend this idea to more vectors, you can figure out how related a given vector is to any of the others and even rank them all by relevance. This is the core idea behind vector search algorithms. The process of comparing many vectors in this way brings its own complexity and challenges. To deal with them, search providers have developed various approaches to nearest neighbor search that balance trade-offs and optimize for different use cases. The next section will discuss two approaches to handle real search use cases. Sometimes, your use case requires that searches return only the true nearest neighbors. For example, think about an authentication app that stores biometric information about its users as embedded vectors so that they can identify themselves later. When they scan their fingerprint or face, the app creates a vector embedding of the scanned data and uses it as the query vector in a nearest neighbor search. An app like this should never misidentify the user as someone else with a similar fingerprint or face. This use case is perfect for an exact nearest neighbor (ENN) search, which guarantees that the search results are the best possible matches. This type of search must always return the closest matching stored vector and ensure that it appears before other similar but more distant matches. One straightforward approach is to brute-force the problem: calculate the distance between the query vector and every stored vector, then return a list of the results sorted from closest to farthest. By checking every vector, you can guarantee that the search results include precisely the most relevant vectors in order. While effective for small datasets, this method quickly becomes computationally expensive and time-consuming as the number of stored vectors increases. Some clever approaches can help exact search scale to larger datasets, such as using tree-based indexes to avoid calculating similarity for every vector. This makes exact search useful for some additional kinds of applications, but ultimately, the problem does not scale well and can take a long time on large datasets. In cases where exactness is required, you have to accept its constraints and find ways around them. For other common cases, however, it is enough to know that your search results are close enough to be the best match. This use case is called an approximate nearest neighbor (ANN) search and is powerful enough for many everyday applications. For example, if you search for in a recommendations app, you don’t need the results to include a specific movie. Rather, you probably just want a list of a few similar sci-fi thrillers with mind-bending plots. A list of results such as &quot;Shutter Island&quot;] is useful, even if it turns out that the movie Interstellar is technically a closer semantic match than any of the returned results. The choice between exact and approximate search comes down to your application’s requirements. You may have strict requirements that necessitate an exact search. However, you may also have a use case where an exact search, while useful, is not necessary to provide value. Or it might not make sense to do an exact search at all. In the next section, you’ll learn how to evaluate search algorithms to help you determine your requirements. Exact versus approximate search movies like Inception [&quot;Minority Report&quot;, &quot;Memento&quot;,"></li><li class="carousel-slide" data-ssplayer-slide-index="69" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/70/1739580232671?e=1743033600&amp;v=beta&amp;t=V5e1lgkWMY33MkPOBBJ5E0PXblPPk_EHSpOhE4bhl44" alt="Graph connectivity 69 Measuring search You can describe a search algorithm in terms of its precision, recall, and latency: If you’ve ever used a city’s public transit network to get around, you may have wondered about how the city chose to put the train or bus stops where they did. There are many factors at play, but if you look at an ideal case, then you can boil the choice down to two related factors: connectivity and latency. Think about the experience of a train rider, let’s call her Alice, visiting her friend, Bob, across the city. It would be great if there was a stop right next to Bob’s house because, then, Alice could see himright after stepping off the train. Of course, you can’t put a train station in front of every house, and after a certain point, adding more stops would increase the average trip time. • Precision measures how accurate the search results are. Precise searches try to return only matches that are relevant to the query and few, if any, irrelevant results. • Recall measures how complete the search results are. If a search returns a large fraction of all relevant results, then it has a high recall. • Latency measures how long a search query takes from start to finish. Every search takes some amount of time to return results. The exact latency varies between searches but, on average, it’s a function of how many vectors are in the search space and your precision and recall requirements. These factors are tightly coupled and require trade-offs that define the nature of nearest neighbor searches. For example, an ENN search has perfect precision and will include the most relevant results. However, to keep the latency reasonable, it might omit some relevant results if there are too many. Because it misses valid results, this search would have a relatively low recall. If the ENN search also required a high recall, then the search would have to run for longer to ensure that enough relevant results are included. In an ANN search, you can relax your precision requirements, which allows you to optimize the other factors instead. You can get more complete results by either allowing the search to take more time or by returning more results that potentially include false positives. If you can tolerate false positives, for example, by filtering them out after the search in your app, then you can use ANN to run fast searches that return highly relevant result sets. You should evaluate your application and determine its top priority regarding these factors. Then, you can choose the appropriate search operation and tune the algorithm until the other factors are appropriately balanced. Tuning a search algorithm involves modifying the configuration parameters that determine how it constructs and traverses its index data structure. To get a better feel for what that means, you’ll spend the next few sections going over the concepts and data structures used to enable vector search operations, starting with the idea of connectivity. Graph connectivity"></li><li class="carousel-slide" data-ssplayer-slide-index="70" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/71/1739580232671?e=1743033600&amp;v=beta&amp;t=ahjpseKnvjasVPeDuqXSzH2K3jB5arpquq0mkAODzgE" alt="70 Vector Databases Every time you change the number of stops or connections, you may affect how long it takes to get between any two destinations in the system. Typically, the job of planning where to place public transit stops is done with thought and consideration by knowledgeable civil engineers, city planners, and other stakeholders. The primary goal of a transit network is to take a rider to a stop that is relatively close to their true final destination in a reasonable amount of time. By understanding their goal and applying specific strategies, city planners try to connect distant parts of the city in a way that’s useful and efficient for transit riders. Similarly, the goal of an ANN search is to find a vector that is close to a given query vector, also in a reasonable amount of time. If you were to take inspiration from transit planners, you could use this similarity to your advantage and design an effective ANN index. In essence, both transit planning and nearest neighbor search boil down to a problem of building and traversing a graph that trades off connectivity and latency. You can use an algorithm called navigable small worlds (NSW) to build such a graph. It takes in vectors one at a time and adds a node to the graph for each one. Each node can also have connections to other nodes, called neighbors, that are assigned during graph construction. The NSW algorithm is designed to balance how relevant a node’s immediate neighbors are with how connected the node is to the rest of the graph. It will mostly assign neighbors that are closely related to a node. However, it may also sometimes connect two less similar nodes that are relatively far apart on the graph. If you think about the transit example, this is like having a bus route that has several stops in the same neighborhood but that also runs downtown. Residents can easily get to their local destinations. If they need to go outside of the neighborhood, then they still have access to the rest of the city. For an example of an NSW graph, refer to Figure 5.2. Notice that each node is connected to a maximum of three neighbors and that, in general, nearby nodes are closely connected. Each node represents a vector and nodes connected with lines are neighbors. The highlighted connections show the path of a greedy nearest neighbor search. Navigable small worlds"></li><li class="carousel-slide" data-ssplayer-slide-index="71" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/72/1739580232671?e=1743033600&amp;v=beta&amp;t=FnPD925O_tuD8oIqqkugja-DP0eV0QKDZWdAB1o1gv8" alt="Figure 5.2: An NSW graph Graph connectivity 71 The exact search algorithm that you use to traverse an NSW graph may vary and affect the behavior of the search as a whole. The most common algorithm is a simple greedy search, where at every step, you find and take the best immediate option with no regard to previous or future steps. For example, a greedy search of an NSW graph first randomly selects a node to start at and then measures to see how close the node is to the query vector. Then, it measures the distance to each of the node’s neighbors. If one of the neighbors is closer than the current node, then the search moves on to that node and continues with the same measure-and-compare process. Otherwise, the search is complete and the current node is an approximate nearest neighbor. Once you’ve constructed an NSW graph of your vectors, you can use it as an index for ANN searches. You can start at a random node and use a search algorithm to follow the neighbor connections until you reach the nearest neighbor. This lets you limit your similarity comparison to only a subset of the total search space. For example, notice how the search path in Figure 5.2 arrives at the nearest neighbor without visiting every node in the graph. How to search a navigable small world Start Query vector Nearest neighbor"></li><li class="carousel-slide" data-ssplayer-slide-index="72" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/73/1739580232671?e=1743033600&amp;v=beta&amp;t=5d2dntdjo9WvQYM25qXi066TiWNu7VPpshFOmLbOxao" alt="72 Vector Databases In this basic example of NSW with greedy search, the definition of approximate is very broad and the search may return suboptimal results. This comes down to the nature of graph search, which, in this case, is designed to find a local minimum of the graph. This local minimum is not guaranteed to be the global minimum, which is what makes the search approximate rather than exact. A greedy search algorithm alone can return false positives if it settles on a local minimum that is too far from the global minimum. You can partially guard against this by tuning the graph’s construction parameters. However, due to the dynamic nature of search queries and the underlying data being searched, you can’t entirely prevent false positive local minima from existing. Instead, you need to find a way to minimize their impact. One way is to run the search multiple times, starting from different randomized entry nodes. This method, called randomized retries, collects multiple samples from the graph and returns the best result out of all the samples. You can also add additional machinery to the algorithm to make it more robust. A common architecture pairs the greedy search algorithm with a configurable priority queue that keeps a sorted list of the nearest neighbors the search has seen. If the search encounters a false positive local minimum, the queue lets it backtrack and explore other branches of the graph that might lead to a nearer neighbor. The exact search method you use depends on the dataset and your goals. For example, randomized retries are easy to implement and can run in parallel. They are useful for subtle, exploratory searches that might match many local minima. However, their random nature makes them non-deterministic, and each retry does a full search, which can quickly scale your costs. Conversely, priority queues are deterministic and precise but are harder to implement and tune. With this information, you have the basis for a useful vector search index. You could stop building the index here and start searching. However, you will quickly find that there are issues with this approach, particularly as you scale the search space to sizes commonly seen in AI apps. Randomized retries have significant computational overhead, and you must do more of them as you scale your data set. A priority queue keeps a search from getting stuck in local minima but does not prevent it from meandering through many nodes on the way to its target. To address these issues, you need to go beyond a single NSW graph. In the next section, you will see how combining multiple NSW graphs together can circumvent meandering searches and make randomized retries less necessary. Think back to Alice’s public transit experience. What if, instead of the same city, she and Bob lived in different cities on opposite sides of the country? Alice could, in theory, limit herself to public transit services by crisscrossing the nation via a series of trains, buses, taxis, and bike shares. This would obviously take a lot of time and require many stops along the way. That’s because transit networks are only effective at the scale of an individual city. Once you zoom out farther, you need a different system. Instead of just using transit, Alice could instead start at her city’s airport and fly to Bob’s city. Even if her trip included a layover and multiple flights, it would still probably be faster than using transitalone. Once she gets to Bob’s city, she can use the subway system to get from the airport to his neighborhood quickly and efficiently. Hierarchical navigable small worlds"></li><li class="carousel-slide" data-ssplayer-slide-index="73" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/74/1739580232671?e=1743033600&amp;v=beta&amp;t=12kz2oHNkrECALJBCDt3-t0tMY2bAeHXr98iA8ovKAg" alt="Figure 5.3: An HNSW graph structure Graph connectivity 73 An actual HNSW algorithm would decide the top layer for each vector probabilistically with a node that exists only in lower layers being more likely than one that also exists in higher layers. A search starts in the top layer by finding the node that’s nearest to the query vector. Then, it moves to the same node but in the next layer down and continues the search from there. This continues until it reaches the nearest neighbor on the final layer, at which point, the search is complete. In Figure 5.3, the highlighted connections show the path of a greedy nearest neighbor search across multiple layers. Alice’s trip took place at two distinct levels. First, she started at the level of airports, where she was free to travel to any destination airport connected to her home airport. At this layer, she had direct access to many different cities, but that access was limited to only one location in each city: the airport. She used the airports to get closer to Bob without spending too much time planning her route and traveling. Once she got to the closest airport to Bob, she dropped down into the second layer and gained access to a transit network that could get her even closer to Bob. This is basically the idea of hierarchical navigable small worlds (HNSW). You can create a hierarchy of layers where each layer is an NSW graph. For example, look at Figure 5.3 to see a typical HNSW graph structure. The top layer has relatively few nodes that are all fairly distant from one another and sparsely connected. Each lower layer has all the nodes of the layer above it plus additional nodes and connections that make the graph denser and more connected. In this chapter’s example, the distinction between transit nodes and airport nodes is a natural way to split the layers. The airports are the top layer and the next layer down includes both the airports and the transit stops."></li><li class="carousel-slide" data-ssplayer-slide-index="74" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/75/1739580232671?e=1743033600&amp;v=beta&amp;t=yw_0nJtqaV1ls0-mOWNLAagQ_Z8P2tGYO0heMYh6fGs" alt="74 Vector Databases HNSW is the foundation of many modern vector search applications. It’s battle-tested and proven to give useful results in a reasonable amount of time. The algorithm is highly suited for ANN use cases with configurable parameters that put you in control of how your searches perform. Now that you have an idea of the inner workings of vector search, you can see how it requires purpose- built logic and data structures. In the next section, you’ll learn how vector databases encapsulate all of the technical details in order to make vector search available to developers. Vectors carry deep semantic information and have many potential use cases that will make them increasingly common over the next few years. Working with them requires specific and complex operations that only process vector data. Additionally, the demand for search can often vary substantially from the demand for more structured database queries. Together, these factors mean vector operations and traditional database workloads are largely independent. This gives rise to the concept of a vector database that’s designed specifically to handle vector data, indexes, and workloads. From a developer’s perspective, vector databases can take several forms. The most basic is a standalone product that’s independent from other operational databases. This type of vector database has the freedom to focus solely on implementing and optimizing vector operations without considering other database operations. However, often, vector search applications require additional filtering or metadata and may perform more traditional database operations based on search results. These use cases require either multiple queries to different databases at runtime or an additional syncing layer that copies data from your operational database to the vector store. Alternatively, a vector database can be baked into an existing database or data service. For example, a general-purpose database management system might support vector search operations in itsquery language if you’ve defined the appropriate vector search index. This allows applications to piggyback off of the existing system’s features and access search within the same system. The vector database can be scaled and run independently within the system but exposed to the user along with traditional operations as part of a unified API. This couples your vector store to your existing database but leads to simpler and easier-to-maintain architectures. Regardless of form, vector databases are a key tool in AI applications. They are purpose-built to store and query vector data. You can configure them to deliver optimal search results and power AI applications. The next section will cover some ways that vector search can be used to enhance ML and AI models, including during training, fine-tuning, and runtime. You’ll also learn how vector search itself enables AI applications without additional functions or models. The need for vector databases"></li><li class="carousel-slide" data-ssplayer-slide-index="75" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/76/1739580232671?e=1743033600&amp;v=beta&amp;t=E7ArL-tGquPOF7OkuzFY26vqoPrpuZqymkAPF_ksuIo" alt="The need for vector databases 75 How vector search enhances AI models AI models encompass a broad class of data structures and techniques. ML forms the core of most modern vector-based AI models, aiming to “teach” computers to do specific tasks via a training process. In general, ML processes work by feeding a curated dataset to a base model that can detect and infer patterns from the data. Once a model has learned these patterns, it’s able to recreate or interpolate them to process new inputs. These techniques and models are ubiquitous in the world of AI and are the secret sauce that powers novel use cases. In general, ML training and AI applications can be split into two concerns, as follows: • Information retrieval involves finding relevant information that’s useful as input to an AI process. Vector search is very well suited for this task. Embedding models can encode the semantics of a huge variety of inputs into a standard vector form. Then, you can use search to find matches for an equally huge range of inputs, both structured and unstructured. • Information synthesis combines multiple pieces of information, possibly from different sources, into a coherent and useful result. This is the domain of GenAI models. These models can’t reliably find or generate true facts, but they can effectively process and reformat input information. Vector search enhances ML and AI models by providing them with access to the most relevant data at every stage, from training to fine-tuning to runtime execution. During training, you can use a vector database to store and search your training data. You can design a process that finds the most relevant data from the corpus to use for each training task. For example, when training a language model for a specific domain such as medicine, you could use vector search to retrieve the most relevant chapters from medical textbooks for each training batch. This ensures that the model learns the most pertinent information without being distracted by noise. You can apply the same idea during fine-tuning, which is essentially a secondary training stage on top of a more generic base model. For example, you could fine-tune the medicine language model to generate reports using a hospital system’s preferred style and structure. Vector search could help find human-written reports that are relevant to each training topic. Whether your model is specialized or general purpose, you can customize its runtime behavior by modifying the inputs you give to it. Vector search can analyze raw input and find related information. Then, you can augment or refine the raw input to include the retrieved context. For example, you might maintain a vector database of rare diseases and search for anything that matches a user’s description in order to get a more tailored diagnosis. AI applications come in many forms, but modern apps increasingly use a runtime customization approach to provide relevant context to generative transformer models. This architecture is the basis of a technique called retrieval-augmented generation (RAG), which you’ll learn about in greater depth in Chapter 8, Implementing Vector Search in AI Applications. Up to this point, you’ve learned the theory and mechanics of vector databases and search operations. Next, you’ll look at some examples of real vector database use cases that highlight how vectors are the core of modern AI apps."></li><li class="carousel-slide" data-ssplayer-slide-index="76" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/77/1739580232671?e=1743033600&amp;v=beta&amp;t=A6HXnHVn0DmESDsCA01tfPiltuyMgWf6oMpqCeHXlhA" alt="76 Vector Databases Figure 5.4: Okta Inbox user request form Case studies and real-world applications Vector search is a powerful tool that enables you to build sophisticated systems for finding information based on its meaning, rather than just its exact words. By understanding the context and relationships between data points, vector search helps you retrieve highly relevant results. So far, you have learned about the different concepts involved with vector search and some of the different offerings that exist in the market, but how do businesses integrate vector search into their applications? In this section, you will explore three popular methods for leveraging vector search: semantic search, RAG, and robotic process automation (RPA). You will look at existing case studies of MongoDBAtlas Vector Search that fit into each of these buckets, and how these applications deliver value to the end user through more accurate search that wasn’t previously possible. Each of the following casestudies was originally published as a part of the Building AI with MongoDB series of customer stories (https:// www.mongodb.com/resources/use-cases/artificial-intelligence?tck=blog-genai&amp; section=resources&amp;contentType=case-study). These stories are presented here to showcase the variety of vector search use cases that can be built on the flexible, scalable, and multifaceted MongoDB Atlas platform. Okta, one of the world's leading identity security providers, uses a natural language RAG interface to allow users to easily request roles for new technologies in their organizations. They built a system called Okta Inbox using Atlas Vector Search and their own custom embedding model that makes it possible for users to map natural language queries to the right roles. Okta – natural language access request (semantic search)"></li><li class="carousel-slide" data-ssplayer-slide-index="77" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/78/1739580232671?e=1743033600&amp;v=beta&amp;t=2DBC3DEbItCxH8jSUtWkTMxb3Uceo0Qozg--uKzMg4g" alt="Figure 5.5: Okta Inbox administrator view Case studies and real-world applications 77 This is an example of leveraging semantic search to solve a problem, where the embedding models trained by Okta’s data science team were capable of mapping natural language requests to the right user roles to be assigned. These requests would get routed to a manager via Slack through an existing workflow. The end result is a simple user experience that makes identity management between both the requesters and the access managers much simpler, thus making the value proposition of Okta as an identity and access management solution even greater. Okta chose to use Atlas Vector Search to query these vectors since they were already using Atlas as their operational data store, and this provided a simplified developer experience. You can read more about this case study at https://www.mongodb.com/solutions/customer-case- studies/okta. One AI provides verticalized AI agents and chatbots for different industries. These services allow detailed AI-assisted analysis to be performed over documents with applications in industries ranging from financial services and real estate to manufacturing and retail. One AI – language-based AI (RAG over business data)"></li><li class="carousel-slide" data-ssplayer-slide-index="78" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/79/1739580232671?e=1743033600&amp;v=beta&amp;t=YcU7InZ9yUiuInn6vIoEWS3pFNujxjWxsSVMtigNPAo" alt="78 Vector Databases Figure 5.6: Example of a CSR “A very common use case in language AI is creating vectors that represent language. The ability to have that vectorized language representation in the same database as other representations, which you can then access via a single query interface, solves a core problem for us as an API company.” —Amit Ben, CEO and founder of One AI This is a prime example of a multitenant RAG application, where data that is indexed and provided for one type of AI service provided by One AI might not be relevant to another service. As discussed later in this chapter, this is a common data modeling pattern that is easy to build within the Atlas platform. You can further read about this case study at https://www.mongodb.com/solutions/ customer-case-studies/one-ai-success-story. The chatbots offered by One AI are all built using the MongoDB Atlas platform, with over 150 million indexed documents from over 20 different internal services. One AI’s goal of bringing AI to everyday life is made feasible by simply adding a vector search index to the data that they store in Atlas and making it queryable via embedded natural language input. Novo Nordisk is one of the world’s leading healthcare companies with a mission to defeat some of the world’s most serious chronic diseases such as diabetes. As a part of the process of getting new medicines approved and delivered to patients, they must generate a clinical study report (CSR). This is a detailed record of the methodology, execution, results, and analyses of a clinical trial and is meant as a critical source of truth for regulatory authorities and other stakeholders in the drug approval process. Novo Nordisk – automatic clinical study generation (advanced RAG/RPA)"></li><li class="carousel-slide" data-ssplayer-slide-index="79" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/80/1739580232671?e=1743033600&amp;v=beta&amp;t=JYFnOc-wfeMWBrt_f6r9DcmMeurp51p6cZm4m-_v5TU" alt="Vector search best practices 79 This section covers the best practices for improving the accuracy of your vector search through intelligent data modeling, deployment model options, and considerations for prototype and production use cases. By following the guidance in this section, you will be more likely to improve the quality of your vector search results and operate your search system in a scalable, production- ready manner. In the context of MongoDB, data modeling refers to the process of designing the structure of the data stored in the database. Unlike traditional relational databases, MongoDB is a NoSQL database that uses a flexible, schema-less model, allowing for more dynamic and hierarchical data storage. The big idea about data modeling for vector search centers around the notion that embedding models are not infinitely capable, and users can take control of the relevance search problems in embedding models by using vectors along with the other data they have. Taking control can be done in simple ways, such as incorporating user-based fields for metadata filtering. You can take control in more complicated ways, too, such as by using LLMs to define graph relationships between chunks and looking these up at query time subsequent to a $vectorSearch query. Typically, a CSR takes around 12 weeks to complete, but the content digitalization team at Novo Nordisk was able to build a tool using Atlas Vector Search to shorten this process to ten minutes. They built a RAG workflow called NovoScribe leveraging Claude 3 and ChatGPT as their chat completion models, and Titan for text embedding hosted on the Amazon Bedrock service. They used MongoDB Atlas Vector Search as a knowledge base to serve relevant data to these models. Functionally, NovoScribe generates validated text using defined content rules and statistical outputs. Atlas Vector Search computes the similarity of each text snippet to the relevant statistics, which isthen fed into a structured prompt to the LLM to produce a CSR that is ready for review by a subject-matter expert, including the lineage of all of the data presented. “What’s great about MongoDB Atlas is that we can store native vector embeddings of the report right alongside all of their associated text snippets and metadata. This means we can run really powerful and complex queries quickly. For each vector embedding we can filter on which source document it’s coming from, who wrote it, and when.” —Tobias Kröpelin, PhD, Novo Nordisk This project allowed Novo Nordisk to build an advanced clinical report generation system by intelligently arranging their data in the right format within MongoDB and defining a vector search index against it. They were allowed to go further with their data in more ways using novel embedding models and LLMs to dramatically improve the process of authoring CSRs as a result. You can read more about this case study at https://www.mongodb.com/solutions/customer-case-studies/novo-nordisk. Vector search best practices Data modeling"></li><li class="carousel-slide" data-ssplayer-slide-index="80" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/81/1739580232671?e=1743033600&amp;v=beta&amp;t=wf9uLdp7O8RrtZpRj6lBUEEWWl_nA6FvWVl1c4VIwx4" alt="80 Vector Databases Dynamic filters are pieces of metadata that vary based on the content of the search query. These can be attributes of the data, such as when a book was published or its price. They are typically selected by a user when executing their search along with their plain English query. Here is an example: One can broadly think about leveraging metadata as using documents to deliver the data back to the user, rather than vectors. Working with documents as the results of an aggregation stage means that different aggregation stages can be composed together to yield greater functionality than any one alone and can benefit from query optimization. This has been the bread and butter of the document model since MongoDB was invented, and it continues to be the case today in the age of GenAI applications. This section will dive deeper into the ways other data can be used prior to, alongside, and following vector search to improve the accuracy of your vector-based information retrieval system. Filtering The most basic yet most effective form of metadata usage is to limit the scope of the vector search by considering only vector data that meets a prefilter. This restricts the scope of valid documents to be considered, which, for selective filters (the most common kind of filter), increases accuracy and reduces query latency. At query time, these prefilters can be considered as a part of a $vectorSearch query using a $match MQL semantic. This means that in addition to point filters such as $eq, the user can define rangefilters such as $gt or $lt to only search against documents that fit a range of values rather than matching a specific one. This can dramatically reduce the number of valid documents that need to be searched, reducing the amount of work that needs to be done and generally improving the accuracy of your search. $match filters can also leverage logical operators such as $and and $or to allow users to compose filters together and build more complex logic into their search applications. Let’s look at two common types of filters, and when and how you might use them. Dynamic filters [ { Oriented &quot;_id&quot;: ObjectID(&quot;662043cfb084403cdcf5210a&quot;), &quot;paragraph_embedding&quot;: [0.43, 0.57, ...], &quot;page_number&quot;: 12, &quot;book_title&quot;: &quot;A Philosophy of Software Design&quot;, &quot;publication_year&quot;: 2018 }, { &quot;_id&quot;: ObjectID(&quot;662043cfb084403cdcf5210b&quot;), &quot;paragraph_embedding&quot;: [0.72, 0.63, ...], &quot;page_number&quot;: 6, &quot;book_title&quot;: &quot;Design Patterns: Elements of Reusable Object- Software&quot;, &quot;publication_year&quot;: 1994"></li><li class="carousel-slide" data-ssplayer-slide-index="81" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/82/1739580232671?e=1743033600&amp;v=beta&amp;t=cYH20tIurCCncFZ3Fuzxk0mAfflVzBZDvXywH2mttxM" alt="Vector search best practices 81 ] }, { &quot;_id&quot;: ObjectID(&quot;662043cfb084403cdcf5210c&quot;), &quot;paragraph_embedding&quot;: [0.12, 0.48, ...], &quot;page_number&quot;: 3, &quot;book_title&quot;: &quot;Guide to Fortran&quot;, &quot;publication_year&quot;: 2008 }, ... Dynamic filters are most common when building a semantic search application since they are typically input by the user prior to executing a query within a search bar. This contrasts with a RAG interface, which is entirely natural language. Static filters and multitenancy There are cases where the filter is associated not with the body of the query, but by the user’s profile. The user may be querying data that is accessible only to their company but is stored in a multi tenanted fashion with many other tenants’ data. In this case, the user ID or company ID that the user belongs to may be used to filter what results are searched against. For cases where there are a high number of tenants and few vectors, filters are the recommended approach for modeling data rather than storing many bits of data across multiple collections and indexes. It is recommended to set the exact flag to true in $vectorSearch when you have a high degree of variation between the number of vectors per tenant and a high number of tenants modeled within the same collection or index. This will lead to an exhaustive search performed in parallel on all segments corresponding to a vector index. In many cases, this will accelerate the search, given the highselectivity of the filter and the large number of potential vectors that would need to be searched and discarded while running a filtered HNSW search. Chunking In the context of RAG, an interesting analogy emerges. Just as chat models require intelligent prompt engineering, embedding models require intelligent chunking. Intelligent chunking requires finding the right level of context that can effectively be mapped to a search or natural language query. This may also be the right level of context to provide to the LLM, but as you’ll see later in the Parent document retrieval section, this is not a strict requirement if you intelligently model your data. You will learn more about basic and advanced chunking strategies in Chapter 8, Implementing Vector Search in AI Applications. For the sake of this section, let’s consider one basic chunking strategy,fixed token count with overlap, and how you can experiment to assess what works best on your own dataset."></li><li class="carousel-slide" data-ssplayer-slide-index="82" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/83/1739580232671?e=1743033600&amp;v=beta&amp;t=QD-X8t7ekAdShsXrqS-M6d92v4ervRXKWhqrAihVzYQ" alt="82 Vector Databases Figure 5.7: An example of chunking based on fixed token count with overlap Experimentation Evaluating which chunking strategy or embedding model works best for your use case requires curating judgment lists of documents along with the queries that you would expect to map to those documents. You would also want to play around with the different embedding models and chunking strategies that can be applied before embedding data to see which works best for your use case. A given embedding model might perform better or worse with a fixed chunking strategy. You can more easily evaluate which combination of chunking and embedding models is best suited for your use case. You could have multiple versions of the same data, each split and processed differently. By comparing these versions, you can determine the optimal splitting method and embedding model for your specific search needs. The best way to determine whether an embedding model is effectively mapping your documents to a sample query is to inspect the similarity score that is returned for a set of queried documents and see how well that aligns with what good responses might be for the actual question, as shown in Table 5.1. Fixed token count with overlap A fixed token count with overlap, which is a common default in many RAG integration frameworks such as LangChain, splits unstructured data into chunks based on the specified maximum number of tokens per chunk and the desired overlap between chunks. This method is more granular than the whole-page ingestion method, and it allows for greater experimentation on your specific dataset. It doesn’t involve exploiting any structure within the unstructured data. This is a positive in terms of simplicity of development but can be a negative when sentences, paragraphs, or other boundaries demarcate semantic significance in a way you would want to model. This technique may be a good fit if you have little control over the source data or are working with unstructured data that doesn’t lend itself well to boundary chunking methods that leveragedocument structure, such as HTML tags, because this technique is compatible with any text format. Figure 5.7 shows an example with different colors indicating separate chunks and overlaps: Hierarchical NSW incrementally builds a multi-layer structure consisting of a hierarchical set of proximity graphs (layers) for nested subsets of the stored elements. Chunk 1 Chunk 2 Chunk 3 Overlaps"></li><li class="carousel-slide" data-ssplayer-slide-index="83" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/84/1739580232671?e=1743033600&amp;v=beta&amp;t=EZMzlbfeV_2nHJm6vfzLvZpWdWz2_opZ9bj_ek7J0Ks" alt="Table 5.1: Vector search results ranked by cosine similarity Vector search best practices 83 2 3 Rank 1 Raw document “One of the main challenges of building software is managing complexity.” “Deep modules provide deep functionality behind a simple interface” “Software systems often grow in complexity due to evolving requirements.” Embedding [0.23, 0.45, …] [0.86, 0.34, …] [0.46, 0.51, …] 0.874 0.563 Cosine similarity 0.901 In the case of a fixed token count with overlap strategy, you will have to figure out the token count that you would like to start with. The 300–500 token range seems sufficient for experimentation in the information retrieval community. Hybridization Hybridization involves modeling multiple sources of relevance within a single document and jointly considering them alongside a single vector search at query time. This technique embodies the flexibility of the aggregation pipelines supported by MongoDB and allows for a great amount of experimentation and tuning of your search system leveraging vector search, lexical search, traditional database operators, geospatial queries, and more. In the following sections, you will explore some of the more popular methods for hybridization, as well as some promising avenues of exploration that you might find relevant to your use case. Vector plus lexical Vector search is a sound methodology for exploiting semantic similarity between a query and indexed document as defined by the capabilities of an embedding model. Lexical search systems such as BM25, which Lucene and, correspondingly, Atlas Search use, are helpful in a completely different way in that they index tokens directly and use a bag-of-words style approach that ranks a set of documents based on the query terms appearing in each document, regardless of their proximity within the document. Despite being based on an original probabilistic retrieval framework developed in the 1980s, this approach is still fairly good at mapping keywords in a query to keywords in a document, especially when that word is used outside the context of what an embedding model was trained on. Small datasets can contain tokens either not seen in the training dataset or with alternative meanings, as shown in Figure 5.8."></li><li class="carousel-slide" data-ssplayer-slide-index="84" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/85/1739580232671?e=1743033600&amp;v=beta&amp;t=qCVwq76Oia8JwlJuJLjrsZ5UwOP20R0nf5A9iLbZbv0" alt="84 Vector Databases Figure 5.8: Out-of-sample terms Some vector search providers provide sparse vector search as an alternative to lexical search, which can be made to operate similarly but has been considered insufficient for customers’ purposes. It also lacks out-of-the-box support for many lexical search features, such as synonym lists, pagination, and faceting. Smaller levels of context are good fits for embedding models, whereas broader levels can be well represented by keyword search. MongoDB allows users to experiment in this direction as much as possible, while also allowing the joint query pattern to be joined on a foreign key, rather than simply a document _id. This makes it possible to have windowing levels of representation for a given document that can be considered by different methodologies. The following code shows how some documents containing paragraph_embeddings can be indexed and queried using a vector search index, while other documents containing full_page_content can be indexed and queried using a text search index: [ { &quot;_id&quot;: ObjectID(&quot;662043cfb084403cdcf5210d&quot;), &quot;page_number&quot;: 81, &quot;paragraph_embedding&quot;: [0.43, 0.91, ...], }, { &quot;_id&quot;: ObjectID(&quot;662043cfb084403cdcf5210e&quot;), &quot;full_page_content&quot;: &quot;Pulling complexity down makes the most sense if (a) the complexity being pulled down is closely related to the class's existing functionality, (b) pulling the complexity down will result in many simplifications elsewhere in the application, and (c) pulling the complexity down simplifies the class's interface. ...&quot;, ] &quot;page_number&quot;: 36, }, ... text-embedding-3- large training dataset Keywords, jargon, proper nouns my dataset"></li><li class="carousel-slide" data-ssplayer-slide-index="85" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/86/1739580232671?e=1743033600&amp;v=beta&amp;t=11qB47dheaYeJOmIPsrCcgvPrnDmUhOUu8f0dza9iy4" alt="Vector search best practices 85 The results of independent $vectorSearch queries could be hybridized and fused using a similar pattern to the vector plus lexical search query pattern seen in the previous section and would allow for multiple sources of relevance to be used to find the most relevant document to a query. In e-commerce search use cases, it is common for a single item to have many sources of relevance that can be embedded and stored within the same document representing that item. These include the following: • Product description • User reviews (and summaries of user reviews) • Product images Each of these sources of relevance can be embedded and jointly considered using the same query pattern as one would use to jointly consider vector and lexical relevance. There might be multiple sources of vector relevance in your dataset that you would want to consider jointly, similar to how you might jointly consider paragraph embeddings and keyword relevance for a whole page. The secondary embedding field you are considering might be a derivative field, such as an LLM-generated chapter summary that is then embedded, or it could be an entirely different source of data. The following code shows a single document with a set of source fields that could be embedded and indexed using a vector search index: Jointly considering the result sets from the two queries shown in the preceding code is what you call hybrid search and can be done using the reciprocal rank fusion method, as shown at https:// www.mongodb.com/docs/atlas/atlas-search/tutorial/hybrid-search/. In the future, Atlas Vector Search will offer support for dedicated stages that make combining result sets based on rank or score much simpler. However, the fundamental concepts will remain the same. Vector plus vector [ { &quot;_id&quot;: ObjectID(&quot;662043cfb084403cdcf5210d&quot;), &quot;book_title&quot;: &quot;A Philosophy of Software Design&quot;, &quot;book_title_embedding&quot;: [0.67, 0.45, ...], &quot;chapter_title&quot;: &quot;The Nature of Complexity&quot;, &quot;chapter_title_embedding&quot;: [0.51, 0.89, ...], &quot;chapter_summary&quot;: &quot;This book is about how to design software systems to minimize their complexity. The first step is to understand the enemy. Exactly what is 'complexity'?...&quot;, &quot;chapter_summary_embedding&quot;: [0.36, 0.90, ...], &quot;raw_text &quot;System designers sometimes assume that complexity can be measured by lines of code. They assume that if one implementation is shorter than another, then it must be simpler; if it only takes a few lines of code to make a change, then the change must be easy...&quot;, &quot;raw_text_embedding&quot;: [0.43, 0.11, ...], }, ..."></li><li class="carousel-slide" data-ssplayer-slide-index="86" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/87/1739580232671?e=1743033600&amp;v=beta&amp;t=g77kKM1Tf9eBSQLjo0C0jBQ1V1bjmF-y67x2jKFhrOI" alt="86 Vector Databases Incorporating user feedback Incorporating user feedback for RAG applications is conventionally thought of as providing signals to the chat model to modify their weights through a process known as reinforcement learning with human feedback. However, search systems have incorporated user signals to inform how results are ranked for decades, and similar principles can be applied to RAG. An interface that provides a ranking mechanism for the sources that are provided to the LLM would allow for feedback to be directly modeled within the document, as seen in the following code. These signals can then be jointly considered using the hybrid search query pattern combining $vectorSearch and the $sort stage using the upvotes or downvotes as a proxy for user relevance. This is a very naive approach, but the principle behind it can be extended to allow for greater personalization of content where similar users are defined by similar interactions with different content, which is the basis for the popular recommendation system algorithm known as collaborative filtering. ] }, ... [ { &quot;_id&quot;: ObjectID(&quot;662043cfb084403cdcf5210a&quot;), &quot;paragraph_embedding&quot;: [0.43, 0.57, ...], &quot;page_number&quot;: 12, &quot;score&quot;: 0.95, &quot;upvotes&quot;: 2, &quot;downvotes&quot;: 58 }, { &quot;_id&quot;: ObjectID(&quot;662043cfb084403cdcf5210b&quot;), &quot;paragraph_embedding&quot;: [0.72, 0.63, ...], &quot;page_number&quot;: 6, &quot;score&quot;: 0.90 &quot;upvotes&quot;: 81, &quot;downvotes&quot;: 3 }, { &quot;_id&quot;: ObjectID(&quot;662043cfb084403cdcf5210c&quot;), &quot;paragraph_embedding&quot;: [0.12, 0.48, ...], &quot;page_number&quot;: 3, &quot;score&quot;: 0.67 &quot;upvotes&quot;: 2, &quot;downvotes&quot;: 5"></li><li class="carousel-slide" data-ssplayer-slide-index="87" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/88/1739580232671?e=1743033600&amp;v=beta&amp;t=zjFZ4UeD4m_E3j5jYDUl9bUpAZqGzvtS3PjPh8QMvUY" alt="Vector search best practices 87 It’s important to note that like all other metadata, capturing relationships between MongoDB documents in this manner must be extracted at ingestion time. While it is still early days in terms of intelligently incorporating user feedback into your RAG application, the flexibility of the document model should allow for a rich amount of experimentation in this area as your search system, and how your users engage with it, evolves over time. Document lookups Once you have a sorted result set of documents, possibly produced from multiple methodologies in an optimized manner, there are still additional operations that can be performed that might leverage relationships inherent within your data. With document lookups, some data may be easier to model outside of the document itself using a foreign lookup key to model tree structures within your data, such as hierarchies within documents, organizations, or some other taxonomy. Parent document retrieval Parent document retrieval involves performing a vector search at one level of context, and then retrieving a document connected to the most relevant retrieved documents via a foreign key. This foreign key is usually a child-parent relationship, such as an embedded paragraph belonging to a specific page of a larger body of text, where that larger bit of context may be stored in another document completely. With this pattern, you can store only the embeddings at the lower level, and then look up a higher level of context containing a much larger amount of text. This may be useful if you find that the queries are more easily mapped semantically to a smaller amount of text, but the amount of data you want to serve to the user or an LLM is much larger, which is often the case. The following code example for hybridizing lexical and vector search is also an example of parent document retrieval, as vector embeddings are searched against to yield a full page of content to provide to the LLM. The foreign key is the page_number. [ { &quot;_id&quot;: ObjectID(&quot;662043cfb084403cdcf5210d&quot;), &quot;page_number&quot;: 81, &quot;paragraph_embedding&quot;: [0.43, 0.91, ...], }, { &quot;_id&quot;: ObjectID(&quot;662043cfb084403cdcf5210e&quot;), &quot;full_page_content&quot;: &quot;Pulling complexity down makes the most sense if (a) the complexity being pulled down is closely related to the class's existing functionality, (b) pulling the complexity down will result in many simplifications elsewhere in the application, and (c) pulling the complexity down simplifies the class's interface. ...&quot;, ] &quot;page_number&quot;: 36, }, ..."></li><li class="carousel-slide" data-ssplayer-slide-index="88" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/89/1739580232671?e=1743033600&amp;v=beta&amp;t=r6JcW1b3rn9uND5TrGizlFzvgFoD7zy-GhsG380l6TA" alt="88 Vector Databases Figure 5.9: The benefits of dedicated search nodes Graph relationships You can exploit even more relationships between documents using the $graphLookup stage. This allows an arbitrary number of hops to be jumped from the results of $vectorSearch. If the customer’s data already contains relationships that can be traversed in a hierarchical manner, this is an immediate benefit to them. Just as you might define a relationship between a document and a page, you might recursively chunk a document into ever smaller chunks, relate each chunk to a parent document using a parent_id field, and embed those chunks. At query time, you could search against all of the chunks and recursively jump up all of the parent_id values to the desired level of resolution to provide to the LLM. Deployment options The simplest deployment model for getting started with Atlas Vector Search is to define a search index definition within your existing cluster or a new cluster. This can be configured using search index management commands for paid tier clusters or the UI/Atlas Administration API for shared tier clusters. When you feel confident in your vector search use case and are ready for increased usage or increased scale of ingested data, it is recommended to move to dedicated search nodes. Dedicated search resources provide a robust and scalable platform for serving demanding search workloads. This will allow for high-availability vector search, more cost-effective resource utilization, and resource isolation from your core database in a way that is more practical for production workloads, as visualized in Figure 5.9. Successfully deploying your AI application is the final hurdle. This section outlines various deployment options and provides guidance on estimating necessary resources to ensure optimal performance and scalability. Deployment Search nodes Atlas cluster Workload isolation Better performance at scale Higher availability Better query performance Scalability Scalability Scalability Scalability Search Database"></li><li class="carousel-slide" data-ssplayer-slide-index="89" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/90/1739580232671?e=1743033600&amp;v=beta&amp;t=albOpbBcNdDv5avI1hcfxOcBk2ju7oB_jMLd7_YIiqU" alt="Summary 89 Migrating to dedicated search nodes is a zero-downtime process that allows for your existing base cluster to continue to serve vector search queries as new resources are spun up and your indexes are built on them. Once that build process completes, $vectorSearch queries will be routed to your dedicated search nodes and the indexes on the original cluster will be deleted. Dedicated search nodes can be configured from the Cluster Configuration UI by following these steps:1. On the Create New Cluster/Edit Configuration page, change the radio button for AWS or Google Cloud for Multi-cloud, multi-region &amp; workload isolation to enabled. Toggle the radio button for Search Nodes for workload isolation to enabled. Select the number of nodes in the textbox. Check the agreement box. Select the right node for your workload. Click Create cluster. 2. 3. 4. 5. The current index type supported within Atlas Vector Search is HNSW, which is memory-resident. This means that you need approximately 3 KB of memory for every 768d vector you plan on indexing, scaling linearly with the number and dimensionality of vectors. If you expect your workload will have low query volume, it is recommended to select the cheapest option on M tier clusters that can allocate 50% of the available resources to storing the index inmemory. When using dedicated search nodes, 90% of the available RAM can be used to host the index. Note that when using M tier clusters, the index will need to be warmed into the cache using representative queries. For dedicated search nodes, the index will be automatically loaded into the cache upon an index build. If you expect your workload to have a high indexing or query concurrency, it is recommended to use dedicated search nodes with the high CPU option or to scale up the number of dedicated searchnodes in your replica set. This will scale up the number of available vCPUs to serve the $vectorSearch queries in a round-robin fashion. Summary In this chapter, you explored a variety of concepts related to vector search. The chapter delved into how high-dimensional vectors produced from embedding models can be useful measures of semantic similarity among the unstructured data passed into those models. It examined the HNSW index and how it can be used to accelerate vector similarity comparisons between a query vector and many indexed vectors. The chapter then illustrated how this type of index can be applied in various real-world contexts by large organizations, including such architecture patterns as RAG, semantic search, and RPA. Finally, the chapter reviewed some of the best practices for building vector search systems within MongoDB Atlas, ranging from ingestion time considerations, such as metadata extraction, to deployment model considerations, such as dedicated search nodes. Resource requirements"></li><li class="carousel-slide" data-ssplayer-slide-index="90" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/91/1739580232671?e=1743033600&amp;v=beta&amp;t=M-wk1CxukNYRE1_v82-mXXQ-jWFVhYJpXF7rjbUd4mU" alt="90 Vector Databases In the next chapter, you will discover the crucial aspects of designing AI/ML applications. You will learn how to effectively manage data storage, flow, freshness, and retention along with techniques to ensure robust security."></li><li class="carousel-slide" data-ssplayer-slide-index="91" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/92/1739580232671?e=1743033600&amp;v=beta&amp;t=s513isEeMdh72AG2haA-XZPjiQxrDksfX7wFIdAKN2M" alt="6 AI/ML Application Design As the landscape of intelligent applications evolves, their architectural design becomes pivotal for efficiency, scalability, operability, and security. This chapter provides a guide on key topics to consider as you embark on creating robust and responsive AI/ML applications. The chapter begins with data modeling, examining how to organize data in a way that maximizes effectiveness for three different consumers: humans, applications, and AI models. You will learnabout data storage, considering the impact of different data types and determining the best storage technology. You will estimate storage needs and determine the best MongoDB Atlas cluster configuration for your example application. As you learn about data flow, you will explore the detailed movement of data through ingestion, processing, and output to maintain integrity and velocity. This chapter also addresses data lifecycle management, including updates, aging, and retention, ensuring that data remains relevant and compliant. Security concerns are stretched further for AI/ML applications due to the risks of exposing data or logic to AI models. This chapter discusses security measures and role-based access control (RBAC) to protect sensitive data and logic integrity. You will also learn the best principles for data storage, flow, modeling, and security, providing practical advice to avoid common pitfalls. Throughout the chapter, you will use a fictitious news application called MongoDB Developer News (MDN), which will be like Medium.com, equipping you to create intelligent applications by using a practical example."></li><li class="carousel-slide" data-ssplayer-slide-index="92" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/93/1739580232671?e=1743033600&amp;v=beta&amp;t=03epuQ6uJ03x2P-zbIBhVot9KIQ1SRI3kyTPj2u0gbY" alt="92 AI/ML Application Design This chapter will cover the following topics: • Data modeling • Data storage • Data flow • • • Freshness and retention Security and RBAC Best practices The following are the prerequisites to follow along with the code in this chapter: • A MongoDB Atlas cluster M0 tier (free) should be sufficient • An OpenAI account and API key with access to the text-embedding-3-large model • A Python 3 working environment • Installed Python libraries for MongoDB, LangChain, and OpenAI • Atlas Search indexes and Vector Search indexes created on the MongoDB Atlas cluster This section delves into the diverse types of data required by AI/ML systems, including structured, unstructured, and semi-structured data, and how these are applied to MDN’s news articles. The following are short descriptions of each to set a basic understanding: • Structured data conforms to a predefined schema and is traditionally stored in relational databases for transactional information. It powers systems of engagement and intelligence. • Unstructured data includes binary assets, such as PDFs, images, videos, and others. Object stores such as Amazon S3 allow storing these under a flexible directory structure at a lower cost. • Semi-structured data, such as JSON documents, allow each document to define its schema, accommodating both common and unique data points, or even the absence of some data. MDN will store news articles, subscriber profiles, billing information, and more. For simplicity, in this chapter, you will focus on the data about each news article and related binary content (which would be images). Figure 6.1 describes the data model of the articles collection. Data modeling Technical requirements"></li><li class="carousel-slide" data-ssplayer-slide-index="93" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/94/1739580232671?e=1743033600&amp;v=beta&amp;t=FfGuntv8sIGPUHii41eis1UVsPsforqQxlfkH6EnPDg" alt="Figure 6.1: Schema for the articles collection Data modeling 93 The articles collection represents a news article with metadata, including creation details, tags, and contributors. All documents feature a title, summary, body content in HTML and plain text, and associated media elements such as images. To complete the MDN data model, you need to consider data that will also be represented and stored via embeddings. Text embeddings for article titles and summaries will enable semantic search, while image embeddings will help find similar artwork used across articles. Table 6.1 describes the data fields, embedding models to use, and their vector sizes. Enriching data with embeddings"></li><li class="carousel-slide" data-ssplayer-slide-index="94" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/95/1739580232671?e=1743033600&amp;v=beta&amp;t=0ehehlsQLbMZ3-Ja7E3izbQcgWOYMSA1Ppjn91oLxi8" alt="94 AI/ML Application Design Table 6.1: Embeddings for the articles collection Figure 6.2: Schema for articles with embeddings Type Text Text Imag e Field(s) title summary contents OpenAI CLIP Embedding model OpenAI 768 Vector size 1,024 Each article has a title and summary. Instead of embedding them separately, you will concatenate them and create one text embedding for simplicity. Ideally, for images, you would store the embedding with each content object in the contents array. However, support for fields inside arrays of objects for vector indexes is not available today in MongoDB Atlas and leads to the anti-pattern of bloated documents. The best practice is to store image embeddings in a separate collection and use the extended reference schema design pattern. You can learn more about indexing arrays with MongoDB, bloated documents, and the extended reference pattern from the links given in the Further Reading chapter of this book. Figure 6.2 shows the updated data model. text-embedding-3-large"></li><li class="carousel-slide" data-ssplayer-slide-index="95" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/96/1739580232671?e=1743033600&amp;v=beta&amp;t=VjGhEuvd4B0D8N-BBnREykMLs3X2FCneeRLdFgBfXwA" alt="Table 6.2: Vector search index definitions Data modeling 95 Collection: Vector index: Table 6.2 shows the corresponding vector indexes. Collection: Vector index: Before finalizing the data model, let’s consider search use cases for articles, and adapt the model once more. Here are some broader search use cases: • Find articles by matching lexically or semantically on title and summary, allowing filtering by brand and subscription type: This use case is called hybrid search and is covered in Chapter 5, Vector Databases. It combines semantic and lexical searches using reciprocal rank fusion. You can create a search index covering the title and summary fields for text search, and the brand and subscription_type fields for filtering. • Same as the first one and extend to include tags: For this use case, you can use the same index and add the tags field. You will also need a vector search index to cover the summary embedding. articles semantic_embedding_vix article_content_embeddings content_embedding_vix title + { &quot;fields&quot;: [ { &quot; n u m D i m e n s i o n s &quot; : 1 0 2 4 , &quot; p a t h &quot; : &quot; s e m a n t i c _ embedding&quot;, } &quot;similarity&quot;: &quot;cosine&quot;, &quot;type&quot;: &quot;vector&quot; } ] { &quot;fields&quot;: [ } { &quot; n u m D i m e n s i o n s &quot; : 7 6 8 , &quot; p a t h &quot; : &quot; c o n t e n t _ e m b e d d &quot; s i m i l a r i t y &quot; : &quot; c o s i n e &quot; , &quot; t y p e &quot; : &quot; v e c t o r &quot; } ] Considering search use cases"></li><li class="carousel-slide" data-ssplayer-slide-index="96" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/97/1739580232671?e=1743033600&amp;v=beta&amp;t=aXNRawdSXVYV1RgZCqDX0mQBwu9Nyb1FFG2V_EcsxJ0" alt="96 AI/ML Application Design Figure 6.3: Updated schema for articles with embeddings • Find other articles that use similar images, filtering by brand and subscription type: For this use case, vector search indexes on MongoDB Atlas support adding traditional fields for filtering. Since the image embeddings are stored in another collection, you will need to duplicate the article’s _id, brand, and subscription_type fields from the articles collection into the article_content_embeddings collection. Since there is already an _id field in this collection, you can create a composite primary key that includes the _id of the article and the _id of the content. Figure 6.3 shows the updated data model."></li><li class="carousel-slide" data-ssplayer-slide-index="97" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/98/1739580232671?e=1743033600&amp;v=beta&amp;t=GF8pURBFxt2KonmNKgMqmxtECEP35lJcGzas6CScXws" alt="Table 6.3: Updated vector search index definitions Data modeling 97 Collection: Vector index: Table 6.3 shows updated vector indexes. Collection: Vector index: articles semantic_embedding_vix article_content_embeddings content_embedding_vix { &quot;fields&quot;: [ embedding&quot;, { &quot;numDimensions&quot;: 1024, &quot;path&quot;: &quot;semantic_ type&quot;, } &quot;similarity&quot;: &quot;cosine&quot;, &quot;type&quot;: &quot;vector&quot; }, { &quot;path&quot;: &quot;brand&quot;, &quot;type&quot;: &quot;filter&quot; }, { &quot;path&quot;: &quot;subscription_ &quot;type&quot;: &quot;filter&quot; } ] { &quot;fields&quot;: [ { &quot;numDimensions&quot;: 768, &quot;path&quot;: &quot;content_ embedding&quot;, type&quot;, } &quot;similarity&quot;: &quot;cosine&quot;, &quot;type&quot;: &quot;vector&quot; }, { &quot;path&quot;: &quot;brand&quot;, &quot;type&quot;: &quot;filter&quot; }, { &quot;path&quot;: &quot;subscription_ &quot;type&quot;: &quot;filter&quot; }, { &quot;path&quot;: &quot;_id.article_id&quot;, &quot;type&quot;: &quot;filter&quot; } ]"></li><li class="carousel-slide" data-ssplayer-slide-index="98" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/99/1739580232671?e=1743033600&amp;v=beta&amp;t=00yOFLdZ2kpG7wOJLi7G3rjaHcDeG0nk_OiJ-Q6TNE0" alt="98 AI/ML Application Design Table 6.4: Text search index definition Collection: Search index: Table 6.4 shows the new text search index. You learned about writing vector search queries in Chapter 4, Embedding Models. To learn more about hybrid search queries, you can refer to the tutorial at https://www.mongodb.com/docs/atlas/ atlas-vector-search/tutorials/reciprocal-rank-fusion/. Now that you understand your data model and the indexes required, you need to consider the number of articles MDN will bear (including the sizes of embeddings and indexes), peak daily times, and more to determine the overall storage and database cluster requirements. articles lexical_six { &quot;mappings&quot;: { } &quot;dynamic&quot;: false, &quot;fields&quot;: { &quot;brand&quot;: { &quot;normalizer&quot;: &quot;lowercase&quot;, &quot;type&quot;: &quot;token&quot; }, &quot;subscription_type&quot;: { &quot;normalizer&quot;: &quot;lowercase&quot;, &quot;type&quot;: &quot;token&quot; }, &quot;summary&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;tags&quot;: { &quot;normalizer&quot;: &quot;lowercase&quot;, &quot;type&quot;: &quot;token&quot; }, &quot;title&quot;: { &quot;type&quot;: &quot;string&quot; } } }"></li><li class="carousel-slide" data-ssplayer-slide-index="99" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/100/1739580232671?e=1743033600&amp;v=beta&amp;t=9EQ6dm3m0Jz3ZfHSh498h-Diy0ZlBzxYdMwCxGc7Uuk" alt="Figure 6.4: MDN subscriber time zones and peak times Data storage 99 Data storage In this section, you will perform sizing, which is an educated estimate, for storage requirements. You will consider not just volume size and speed, but also several other aspects of the database cluster that are needed for harnessing the data of your application while following expected data access patterns. MDN plans to publish 100 articles daily. Keeping the articles from the last 5 years, the number of articles would total 182,500. With 48 million subscribers and 24 million daily active users, peakaccess occurs for 30 minutes daily across three major time zones, as shown in Figure 6.4. MongoDB Atlas provides two main cluster types: • Replica sets have a primary node for writes and secondary nodes for high availability, which can also be used for reads. These sets scale vertically and can also scale horizontally for reads by adding more nodes in the same or different cloud regions. First, you will estimate the total data size. Each article has one 1,024-dimension embedding for semantic search and five 768-dimension embeddings for image search, totaling 40 KB uncompressed (dimensions use the double type). With the title, summary, body (with and without markup), and other fields, the average article size will be about 300 KB uncompressed. Five years of articles will require about 100 GB uncompressed. With MongoDB’s WiredTiger compression (Snappy, zlib, and zstd are also available as compression options), this reduces to about 50 GB on disk. The defined vector indexes add about 3.6 GB. Images and binary assets will be stored in Amazon S3. For simplicity, you will not estimate the size of search and traditional indexes. You can safely say that MDN will need 80 to 100 GB on disk in MongoDB Atlas, which is very manageable by today’s cloud computing standards. Now, you will determine the most suitable MongoDB Atlas cluster configuration. Determining the type of database cluster"></li><li class="carousel-slide" data-ssplayer-slide-index="100" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/101/1739580232671?e=1743033600&amp;v=beta&amp;t=4RnpUXwtEMWK67GZc4bSI0jhQaY2LdKQ4AXUYqn9DTQ" alt="100 AI/ML Application Design Table 6.5: MongoDB Atlas storage types on AWS Storage types Standard IOPS Provisioned IOPS NVMe Lowest IOPS/storage 3,000 IOPS/10 GB 100 IOPS/10 GB 100,125 100% random read IOPS 35,000 write IOPS 380 GB Highest IOPS/storage 12,288 IOPS/4 TB 16,000 IOPS/14 TB* *Extended storage enabled 64,000 IOPS/4 TB 3,300,000 100% random read IOPS 1,400,000 write IOPS 4,000 GB MDN is a low-write, high-read use case. With only 100 articles added per day, there is minimal pressure on the storage system for writes. Table 6.5 shows the storage and IOPS options provided by MongoDB Atlas. As shown in Figure 6.4, there will be a 30-minutes peak period, during which 24 million users are expected to be active daily. So, you need to provision 6,000 IOPS, as shown in Table 6.6. This is based on subscriber distribution, memory versus disk reads, and each article requiring 3 IOPS for disk reads (150 KB compressed ÷ 64 KB I/O size of Amazon EBS). • Sharded clusters consist of multiple shards, each being a part of the overall dataset, and each being a replica set. They scale vertically and horizontally for both reads and writes. Shards can be placed in different cloud regions to enhance data locality and compliance. So, how can you determine whether a replica set is sufficient or a sharded cluster is needed? Key factors include the size of the dataset or the throughput of applications that can challenge the capacity of a single server. For example, high query rates can exhaust the server’s CPU capacity and working set sizes larger than the system’s RAM can stress the I/O capacity of disk drives. MDN publishes 100 articles per day, so sharding is not necessary for this reason. Other reasons for sharding include data governance and compliance and recovery point objective (RPO) and recovery time objective (RTO) policies, which are key metrics in disaster recovery and business continuity planning. None of these are applicable to MDN. Considering the small number of writes per second and manageable data size, it makes sense to use a replica set. You will now need to determine the amount of RAM and IOPS needed; both are key components for fast response times. Determining IOPS"></li><li class="carousel-slide" data-ssplayer-slide-index="101" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/102/1739580232671?e=1743033600&amp;v=beta&amp;t=hncHkc4b-SQ7fe7glUp7nGZJWMoL_Uw3kz45pCRVS4I" alt="Table 6.6: MDN global subscriber distribution Data storage 101 Region ^ ^ ^ ^ Zones overlapping at peak time Allocation 40% 20% 25% 15% DAU 9,600,00 0 4,800,00 0 6,000,00 0 3,600,00 0 20% reads from disk 1,920,000 960,000 1,200,000 720,000 Peak IOPS Disk reads/sec during peak time 1,067 533 667 400 6,000 Note You learned about HNSW or hierarchical navigable small worlds in Chapter 5, Vector Databases. IOPS required 3,200^ 1,600^ 2,000 1,200^ For data nodes, the Atlas M40 tier provides 16 GB of RAM. The MongoDB WiredTiger storage engine reserves 50% of (RAM - 1 GB) for its cache. With documents averaging 300 KB in size, the cache can hold approximately 28,000 documents. Keep in mind that traditional index sizes might slightly reduce this number. Given the addition of 100 new articles daily, the cache on an M40 tier can accommodate data for about 280 days, or roughly 9 months, which is more than sufficient for this example. The Search S40 tier offers 16 GB of RAM, 2 vCPUs, and 100 GB of storage. The HNSW graph, or the vector index, must fit in the memory. The minimum standard IOPS on any Atlas cluster on AWS is 3,000. To achieve 6,000 IOPS you would need to use an Atlas M50 tier with 2TB disk, which feels over-provisioned and would not provide low latency to all readers if deployed in a single cloud region. To address this, MDN will deploy the application stack in major geographies, enabling regional provisioning, workload distribution, and local reads for an optimal customer experience. With MongoDB Atlas, you can place vector search nodes across regions. The S40 tier offers 26,875 read IOPS, which is sufficient for this example, and a 2-node minimum per region, which ensures high availability. While Vector Search nodes will handle lexical, semantic, and image search, the full JSON document must be fetched from a MongoDB data node after matching. To fully support local reads, we must provision read-only nodes in the same regions and meet IOPS requirements. We can do this with the Atlas M40 tier. Having determined the IOPS needed, you now need to estimate RAM. AMER EMEA APAC LATAM Determining RAM"></li><li class="carousel-slide" data-ssplayer-slide-index="102" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/103/1739580232671?e=1743033600&amp;v=beta&amp;t=B6ND_SMrJeYyFaYfyEM9DyFkPWXyIKdh2urluNnCqxg" alt="102 AI/ML Application Design Table 6.7: MongoDB Atlas cluster configuration for MDN Region (primary region) EMEA (three included) Atlas base tier nodes M40 x2 M40 x2 M40 x2 MDN global cloud architecture Atlas read-only nodes x2 x2 x2 Atlas Vector Search nodes S30 x2 You have now determined the cluster configuration for MDN. Table 6.7 describes the MDN global cloud architecture, detailing the distribution of Atlas nodes across different regions. The AMER region, identified as the primary region, uses M40 tier nodes and S30 vector search nodes to serve writes and searches for the Americas, while the EMEA, APAC, and LATAM regions use M40 read-only nodes and S30 vector search nodes to serve local searches only for their respective region. Each region will need a deployment of the MDN application stack, as pictured in the global map in Table 6.7. One article uses 1 x 1,024 vector + 5 x 768 vectors = 19.5 KB. With 3.5 GB needed for 182,500 articles, 16 GB of RAM is more than sufficient for vector search and leaves room for the lexical search index. The S30 tier, which offers 4 GB of RAM, 1 vCPU, and 50 GB storage, is less costly, but note that more CPUs allows more concurrent searches. AMER APAC LATAM M40 S30 S30 S30 Final cluster configuration"></li><li class="carousel-slide" data-ssplayer-slide-index="103" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/104/1739580232671?e=1743033600&amp;v=beta&amp;t=_DPeGrenxTMTEszN7bMETNhlh0yvgNnrNv9xgidbewY" alt="Figure 6.5: Typical data flow in an AI/ML application Data flow 103 Handling static data sources The simplest way to import static data is to use mongoimport, which supports JSON, CSV, and TSV formats. It is ideal for initial loads or bulk updates as it can handle large datasets. Moreover, increasing the number of insertion workers to match the host’s vCPUs can boost import speed. mongoimport can also be used dynamically to update externally sourced data. You can build invocation commands at runtime and execute them as out-of-process tasks. Some video game companies use this method to update player profiles with purchase data from mobile app stores. Performance and availability versus cost Notice that additional read-only nodes were not provisioned in the AMER region, using the two secondary nodes as read-only instead. This saves costs due to MDN’s low write profile, despite potential resource competition. Provisioning only one M40 read-only node in other regions saves more costs but increases latency during maintenance windows, as reads will be rerouted. To protect against a complete AMER outage while adhering to best practices, consider provisioning five nodes across three regions and deploying the application stack in the two regions with two electable nodes each. Data flow involves the movement of data through a system, affecting the accuracy, relevance, and speed of the results delivered to consumers, which, in turn, influences their engagement. This section explores design considerations for handling data sources, processing data, prompting LLMs, and embedding models to enrich data using MDN as an example. Figure 6.5 illustrates this flow. Let's us begin with the design for handling data sources. Data can be ingested into MongoDB Atlas either statically (at rest) from files as it is, or dynamically (in motion), allowing for continuous updates, data transformation, and logic execution. Data flow Data sources Embedding creation Operational data enriched with vector embeddings + metadata Prompt engineering LLMs and embedding models Apps Data pre- processing Core data, metadata, and embeddings stored Prompt assembly Traditional request/response Similarity search results User request received, sent through embedding creation pipeline, and used for vector similarity search Response returned"></li><li class="carousel-slide" data-ssplayer-slide-index="104" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/105/1739580232671?e=1743033600&amp;v=beta&amp;t=M6dHGDLD5Oqm_VM7Z-lR8oiVpt2hL4bWMqP3xWafgJY" alt="104 AI/ML Application Design Table 6.8: Example of using mongoimport to merge data The Collection: Collection: after merge invocation to merge data matching on the field While mongoimport is a versatile tool for various data import needs, it does not support continuous synchronization, logic execution, or data transformations. You will now explore some methods that do support these functions. When original representations are stored or updated, their corresponding vector embeddings must be refreshed to accurately reflect the content. This can be done in the following ways: • Synchronously: Obtains the updated vector embedding before the database operation, writing both data and embedding together. This method is suitable for fast, simple embedding models Using MDN as an example, users can provide their GitHub ID when subscribing. With GitHub’s API, you can create a list of the programming languages used in the repositories that users own or have contributed to. A scheduled job can fetch this data periodically. The list of languages can then imported and merged into their profiles to recommend articles later. Table 6.8 demonstrates how you can do this. File: github-20240719.json mongoimport mdn.subscribers mdn.subscribers github_id mongoimport --uri=<connection string to Atlas cluster> --db=mdn --collection=subscribers --mode=merge --file=github-20240719.json --upsertFields=github_id --numInsertionWorkers=4 { &quot;_id&quot; : ObjectId(&quot;669…ab8&quot;), &quot;github_id&quot; : &quot;user1&quot;, … } { &quot;_id&quot; : ObjectId(&quot;669…ab9&quot;), &quot;github_id&quot; : &quot;user2&quot;, … } … { &quot;github_id&quot; : &quot;user1&quot;, &quot;languages&quot; : [&quot;python&quot;, &quot;csharp&quot;], …} { &quot;github_id&quot; : &quot;user2&quot;, &quot;languages&quot; : [&quot;python&quot;, &quot;cpp&quot;], …} … { &quot;_id&quot; : ObjectId(&quot;669…ab8&quot;), &quot;github_id&quot; : &quot;user1&quot;, &quot;languages&quot; : [&quot;python&quot;, &quot;csharp&quot;], … } { &quot;_id&quot; : ObjectId(&quot;669…ab9&quot;), &quot;github_id&quot; : &quot;user2&quot;, &quot;languages&quot; : [&quot;python&quot;, &quot;cpp&quot;], … } … Storing operational data enriched with vector embeddings"></li><li class="carousel-slide" data-ssplayer-slide-index="105" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/106/1739580232671?e=1743033600&amp;v=beta&amp;t=TK8m8-NgwH_vTVkWpb8vXFrJTixsap_CIwPL2GDLrnY" alt="Data flow 105 or when the model is locally hosted. However, it may fail if the response times of the embedding model vary. • Asynchronously: Ensures immediate consistency of primary data and allows for prompting the embedding model afterward. While this offers scalability and handles unpredictable models, it introduces latency during which embeddings are temporarily outdated. You can keep embeddings up to date asynchronously in MongoDB using the following four methods: • Kafka connector: You can facilitate data flow from Apache Kafka into MongoDB collections through the Kafka connector. It is a Confluent-verified connector and allows data to flow from Apache Kafka topics into MongoDB as a data sink and publishes changes from MongoDB to Kafka topics as a data source. To keep embeddings up to date, you would use the sink connector and develop a post-processor in Java. You can learn more about sink post-processors here: https://www.mongodb.com/docs/kafka-connector/v1.3/sink-connector/ fundamentals/post-processors/#sink-connector-post-processors. • Atlas Stream Processing: This method handles complex data streams with the same query API as MongoDB Atlas databases. It enables continuous aggregation and includes schema validation for message integrity and timely issue detection. Processed data can be written to Atlas collections, and they are integrated into Atlas projects and independent of Atlas clusters. Atlas Stream Processing logic is programmed in JavaScript using MongoDB aggregation syntax. For an example of using Atlas Stream Processing to handle embedding data, see https:// www.mongodb.com/solutions/solutions-library/rag-applications. Atlas Triggers: Atlas Triggers execute application and database logic by responding to events or following predefined schedules. Each Trigger listens for specific event types and is linked to an Atlas Function. When a matching event occurs, the Trigger fires and passes the event object to the linked Function. Triggers can respond to various events, such as specific operations in a collection, authentication events such as user creation or deletion, and scheduled times. They are fully managed instances of change streams but limited to JavaScript. For an example of using Atlas Triggers to keep embeddings up to date, see https://www.mongodb.com/ developer/products/atlas/semantic-search-mongodb-atlas-vector-search/ . • • Change streams: This method provides real-time access to data changes. Applications can subscribe to changes in a collection, database, or entire deployment and react immediately, with events processed in order and being resumable? Using the aggregation framework, change streams allow filtering and transforming notifications. They can be used with any programming language supported by an official MongoDB driver. However, they are not fully managed, requiring a running host to be maintained alongside the main application. Given that this book is written for Python developers, you will learn how to use a change stream written in Python. Table 6.9 shows a Python 3 change stream using LangChain and OpenAI to embed the title and summary of an MDN article. It is triggered for new articles or changes to the title or summary following the data model from Figure 6.3 and the vector index from Table 6.3."></li><li class="carousel-slide" data-ssplayer-slide-index="106" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/107/1739580232671?e=1743033600&amp;v=beta&amp;t=WnRKci7jKiytSELR0T-ItoOxCdt3_KDla2UkPgXxGys" alt="106 AI/ML Application Design import os from langchain_openai import OpenAIEmbeddings from pymongo import MongoClient from pymongo.errors import PyMongoError # Set the OpenAI API key as an environment variable os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;YOUR-OPENAI-API-KEY&quot; # Generate embeddings for the text embeddings = embedding_model.embed_documents(text) # Define the MongoDB Atlas connection string ATLAS_CONNECTION_STRING = &quot;YOUR-MONGODB_ATLAS-CONNSTRING&quot; # Select the 'articles' collection from the 'mdn' database coll = client[&quot;mdn&quot;][&quot;articles&quot;] # Create a filter to identify the document in the collection doc_filter = { &quot;_id&quot;: doc_id } # Create a MongoClient instance to connect to MongoDB Atlas client = MongoClient( ) ATLAS_CONNECTION_STRING, tls=True, tlsAllowInvalidCertificates=True # Define a function to handle changes detected in the MongoDB collection def handle_changes(change): # Extract the document ID from the change event doc_id = change[&quot;documentKey&quot;][&quot;_id&quot;] # Instantiate the OpenAIEmbeddings model with specified parameters embedding_model = OpenAIEmbeddings( ) model=&quot;text-embedding-3-large&quot;, dimensions=1024, disallowed_special=() # Combine the title and summary of the document into a single text string text = [change[&quot;fullDocument&quot;][&quot;title&quot;] + &quot; &quot; + change[&quot;fullDocument&quot;] [&quot;summary&quot;]]"></li><li class="carousel-slide" data-ssplayer-slide-index="107" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/108/1739580232671?e=1743033600&amp;v=beta&amp;t=nptp-7vM6kQ8fVCWQnl91boa84AR4CEv0PEypb3FWT0" alt="Data flow 107 print(f&quot;Updated embeddings for document {doc_id}&quot;) # Update the document in the collection with the new embeddings coll.update_one(doc_filter, set_fields) # Start monitoring the MongoDB collection for changes try: # Define a stream filter to match insert and update operations affecting the title or summary fields title&quot;: { summary&quot;: { stream_filter = [ { &quot;$match&quot;: { &quot;$or&quot;: [ {&quot;operationType&quot;: &quot;insert&quot;}, { &quot;$and&quot;: [ {&quot;operationType&quot;: &quot;update&quot;}, { &quot;$or&quot;: [ { &quot;updateDescription.updatedFields. &quot;$exists&quot;: True } }, { &quot;updateDescription.updatedFields. &quot;$exists&quot;: True } }, ] }, ] }, # Create an update document to set the 'semantic_embedding' field with the generated embeddings set_fields = { &quot;$set&quot;: { &quot;semantic_embedding&quot;: embeddings[0] } }"></li><li class="carousel-slide" data-ssplayer-slide-index="108" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/109/1739580232671?e=1743033600&amp;v=beta&amp;t=M6whyyXu3-q4xzZr21qe2iTb8n3Od3sdEbpgwjfdwxs" alt="108 AI/ML Application Design Table 6.9: Change stream written in Python to set or update embeddings ] } } ] finally: # Close the MongoDB client connection client.close() except PyMongoError as e: # Print an error message if a PyMongoError occurs print(f&quot;An error occurred: {e}&quot;) # Open a change stream to watch for changes in the collection with coll.watch(stream_filter, full_document=&quot;updateLookup&quot;) as stream: print(&quot;Listening for changes...&quot;) for change in stream: print(f&quot;Change detected: {change}. Processing&quot;) handle_changes(change) The primary concern is to ingest and update new data in real time, making it available across all cloud regions. For the news site, this means new articles and their vector embeddings should be promptly persisted and replicated for global access. To achieve this with a distributed data model and application, use an ACID transaction to ensure that the article and its content embeddings are written together as a single unit. For an example of creating MongoDB transactions in Python, see https://learn.mongodb.com/learn/course/ mongodb-crud-operations-in-python/lesson-6-creating-mongodb-transactions- in-python-applications/learn?page=2. Now that you have learned how to handle the data flow for setting or updating embeddings, you will learn about data freshness and retention, which are essential for delivering relevant and timely content. Fresh data and effective retention strategies ensure that your content is relevant and delivered on time. Freshness keeps users engaged with the latest articles, comments, and recommendations. Retention strategies manage the data lifecycle, preserving valuable historical data for analytics while purging obsolete data. This section explores methods for ensuring up-to-date content and efficient data flow. Freshness and retention Real-time updates"></li><li class="carousel-slide" data-ssplayer-slide-index="109" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/110/1739580232671?e=1743033600&amp;v=beta&amp;t=JtGXo4YFbiRiPGkxhYXK8ktBQYtibDmdFoytUWI0sEc" alt="Freshness and retention 109 Next, balance data reliability, consistency, and performance in a distributed setup using MongoDB’s tunable consistency with writeConcern, readConcern, and readPreference. These modifiers help to ensure data integrity and quick access. The following is an explanation of these modifiers, but for a deeper understanding, you can visit https://www.mongodb.com/docs/manual/core/ causal-consistency-read-write-concerns/: • writeConcern:majority ensures data consistency and durability by acknowledging write operations only after data is written to the majority of replica set members, reducing the risk of data loss during failures. It is the default write concern. readConcern:majority provides read consistency by ensuring that read operations return the most recent data acknowledged by the majority of the replica set members, providing a consistent view of the data across the application. • • readPreference:nearest optimizes latency by directing read operations to the replica set member with the lowest network latency. For MDN, this minimizes response times by allowing each regional application deployment to read from the nearest MongoDB data and vector nodes, and balancing consistency and performance. Now that you have learned how to ensure data availability and speed, the next focus is on data lifecycle management, a key aspect of data freshness and retention. Data lifecycle refers to the various stages data goes through from creation to deletion, and how it may traverse and change systems or storage formats, including when data is archived or deleted. As latest content is added, older content may become less relevant. For example, older articles can be moved to an archive database or cold storage, reducing storage costs and optimizing active database performance. However, moving data to cold storage may reduce search capabilities compared to the operational database. Here are three approaches for handling the data lifecycle, along with their trade-offs:• All data in the operational cluster: Keeping all data in the operational cluster is the most performant but costly approach, suitable for scenarios where most data is frequently accessed, such as global online games, authentication providers, or financial platforms. MongoDB Atlas supports this with sharded clusters and global clusters. Global clusters allocate data zones to cloud regions for capacity management and data locality. • Active and historic operational data clusters: This involves using high-performance hardware for recent data and less capable hardware for older data, balancing functionality, and cost savings. With MongoDB Atlas, data can be moved from active to historic cluster(s) using Cluster-to-Cluster Sync and TTL indexes. Other platforms such as Apache Kafka, Confluent, and Striim also support this method. Data lifecycle"></li><li class="carousel-slide" data-ssplayer-slide-index="110" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/111/1739580232671?e=1743033600&amp;v=beta&amp;t=wbCnzWhRa3FcHEpxXrvEDDcrS0k7GV9QbLgYW3hZVJc" alt="110 AI/ML Application Design • Active data cluster and historical storage: Full historical data can be offloaded to cold storage while retaining key fields in the operational cluster, allowing for full or limited query and search capabilities. For MDN, this ensures that users can find historical articles through lexical semantic searches, with full articles stored in cold storage and accessed when needed. With MongoDB Atlas, this can be achieved using Online Archive and Data Federation. Online Archive automatically moves data from the cluster to lower-cost cloud storage based on the set expiration. Data Federation allows transparent querying of both clusters and the archive, regardless of the source. This section covered data lifecycle management, emphasizing how data is managed from creation to archival. You learned about three strategies: maintaining all data in the operational cluster for maximum performance, separating active and historical data to balance cost and performance, and offloading historical data to cold storage while retaining some search functionality. Now, you will learn about upgrading embedding models. OpenAI superseded the text-search-davinci-*-001 model with text- embedding-ada-002 on December 15, 2022, and subsequently with text-embedding-small/large on January 25, 2024. It is likely that by the time you read this book, these models will be replaced too. As you learned in the Chapter 4, Embedding Models, embeddings from one model are not compatible with another. Re-embedding previously indexed data may be necessary as newer models areadopted. This is a resource-intensive activity that requires design considerations upfront. You will need to choose an approach toward adopting new embedding models. You can either continue using the existing vector fields and perform lengthy all-or-nothing upgrades, double-embed for a period, or implement a gradual upgrade. Let's explore these three approaches: • Use existing vector fields: This approach keeps the application code intact but requires downtime to re-embed data and replace vector indexes. This approach is suitable if the re-embedding and re-indexing time fits within your allowable downtime windows. • Double-embed temporarily: This approach double embeds fields for new or modified data using the old and new model. It uses a background job to add new embeddings for data that is not modified. When all data has double embeddings, the application will be updated and deployed to use the new embeddings. Once stable, the deprecated vectors and indexes can be removed with another background job. Ensure sufficient disk space and memory for when two sets of vectors coexist. This approach is suitable if the downtime windows are small and only accommodate application deployment times. • Gradual upgrade: This approach is architecturally complex. Vector creation and search can be moved to a microservice. By leveraging MongoDB’s flexible data model, the service adds new vectors and retires old ones when documents change (such as a non-blocking lazy schema change). A background job handles untouched documents. For searches, results from searching using both vectors are combined (such as the reciprocal rank fusion approach). Adopting new embedding models"></li><li class="carousel-slide" data-ssplayer-slide-index="111" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/112/1739580232671?e=1743033600&amp;v=beta&amp;t=oV7aa8ublwdTEcINfh8aFXef80BdZaLkL1_4bq3iCKc" alt="Security and RBAC 111 Using the filter type of MongoDB’s vector indexes (shown in Table 6.3), you can introduce a new field to distinguish between documents with old and new vectors and implement the union. Eventually, old vectors and indexes can be dropped, and you can remove unneeded logic. This approach is suitable if no downtime is allowed. By addressing these three main concerns—data ingestion and real-time updates, managing the data lifecycle and aging, and upgrading embedding models—your application can ensure that its data remains fresh and relevant, providing an optimal platform and striving for the best user experience. Now, you will learn about security and its considerations for AI-intensive applications. Security and RBAC Security measures protect data from unauthorized access and breaches, while RBAC ensures appropriate access levels based on roles. Here are key security and RBAC strategies to protect data integrity and privacy: • Data encryption and secure storage: Encrypting data at rest and in transit is crucial for securing an application. Encryption at rest protects data from unauthorized access, while encryption in transit secures data as it moves between users and the application. MongoDB Atlas offers built-in integration with AWS Key Management Service (AWS KMS) for encryption at rest and TLS/SSL out of the box for data in transit. • Access controls and user authentication: RBAC manages permissions, ensuring that users access only necessary data and functionalities. In the case of MDN, separate roles, such as editors and readers, require various levels of access. Different database users on MongoDB can be set up with distinct levels of permissions following the principle of least privilege. For example, only the application identity used by the microservice that embeds data would have write permissions to the collections where embeddings are stored, while the application identity used by human actors would only have read permissions. • Monitoring and auditing: Continuous monitoring and auditing detect and respond to security incidents in real time. Monitoring tools and audit logs track user activities and identify unusual access patterns. MongoDB Atlas offers advanced monitoring and alerting capabilities, allowing administrators to set up alerts for suspicious activities. Regularly reviewing audit logs ensures compliance with security policies and provides insights for improving security. • Data backup and recovery: Maintain data integrity and availability with regular backups to minimize downtime and loss during security breaches or incidents. MongoDB Atlas offers automated backup solutions with snapshots, ensuring quick recovery. If encryption at rest is enabled (for example, AWS KMS), embeddings and operational data are encrypted under the same key in both volumes and backups. While there are many security-related concerns, the ones just covered should suffice to start building AI applications. Ensuring security is a continuous effort that organizations must adopt and enforce to maintain compliance, foster user trust, and safeguard application integrity."></li><li class="carousel-slide" data-ssplayer-slide-index="112" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/113/1739580232671?e=1743033600&amp;v=beta&amp;t=l-eqE9NpOHgjXbidxtCiL3i6H7bd6TySYre5GXlBPSs" alt="112 AI/ML Application Design Best practices for AI/ML application design This section covers best practices for the five concerns covered in this chapter—data modeling, data storage, data flow, data freshness and retention, and security and RBAC. These guidelines will help ensure that your application is efficient, scalable, and secure, providing a solid foundation for building reliable and high-performing AI apps. Here are the top two best practices for each aspect of your AI/ ML application design. 1. Data modeling: The following techniques ensure efficiency and performance for handling embeddings: &nbsp; Embeddings in separate collections: Store embeddings in a separate collection to avoid bloated documents, especially when multiple embeddings and nested indexing limitations are involved. Duplicate fields to ensure efficient filtering and maintain performant searches. &nbsp; Hybrid search: Combine semantic and lexical searches using reciprocal rank fusion. This hybrid approach boosts search functionality by leveraging the strengths of both. 2. Data storage: To optimize database cluster sizing, implement the following best practices: &nbsp; Sufficient IOPS and RAM based on peak usage: Calculate required IOPS based on peak access times and application read/write patterns. Ensure data and search nodes have enough RAM to handle the caching and indexing needs of the most requested data. &nbsp; Local reads: Deploying nodes across regions helps minimize read latency and enhances the user experience. Ensure that each region has all the nodes required to fully serve data locally. 3. Data flow: Consider the following strategies for harnessing data flow effectively: &nbsp; Asynchronous embedding updates: Ensure primary data consistency by updating vector embeddings asynchronously. This method accommodates scalability and unpredictable model response times, although it introduces temporary latency. &nbsp; Dynamic data handling: Leverage technologies such as change streams, Atlas Triggers, Kafka, and Atlas Stream Processing to handle continuous updates, transformations, and logic execution. 4. Data freshness and retention: The following best practices can ensure that your application is relevant and prompt: &nbsp; Up-to-date embedding models: Embeddings from one model are not compatible with another. Plan for model upgrades during downtime if possible, or consider gradual upgrades, which are architecturally complex but require no downtime. Leverage MongoDB’s flexible data model to transition between embeddings. &nbsp; Data tiering: Implement a data aging strategy by moving older data to an archive cluster or cold storage while keeping recent data in high-performance clusters. Use broader MongoDB Atlas features such as Online Archive, Data Federation, and more for effective data tiering."></li><li class="carousel-slide" data-ssplayer-slide-index="113" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/114/1739580232671?e=1743033600&amp;v=beta&amp;t=IcMxr7CGQnz_GzThCAXB5S_tk5PoJuLW1SX_g8PL8tY" alt="Summary 113 5. Security and RBAC: Following are the best practices for ensuring the security of your data: &nbsp; RBAC: Assign role-based permissions and follow the principle of least privilege (PoLP), ensuring that users and entities access only necessary data and actions. For instance, code embedding data should have write access only to embedding collections. &nbsp; Encryption and storage: Turn on encryption at rest and integrate with KMS to ensure that all data volumes and backups are encrypted with your own key. Implementing these best practices boosts the efficiency, scalability, and security of your AI/ML applications. Though just a starting point, these guidelines lay a solid foundation for building reliable, high-performing systems. With these best practices, you can navigate the complexities of modern AI and prepare your applications for long-term success and adaptability in a rapidly evolving tech landscape. This chapter covered critical architectural considerations for developing intelligent applications. You learned about data modeling and how to evolve your model to fulfill use cases, address technical limitations, and consider patterns and anti-patterns. This approach ensures that data is not only useful but also accessible and optimally utilized across various components of your AI/ML system. Data storage was another key aspect of this chapter, focusing on the selection of appropriate storage technologies based on different data types and the specific needs of the application. It highlighted the importance of accurately estimating storage requirements and other aspects of choosing the right MongoDB Atlas cluster configuration. The fictitious example of the MDN application served as a practical case study, illustrating how to apply these principles in a real-world scenario. The chapter also explored the flow of data through ingestion, processing, and output to ensure data integrity and maintain the velocity of data operations. This chapter also addressed data lifecycle management, including the importance of data freshness and retention. You learned strategies for managing updates and changing embedding models used by your application. Security is a paramount concern in AI/ML applications, and you learned brief but important points about protecting the integrity of data and application logic. Concluding with a compilation of best practices, this chapter summarized key principles from data modeling, storage, flow, and security, offering practical advice to avoid common pitfalls and enhance the development of robust AI/ML applications. In the next chapter, you will explore different AI/ML frameworks, Python libraries, and publicly available APIs and other tools. Summary"></li><li class="carousel-slide" data-ssplayer-slide-index="114" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/115/1739580232671?e=1743033600&amp;v=beta&amp;t=KBadYbMCxrSNhgZgNVmb0JmRq4os8Gz6qd208LYAszc" alt="115"></li><li class="carousel-slide" data-ssplayer-slide-index="115" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/116/1739580232671?e=1743033600&amp;v=beta&amp;t=tyWCOSip7acoFoIJSh54Ts2BMAwXtEBv4rnEEDCrzuo" alt="Part 2 Building Your Python Application: Frameworks, Libraries, APIs, and Vector Search This following set of chapters will equip you with the necessary tools for AI development through detailed instructions and examples on enhancing developer and user experience with Python and retrieval-augmented generation. This part of the book includes the following chapters: • Chapter 7, Useful Frameworks, Libraries, and APIs • Chapter 8, Implementing Vector Search in AI Applications"></li><li class="carousel-slide" data-ssplayer-slide-index="116" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/117/1739580232671?e=1743033600&amp;v=beta&amp;t=pydkNnl1aalf-Az1JnxsMia8EqDr1hYSS1Yqa3Kn32s" alt="117"></li><li class="carousel-slide" data-ssplayer-slide-index="117" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/118/1739580232671?e=1743033600&amp;v=beta&amp;t=h7UzXkq7nvGXM1zy1f0b0dK-kjsM_xvek0oRDWTJqLc" alt="7 Useful Frameworks, Libraries, and APIs As you might expect, Python is the most popular programming language for building intelligent AI applications. This is due to its flexibility and ease of use, as well as for its vast number of AI and machine learning (ML) libraries. Python has a specialized library for nearly all the necessary tasks required to build a generative AI (GenAI) application. In Chapter 1, Getting Started with Generative AI, you read about the GenAI stack and the evolution of AI. Like the AI landscape, the Python library and framework space also went through an evolution phase. Earlier, libraries such as pandas, NumPy, and polars were used for data cleanup and transformation work, while PyTorch, TensorFlow, and scikit-learn were used for training ML models. Now, with the rise of the GenAI stack, LLMs, and vector databases, a new type of AI framework has emerged. These new libraries and frameworks are designed to simplify the creation of new applications powered by LLMs. Since building GenAI applications requires the seamless integration of data from many sources and the use of diverse AI models, these AI frameworks provide built-in functionalities to facilitate acquiring, migrating, and transforming data. This chapter delves into the world of AI/ML frameworks, exploring their importance and highlighting why Python has emerged as the go-to language for AI/ML development. By the end of this chapter, you’ll be able to understand the most popular frameworks and libraries, as well as how they help you—the developer—build your GenAI application."></li><li class="carousel-slide" data-ssplayer-slide-index="118" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/119/1739580232671?e=1743033600&amp;v=beta&amp;t=XA-i0mdIHowIYEhL-LDeYsFsJQPGjMKKsv2HsSjszAM" alt="118 Useful Frameworks, Libraries, and APIs This chapter will cover the following topics: • AI/ML frameworks Python libraries Publicly available APIs and other tools • • To perform the steps shown in this chapter, you will need the following: • Latest major version of Python. • A free tier Atlas cluster running MongoDB version 6.0.11, 7.0.2, or later. • Your current IP address added to your Atlas project access list. • An environment set up to run Python code in an interactive environment, such as Jupyter Notebook or Colab. This chapter uses Jupyter Notebook. Python has established itself as the go-to programming language in various fields, but most notably in AI, ML, and building applications powered by large language models (LLMs). Python offers simplicity, readability, and a robust ecosystem of libraries, making it an ideal choice for all kinds of users, whether they are developers, researchers, or even students just getting started with programming. Python has also emerged as the language of choice for building new LLM-powered applications, underscoring Python’s usefulness, popularity, and versatility. In this section, you will learn some of the reasons that make Python a great choice for building modern AI-powered applications:• Simplicity and readability: Python’s syntax is designed to be intuitive and clear, which is one of its core strengths. Python can represent complex algorithms and tasks in a few lines of code that are easily readable and understandable. • Rich ecosystem of libraries and frameworks: Python offers an extensive range of libraries and frameworks specifically designed for AI/ML use cases. Libraries such as TensorFlow, PyTorch, and scikit-learn have traditionally been popular for ML tasks. Hugging Face’s Transformers library has also become an indispensable part of the developer workflow for building modern LLM-powered applications. It provides pre-trained models and straightforward APIs to fine-tune models for specific tasks. These libraries not only accelerate development time but also provide cutting-edge solutions to developers across the world. Python for AI/ML Technical requirements"></li><li class="carousel-slide" data-ssplayer-slide-index="119" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/120/1739580232671?e=1743033600&amp;v=beta&amp;t=dKWMYDvvkmq0C9iWpDqLoE239vWF9Ia3aJASudbYFpM" alt="AI/ML frameworks 119 AI/ML frameworks are essential tools that streamline the development and deployment of ML models, providing pre-built algorithms, optimized performance, and scalable solutions. They enable developers to focus on refining their models and GenAI applications rather than getting bogged down by low-level implementations. Using frameworks ensures efficiency, adaptability, and the ability to harness cutting-edge AI advancements. Developers should be interested in these frameworks as they also reduce development time and enhance the potential for breakthroughs in GenAI. MongoDB has integrations with many AI/ML frameworks that may be familiar to developers, such as LangChain, LlamaIndex, Haystack, Microsoft Semantic Kernel, DocArray, and Flowise. In this section, you will learn about LangChain, one of the most popular GenAI frameworks. Although it is very popular, it is certainly not the only popular framework. If you are interested in other frameworks, you can check out the documentation linked in the Appendix: Further Reading chapter at the end of this book or see the latest list of supported AI/ML frameworks for Python at https://www.mongodb.com/docs/languages/python/. • Strong community and support: Python is one of the most popular programming languages in the world, and hence has a huge community. According to the Stack Overflow survey 2023 (https://survey.stackoverflow.co/2023/), it’s the second most popular programming language after JavaScript (excluding HTML/CSS). This strong and large community provides a wealth of resources, including tutorials, discussion forum engagements, and open source projects, which offer a helpful support system for someone working on building modern applications. • Integration with other technologies: Python’s ability to integrate seamlessly with other technologies and programming languages makes it a great choice for AI/ML tasks and building LLM-powered applications. For example, Python can easily interface with programming languages such as C/C++ for performance-critical tasks. It also interfaces well with languages such as Java and C#. This flexibility of Python is helpful for deploying LLM-powered applications in diverse environments, ensuring that Python can be part of large heterogeneous systems. • Rapid prototyping and experimentation: Building a sophisticated AI/ML-powered application requires many iterations of tests, experiments, and fine-tuning. Python allows developers to quickly build prototypes in a few lines of code. Easy testing and debugging also help to prototype a quick solution. Python’s interactive environments, such as Jupyter Notebook, provide an excellent platform for this purpose. With Python, developers building LLM-powered applications can quickly test hypotheses, visualize data, and debug code in an interactive manner. Python combines speed, simplicity, specialized libraries and frameworks, and strong community support with easy integration with other languages and technologies, all of which make it an excellent choice for building modern LLM-powered applications. AI/ML frameworks"></li><li class="carousel-slide" data-ssplayer-slide-index="120" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/121/1739580232671?e=1743033600&amp;v=beta&amp;t=_oRDsnjqhKUjWLlSovjgjwmZ-7H325ndAz9E4_ZOtgs" alt="120 Useful Frameworks, Libraries, and APIs LangChain LangChain is a framework for developing applications powered by LLMs. LangChain simplifies every stage of the LLM application lifecycle. It enables building applications that connect external sources of data and computation to LLMs. The basic LLM chain relies solely on the information provided in the prompt template to generate a response, and the concept of a LangChain allows you to extend these chains for advanced processing. In this section, you will learn how to use LangChain to perform semantic search on your data and build a retrieval-augmented generation (RAG) implementation. Before you begin, make sure you have all the necessary tools installed and set up on your computer, as listed in the Technical requirements section of this chapter. Getting started with LangChain Perform the following steps to set up your environment for LangChain: 2. 3. Run the following code to import the required packages: 1. Start by installing the necessary dependencies: Make sure your connection string for the drivers is in the following format, which has both username and password included in the connection string: After you have imported the necessary packages, make sure the environment variables are set properly. You have two important secrets to store as environment variables: your OpenAI API key and MongoDB Atlas connection string. Run the following command to store your OpenAI API key as an environment variable: os.environ[&quot;OPENAI_API_KEY&quot;] = getpass.getpass(&quot;OpenAI API Key:&quot;) import getpass, os, pymongo, pprint from langchain_community.document_loaders import PyPDFLoader from langchain_core.output_parsers import StrOutputParser from langchain_core.runnables import RunnablePassthrough from langchain_mongodb import MongoDBAtlasVectorSearch from langchain_openai import ChatOpenAI, OpenAIEmbeddings from langchain.prompts import PromptTemplate from langchain.text_splitter import RecursiveCharacterTextSplitter from pymongo import MongoClient pip3 install --quiet --upgrade langchain==0.1.22 langchain- mongodb==0.1.8 langchain_community==0.2.12 langchain-openai==0.1.21 pymongo==4.5.1 polars==1.5.0 pypdf==3.15.0 mongodb+srv://<username>:<password>@<clusterName>.<hostname>.mongodb. net"></li><li class="carousel-slide" data-ssplayer-slide-index="121" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/122/1739580232671?e=1743033600&amp;v=beta&amp;t=IhXjz_2uL4WEKlcBAh377XF8cYv2Si2Md2DwE1T5aEo" alt="AI/ML frameworks 121 4. 5. Run the following command to store your MongoDB Atlas connection string as an environment variable: You are now ready to connect to the MongoDB Atlas cluster. Next, you’ll instantiate the MongoClient and pass your connection string to establish communications with your MongoDB Atlas database. Run the following code to establish the connection: Next, you’ll specify the name of the database and the collection you want to create. In this example, you’ll create a database named langchain_db and a collection called test. You’ll also define the name of the vector search index to create and use with the following code: With these steps, you’ve set up the basics of connectivity. Now that you have the bare bones of your database, you’ll want to define what your application does. In this case, you will do the following: 1. 2. Fetch a publicly accessible PDF document. Split it into smaller chunks of information for easy consumption by your GenAI application. 3. Upload the data into the MongoDB database. This functionality is not something you have to build from scratch. Instead, you’ll use the free, open source library integration provided by LangChain called PyPDFLoader, which you imported in Step 2 earlier in this section. # Connect to your Atlas cluster client = MongoClient(ATLAS_CONNECTION_STRING) # Define collection and index name db_name = &quot;langchain_db&quot; collection_name = &quot;test&quot; atlas_collection = client[db_name][collection_name] vector_search_index = &quot;vector_index&quot; ATLAS_CONNECTION_STRING = getpass.getpass(&quot;MongoDB Atlas SRV Connection String:&quot;)"></li><li class="carousel-slide" data-ssplayer-slide-index="122" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/123/1739580232671?e=1743033600&amp;v=beta&amp;t=xU35p6txZb4oL-kZElkGEQi4sTnplwryZDCEbHvQH88" alt="122 Useful Frameworks, Libraries, and APIs Fetching and splitting public PDF documents Using PyPDFLoader to fetch publicly available PDFs is quite simple. In the following code, you will fetch a publicly accessible PDF document and split it into smaller chunks that you can later upload into your MongoDB database: You will then receive the following output: With this code, you first instantiated PyPDFLoader and then passed it the URL to the publicly accessible PDF file: https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/ RE4HkJP. Next, you loaded the fetched PDF file into the data variable. After that, you split the PDF file’s text into smaller chunks. For this example, you set the chunk size to 200 characters and allowed an overlap of 20 characters between chunks. The overlap maintains the context between chunks. Note that this number is not arbitrary, and there are many opinions about what your chunking strategy should be. Some of those resources are discussed in the Appendix: Further Reading chapter of this book. You stored the split chunks in the docs variable and printed the first chunk of the split document. This indicates that your output request via the print command was successful, and you can easily confirm whether the information is correct for this entry. # Print the first document docs[0] # Split PDF into documents text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_ overlap=20) docs = text_splitter.split_documents(data) # Load the PDF loader = PyPDFLoader(&quot;https://query.prod.cms.rt.microsoft.com/cms/api/am/ binary/RE4HkJP&quot;) data = loader.load() Document(metadata={'source': 'https://query.prod.cms.rt.microsoft.com/ cms/api/am/binary/RE4HKJP', 'page': 0}, page_content='Mong oDB Atlas Best Practices January 20 19A MongoD B White P aper')"></li><li class="carousel-slide" data-ssplayer-slide-index="123" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/124/1739580232671?e=1743033600&amp;v=beta&amp;t=eef9DqSm9Ep_4W03T30RTGSF8GpusBH3BrOSM_IrJwk" alt="AI/ML frameworks 123 Creating the vector store After you have split your documents into chunks, you will instantiate the vector store with the following code: In the preceding code, you created a vector store named vector_store using the MongoDBAtlasVectorSearch.from_documents method and specified various parameters: • documents = docs: The name of the document that you want to store in your vector database • embedding = OpenAIEmbeddings(disallowed_special=()): The class that generates vector embeddings for the documents using OpenAI’s embedding model • collection = atlas_collection: The Atlas collection where documents will be stored • index_name = vector_search_index: The name of the index to use for querying the vector store You’ll also need to create your Atlas Vector Search index in the MongoDB database. For explicit instructions on how this is done, see Chapter 8, Implementing Vector Search in AI Applications. This must be completed before you can successfully run the previous code. When you are creating a Vector Search index, use the following index definition: { &quot;fields&quot;:[ } { &quot;type&quot;: &quot;vector&quot;, &quot;path&quot;: &quot;embedding&quot;, &quot;numDimensions&quot;: 1536, &quot;similarity&quot;: &quot;cosine&quot; }, { &quot;type&quot;: &quot;filter&quot;, &quot;path&quot;: &quot;page&quot; } ] # Create the vector store vector_store = MongoDBAtlasVectorSearch.from_documents( ) documents = docs, embedding = OpenAIEmbeddings(disallowed_special=()), collection = atlas_collection, index_name = vector_search_index"></li><li class="carousel-slide" data-ssplayer-slide-index="124" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/125/1739580232671?e=1743033600&amp;v=beta&amp;t=Jx8XJMfModid3gnmOeCYNCy_ltNACpp-66H_w2_KTI4" alt="124 Useful Frameworks, Libraries, and APIs You pass the query to the similarity_search_with_score function and specify the k as 3 to limit the number of documents to return to 3. Then, you can print the output: parameter This index defines two fields: • Embedding field: A vector type field for storing embeddings created using OpenAI’s text- embedding-ada-002 model. It has 1,536 dimensions and uses cosine similarity to measure similarity. You may also want to consider other newer models from OpenAI, text-embedding- 3-small and text-embedding-3-large, which are optimized for different use cases and therefore have a different number of dimensions. See https://platform.openai.com/ docs/guides/embeddings for more details as well as current options. • Page field: A filter type field used for pre-filtering data based on the page number in the PDF. Now, you can run your code successfully, fetch a publicly available PDF, chunk it into smaller portions of data, and store them in a MongoDB Atlas database. With these steps accomplished, you can conduct additional tasks, such as running queries to perform semantic search on your data. You can learn about basic semantic search in Chapter 8, Implementing Vector Search in AI Applications, and Chapter 10, Refining the Semantic Data Model to Improve Accuracy. For more information on this topic, you can also consult the official documentation from LangChain, available at https://python.langchain.com/v0.2/docs/integrations/vectorstores/ mongodb_atlas/#pre-filtering-with-similarity-search. Next, let’s cover some specific LangChain functionalities that you will find most useful when building GenAI applications. LangChain provides some particularly helpful methods to perform semantic search on your data and return a score. This score refers to the measure of relevance between the query and the matching documents based on their semantic content. You can use this score when you want to return more than one result to your users and also limit the number of results. For example, this score can prove useful in returning the top three most relevant pieces of content about a topic. The method that you will use here is similarity_search_with_score: LangChain semantic search with score query = &quot;MongoDB Atlas security&quot; results = vector_store.similarity_search_with_score( ) pprint.pprint(results) query = query, k = 3"></li><li class="carousel-slide" data-ssplayer-slide-index="125" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/126/1739580232671?e=1743033600&amp;v=beta&amp;t=FZP6sEW9_CybYRm2XocX9HBEQODuuYlkdIQQLYYotjs" alt="AI/ML frameworks 125 pprint.pprint(results) query = &quot;MongoDB Atlas security&quot; results = vector_store.similarity_search_with_score( ) query = query, k = 3, pre_filter = { &quot;page&quot;: { &quot;$eq&quot;: 17 } } [(Document (page_content='To ensure a secure system right out of the box, \nauthentication and IP Address whitelist ing are\nautomatically enabled. \nReview the security section of the MongoD B Atlas', metadata={'_id': {'Soid&quot;: &quot;667 20a81b6cb1d87043c0171'), 'source': 'https://query.prod.cms. rt.microsoft.com/cms/api/am/binary/RE4HKJP', 'page': 17}), 0.9350903034210205), (Document(page_content='MongoD B Atlas team are also monitoring the underlying\ninfrastructure, ensuring that it i s always in a healthy state. \nApplication Logs And Database L ogs', metadata={'_id': {'soid': '66720a81b6cb1d87043 c013c'), 'source': 'https://query.prod.cms. rt.microsoft.com/cms/api/am/binary/RE4HKJP', 'page': 15}), 0.9336163997650146), (Document(page_content=&quot;MongoD B.\nMongoD B Atlas incorporates best practices to help keep\nmanaged databases heal thy and optimized. T hey ensure\noperational continuity by converting complex manual tasks', metadata= {'_id': {'so id: '66728a81b6cb1d87043c011f'), 'source': 'https:// query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4HKJP', 'p age': 13)), 0.9317773580551147)] In this code example, you have the same query string for which you performed a plain semantic search earlier. The k value is set to 3 so that it only returns the top three matching documents. You have also provided a pre_filter query, which is basically an MQL expression that uses the $eq operator to specify that MongoDB should only return content and chunked information that is on page 17 of the original PDF document. As you can see in the output, three documents are returned that have the highest relevance score. Each returned document also has a relevance score attached to it that ranges between 0 and 1. MongoDB allows you to pre-filter your data using a match expression to narrow down the search space before performing a more computationally intensive vector search. This offers several benefits to developers, such as increased performance, improved accuracy, and enhanced query relevancy. When pre-filtering, remember to index any metadata fields by which you want to filter during index creation. Here is a code snippet that shows how you can perform semantic search with pre-filtering: Semantic search with pre-filtering"></li><li class="carousel-slide" data-ssplayer-slide-index="126" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/127/1739580232671?e=1743033600&amp;v=beta&amp;t=qrz_MZ5gdvYZf0Vm7CwoOKEKrMA1TRMy0PaOGOSkA3E" alt="126 Useful Frameworks, Libraries, and APIs Implementing a basic RAG solution with LangChain LangChain’s functionalities are not only limited to performing semantic search queries on your data stored in vector databases. It also allows you to build powerful GenAI applications. With the following code snippet, you will learn an easy way to do the following: • Set up a MongoDB Atlas Vector Search retriever for similarity-based search. • Return the 10 most relevant documents. • Utilize a custom RAG prompt with an LLM to answer questions based on the retrieved documents: # Instantiate Atlas Vector Search as a retriever retriever = vector_store.as_retriever( ) search_type = &quot;similarity&quot;, search_kwargs = { &quot;k&quot;: 3 } llm = ChatOpenAI() def format_docs(docs): return &quot;\n\n&quot;.join(doc.page_content for doc in docs) # Prompt the chain question = &quot;How can I secure my MongoDB Atlas cluster?&quot; answer = rag_chain.invoke(question) Question: {question} &quot;&quot;&quot; custom_rag_prompt = PromptTemplate.from_template(template) # Define a prompt template template = &quot;&quot;&quot; Use the following pieces of context to answer the question at the end.If you don't know the answer, just say that you don't know, don't try to make up an answer. {context} # Construct a chain to answer questions on your data rag_chain = ( ) { &quot;context&quot;: retriever | format_docs, &quot;question&quot;: RunnablePassthrough()} | custom_rag_prompt | llm | StrOutputParser()"></li><li class="carousel-slide" data-ssplayer-slide-index="127" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/128/1739580232671?e=1743033600&amp;v=beta&amp;t=BddfD8p_R4Y5ZapQscIBx4Ude2WidTpm6qRbknIu5ss" alt="AI/ML frameworks 127 print(«Question: « + question) print(«Answer: « + answer) # Return source documents documents = retriever.get_relevant_documents(question) print(«\nSource documents:») pprint.pprint(documents) Question: How can I secure my MongoDB Atlas cluster? Answer: To secure your MongoDB Atlas cluster, you can enable authentication and IP Address whitelisting, review the security section of the MongoDB Atlas documentation, and utilize encryption of data at rest with encrypted storage volumes. Additionally, you can set up global clusters with a few clicks in the MongoDB Atlas UI, ensure operational continuity by converting complex manual tasks, and consider setting up a larger number of replica nodes for increased protection against database downtime. Source documents: The preceding code instantiates Atlas Vector Search as a retriever to query for similar documents in the vector database. In LangChain, a retriever is an interface that returns documents given an unstructured query. Retrievers accept a string query as input and return a list of documents as output. Note that you are setting the value of k as 3 to search for only the three most relevant documents. In the preceding code, notice the line that says custom_rag_prompt = PromptTemplate. from_template(template). It refers to prompt templates, which are detailed in the next section. Prompt templates in LangChain are predefined recipes for generating prompts for language models. Prompt templates may contain various elements, such as instructions, few-shot examples, and specific contexts and questions that are appropriate for a given task. In this case, you have added some instructions and are passing context as an input variable and the original query for the LLM. Let’s set up a chain, a key feature of LangChain that specifies three main components: • Retriever: You will use MongoDB Atlas Vector Search to find relevant documents that provide context for the language model • Prompt template: This is the template you created earlier to format the query and the contextual information • LLM: You will use the OpenAI chat model to generate responses based on the provided context You will use this chain to process a sample input query about MongoDB Atlas Security recommendations, format the query, retrieve the results of the query, and then return a response to the user along with the documents used as context. Due to LLM variability, you will likely never receive the exact same response twice, but here is an example showing the potential output: LangChain prompt templates and chains"></li><li class="carousel-slide" data-ssplayer-slide-index="128" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/129/1739580232671?e=1743033600&amp;v=beta&amp;t=Jm_6TRpM7wuMBpsL7zaRhCoCegOzyiznGicdPYF3TG4" alt="128 Useful Frameworks, Libraries, and APIs [Document (page_content='To ensure a secure system right out of the box, \ nauthentication and IP Address whitelisti ng are\nautomatically enabled.\ nReview the security section of the MongoD B Atlas', metadata={'_id': {'$oid': '6672 @a81b6cb1d87043c0171'), 'source': 'https://query.prod.cms.rt.microsoft.com/ cms/api/am/binary/RE4HKJP', 'page': 17}), Document(page_content='MongoD B Atlas team are also monitoring the underlying\ninfrastructure, ensuring that it is always in a healthy state. \nApplication L ogs And Database L ogs', metadata('id': ('soid': '66728a81b6cb1d87043c0 13c'), 'source': 'https://query.prod.cms. rt.microsoft.com/cms/api/am/binary/RE4HKJP', 'page': 15}), Document(page_content='All the user needs to do in order for MongoD B Atlas to\nautomatically deploy the cluster i s to select a handful of\noptions: \n Instance size\n•Storage size (optional) \n Storage speed (optional)', metadata= {&quot;_id&quot;: &quot;soid: '66728a81b6cb1d87043c012a'), 'source': 'https:// query.prod.cms.rt.microsoft.com/cms/api/am/binary/ RE4HKJP', 'page': 14)), The pandas library is a powerful and flexible open source data manipulation and analysis library for Python. It provides data structures such as DataFrames and Series, which are designed to handle structured data intuitively and efficiently. When working with tabular data stored in spreadsheets or databases, pandas is a great tool for data analysis. With pandas, you can perform a wide range of operations, including cleaning, transforming, and aggregating data. In addition to AI/ML frameworks, there are also many Python libraries that will make the experience of building your GenAI application easier. Whether you require assistance with data cleansing, formatting, or transformation, there are likely half a dozen potential Python libraries to solve every problem. The following subsections list some favorites and explain how they can assist you during your GenAI journey. For this book, you can broadly divide these libraries into three categories: • General-purpose scientific libraries such as pandas, NumPy, and scikit-learn • MongoDB-specific libraries such as PyMongoArrow • Deep learning frameworks such as PyTorch and TensorFlow The rest of this section covers one relevant and popular library from each of these categories This output both answers the user’s inquiry and provides the source information, increasing not only user trust but also the ability of the user to follow up and get more details as they require. This brief overview of the LangChain framework has tried to convince you of this framework’s utility and potential and give you a preview of its capabilities to save you valuable time when crafting your GenAI application. Key Python libraries pandas"></li><li class="carousel-slide" data-ssplayer-slide-index="129" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/130/1739580232671?e=1743033600&amp;v=beta&amp;t=sxUPGnRY3xNESA8_tdR5dtXElNsdJ26Uq2BwjXyh0YQ" alt="Figure 7.1: DataFrame output from pandas Key Python libraries 129 Your output should be in the format of a pandas DataFrame, similar to Figure 7.1: Among many other noticeably out-of-the-box functionalities, pandas also offers great support for time series and has an extensive set of tools for working with dates, times, and time-indexed data. In addition to providing a wide range of methods to work with numerical data, pandas gives great support for working with text-based data. Here is a short example of how to work with the pandas library. In the following example, you will create a pandas DataFrame from a Python dictionary. Then, you will print the entire DataFrame. Next, you will select a specific column, which is Age, and print it. Then, you will filter data by row label or by the specific position of a row. The next line shows how you can filter data using Boolean masking in pandas. Here, you will print out the DataFrame format: df = pd.DataFrame(data) # Display the DataFrame print(&quot;DataFrame:&quot;) print(df) pip3 install pandas==1.5.3 import pandas as pd # Create a DataFrame data = { } 'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'], 'Age': [24, 27, 22, 32, 29], 'City': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix']"></li><li class="carousel-slide" data-ssplayer-slide-index="130" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/131/1739580232671?e=1743033600&amp;v=beta&amp;t=8sxZto-2jLd7D-wj-8rZ7i3rRQDtH4zqvldayM2Urjk" alt="130 Useful Frameworks, Libraries, and APIs Figure 7.2: DataFrame output of ages Figure 7.3: Filtered DataFrame output You’ll get the output shown in Figure 7.2: This code will filter the data and then output the results in DataFrame format, as in Figure 7.3: You can then manipulate this data in various ways, each time outputting the results as you see fit, but always formatted as a pandas DataFrame. To print only the ages of the users, you would use the following code: You can also perform calculations with the pandas library in a straightforward way. To calculate the average age, for instance, you would use code such as this: You can also filter the output. Here, you will filter data to show only those people who are older than 25, and then present the results as a DataFrame: # Select a column print(&quot;\nAges:&quot;) print(df['Age']) # Filter data print(&quot;\nPeople older than 25:&quot;) print(df[df['Age'] > 25])"></li><li class="carousel-slide" data-ssplayer-slide-index="131" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/132/1739580232671?e=1743033600&amp;v=beta&amp;t=eE3_fVGAM3B9HNU0OlP2MemQT6ly8QzQ30_bIIISbgg" alt="Figure 7.4: Calculated field output Key Python libraries 131 # Calculate average age average_age = df['Age'].mean() print(&quot;\nAverage Age:&quot;) print(average_age) And your output would look like Figure 7.4: As you can see, data manipulation in pandas is fairly easy, and the outputs are immediately readable and well-formatted for further analysis. The intuitive syntax and powerful functions of pandas make it an essential tool for Python developers, enabling them to handle large datasets with ease and precision. For those building GenAI applications, pandas streamlines the data preprocessing steps, ensuring that data is clean, structured, and ready for model training. Additionally, its robust integration with other Python libraries enhances its utility, making complex data analysis and visualization straightforward and efficient. PyMongoArrow is a Python library built on top of the official MongoDB Python driver, PyMongo, which allows you to move data out of the MongoDB database into some of the most popular Python libraries, such as pandas, NumPy, PyArrow, and polars, and vice versa. PyMongoArrow simplifies loading data from MongoDB into other supported data formats. The example covered below demonstrates how you can work with MongoDB, PyMongoArrow, and libraries such as pandas and NumPy. You may find this useful in the context of GenAI applications in the following situations: • When you require data in a specific format for summarization and analysis (CSV, DataFrame, NumPy array, Parquet file, etc.) from MongoDB • If you need to merge data of various types for calculations or transformations that are then used for GenAI analysis As an example, if you have inbound financial data from Application A, inbound sales data from Application B, PDF files from Team 1, and .txt files from Team 2, and you’d like your GenAI application to summarize annual data from all these different places, you will likely get more accurate results if all types of data are in the same format. This will require some upfront programmatic effort, and PyMongoArrow simplifies transforming MongoDB JSON into other data types as well as ingesting those other data types and converting them into JSON. PyMongoArrow"></li><li class="carousel-slide" data-ssplayer-slide-index="132" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/133/1739580232671?e=1743033600&amp;v=beta&amp;t=R4s74P9J9c8ThGBui2_GgNsMp-d2QdtQNxeXjm9U5a8" alt="132 Useful Frameworks, Libraries, and APIs 2. 3. 4. 5. Add some test data to your collection: Now, make sure you have your Atlas cluster connection string handy: Follow these steps to complete this example with PyMongoArrow: 1. Start by installing and importing the latest version of PyMongoArrow: PyMongoArrow uses a data schema to convert query results into tabular form. If no schema is provided, it infers one from the data. You can define a schema by creating a schema object and mapping field names to type-specifiers: Next, you will extend the PyMongo driver via the pymongoarrow.monkey module. This allows you to add the PyMongoArrow functionality directly to MongoDB collections in Atlas. By calling patch_all() from pymongoarrow.monkey, new collection instances will include PyMongoArrow APIs, such as pymongoarrow.api.find_pandas_all(). This is useful because you can now easily export your data from MongoDB to various formats such as pandas. pip3 install PyMongoArrow import pymongoarrow as pa import getpass, os, pymongo, pprint from pymongoarrow.monkey import patch_all patch_all() from pymongoarrow.api import Schema schema = Schema({'_id': int, 'amount': float, 'last_updated': datetime}) from datetime import datetime from pymongo import MongoClient client = MongoClient(ATLAS_CONNECTION_STRING) client.db.data.insert_many([ {'_id': 1, 'amount': 21, 'last_updated': datetime(2020, 12, 10, 1, 3, 1), 'account': {'name': 'Customer1', 'account_number': 1}, 'txns': ['A']}, {'_id': 2, 'amount': 16, 'last_updated': datetime(2020, 7, 23, 6, 7, 11), 'account': {'name': 'Customer2', 'account_number': 2}, 'txns': ['A', 'B']}, {'_id': 3, 'amount': 3, 'last_updated': datetime(2021, 3, 10, 18, 43, 9), 'account': {'name': 'Customer3', 'account_number': 3}, 'txns': ['A', 'B', 'C']}, {'_id': 4, 'amount': 0, 'last_updated': datetime(2021, 2, 25, 3, 50, 31), 'account': {'name': 'Customer4', 'account_number': 4}, 'txns': ['A', 'B', 'C', 'D']}])"></li><li class="carousel-slide" data-ssplayer-slide-index="133" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/134/1739580232671?e=1743033600&amp;v=beta&amp;t=FSHD55HAOODP9_lpcEgS34Uh3oVjTnW-MoQhuP_ir4I" alt="Key Python libraries 133 6. The result of this code is converted into a pandas DataFrame that would consist of a total sum. The first line of code converts the query results into a pandas DataFrame. The second line of code converts the query results set into an arrow table. The third line converts the query results set into a polars DataFrame, and finally, the fourth line converts the query result set into a NumPy array. You are not limited to performing find operations to convert the query result set into other supported data formats. PyMongoArrow also allows you to use MongoDB’s powerful aggregation pipeline to perform complex queries on your data to filter out the needed data before exporting it to other data formats. For example, the following code performs an aggregation query on the data collection in a MongoDB database, grouping all documents and calculating the total sum of the amount field: MongoDB’s key feature is its ability to represent nested data using embedded documents, along with its support for lists and nested lists. PyMongoArrow fully supports these features out of the box, providing first-class functionality for handling embedded documents, lists, and nested lists seamlessly. Let’s perform some find operations on the data. The following code demonstrates querying a MongoDB collection called data for documents where the amount field is greater than 0, using PyMongoArrow to convert the results into different data formats. A predefined schema is used for the conversion, but it’s optional. If you omit the schema, PyMongoArrow tries to automatically apply a schema based on the data contained in the first batch: Now that you have learned a little bit about pandas and NumPy, it’s important you also have some knowledge of another popular Python ML library, PyTorch. PyTorch, developed by Meta’s AI Research lab, is an open source deep learning framework known for its flexibility and ease of use. It is widely appreciated for its dynamic computation graph, whichallows intuitive coding and immediate execution of code. This feature is particularly useful for researchers and developers who need to experiment and iterate quickly. df = client.db.data.find_pandas_all({'amount': {'$gt': 0}}, schema=schema) df = client.db.data.find_polars_all({'amount': {'$gt': 0}}, schema=schema) ndarrays = client.db.data.find_numpy_all({'amount': {'$gt': 0}}, schema=schema) arrow_table = client.db.data.find_arrow_all({'amount': {'$gt': 0}}, schema=schema) df = client.db.data.aggregate_pandas_all([{'$group': {'_id': None, 'total_ amount': { '$sum': '$amount' }}}]) PyTorch"></li><li class="carousel-slide" data-ssplayer-slide-index="134" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/135/1739580232671?e=1743033600&amp;v=beta&amp;t=6E-RqWD5Z0IJ5TAp3npm3DmHyXJk7bc75hWvw_Nlp5Y" alt="134 Useful Frameworks, Libraries, and APIs In the context of building a GenAI application, PyTorch serves as a powerful tool for the following: • Model training and development: PyTorch is utilized for developing and training the core generative models, such as generative pre-trained transformer (GPT) variants, which form the backbone of the GenAI application. • Flexibility and real-time experimentation: The dynamic computation graph in PyTorch allows on-the-fly modifications and real-time experimentation, which are crucial for fine-tuning generative models to produce high-quality output. Developers who are adapting pre-trained language models to their specific requirements or developing their own custom model for unique tasks may be interested in using this library, along with some of the APIs discussed in the following section. AI/ML APIs When developing GenAI applications, developers have access to a variety of APIs that can significantly enhance the capabilities and efficiency of their projects. As these APIs are widely used, they offer performance, stability, and consistency across thousands of projects, ensuring that developers don’t need to reinvent the wheel. Here are just some of the functionalities that these APIs offer:• Text generation and processing: APIs such as OpenAI, Hugging Face, and Google Gemini API enable developers to generate coherent and contextually appropriate text, which is crucial for applications such as content creation, dialogue systems, and virtual assistants. • Translation capabilities: The Google Cloud Translation API, Azure AI Translator, and Amazon Translate API provide robust translation capabilities, making GenAI applications multilingual and globally accessible. • Speech synthesis and recognition: Services such as Google Text-to-Speech, Amazon Polly, and IBM Watson Text-to-Speech convert generated text into natural-sounding speech, enhancing user interaction and accessibility. • Image and video processing: APIs from Clarifai and DeepAI allow GenAI applications to create, modify, and analyze visual content, enabling tasks such as image generation from text and object recognition. These APIs provide a range of capabilities that, when combined, can significantly accelerate the development and enhance the functionality of GenAI applications. Next, you’re going to dig deeper into two of these APIs, the OpenAI API and the Hugging Face Transformers APIs."></li><li class="carousel-slide" data-ssplayer-slide-index="135" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/136/1739580232671?e=1743033600&amp;v=beta&amp;t=zE56DXQmgf-Usf9CaRvztdnsyXRrYsxD_jfUe3jtbnc" alt="AI/ML APIs 135 OpenAI API As you may recall from Chapter 3, Large Language Models, OpenAI provides a foundational model trained on a broad spectrum of data. It offers this model via an API, which allows you to harness the power of advanced ML models without needing to manage the underlying infrastructure. The computational and financial costs of retraining or hosting a custom LLM for an organization or domain-specific information are very high, so most developers will utilize someone else’s LLM to provide GenAI capabilities to their applications. Although each API has its own strengths and weaknesses, the OpenAI API is currently the most widely used. It provides a simple interface for developers to create an intelligence layer in their applications. It is powered by OpenAI’s state-of-the-art models and cutting-edge natural language processing (NLP) capabilities, enabling applications to perform tasks such as text generation, summarization, translation, and conversation. The API is designed to be flexible and scalable, making it suitable for a wide range of use cases, from chatbots to content creation tools. It is also well documented, with a large community, and there are many tutorials available for seemingly every use case. The OpenAI API is already somewhat of an industry standard, and many GenAI tools and technologies have support and seamless integrations with it. If you’d like to avoid a lot of unnecessary effort and costs, your best bet is to work with the OpenAI API. Let’s get started with the OpenAI API in the following example: 2. 3. Include your API key from OpenAI in the environment variable file: Send your first API test request to the OpenAI API using the Python library. To do this, create a file named openai-test.py using the terminal or an IDE. Then, inside the file, copy and paste one of the following examples: 1. To get started, you’ll need to install openai from the terminal or command line: from openai import OpenAI client = OpenAI() pip3 install --upgrade openai==1.41.0 export OPENAI_API_KEY='your-api-key-here' completion = client.chat.completions.create( model=&quot;gpt-4o-mini&quot;, messages=[ {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.&quot;}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Compose a poem that explains the concept of recursion in programming.&quot;} ) print(completion.choices[0].message) ]"></li><li class="carousel-slide" data-ssplayer-slide-index="136" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/137/1739580232671?e=1743033600&amp;v=beta&amp;t=mpPdGnFTxXhk7WuI_0pgNgH-6XFs-h6MVnGzUE772Wo" alt="136 Useful Frameworks, Libraries, and APIs 4. Run the code by entering python openai-test.py into the terminal or command line. This should output a creative poem about recursion. Every result is different because the GPT will use creativity to invent something new each time. This is what it created in this attempt: Hugging Face is a prominent AI community and ML platform. Its ecosystem is the Hugging Face Hub, a platform designed to facilitate collaboration and innovation in the AI community. The Hub, located at https://huggingface.co/docs/hub/en/index, boasts a vast repository of over 120,000 models, 20,000 datasets, and 50,000 demonstrations as of writing, making it one of the largest collections of ML resources available. It has the following: • Extensive model repositories: The Hub includes pre-trained models for a variety of tasks, such as text classification, translation, summarization, and question answering, providing a wide range of options for developers. Datasets: It provides access to a diverse array of datasets that are crucial for training and evaluating ML models. Datasets cover multiple domains and languages, supporting the development of robust and versatile AI applications. • • Community and collaboration: The platform supports collaboration by allowing users to share models, datasets, and code. Developers can contribute to the community by uploading their own models and datasets, fostering a collaborative environment. The result is surprisingly good. You should try it for yourself to see what new creative poem will be crafted. GPT excels at answering questions, but only on the topics it recalls from its training data. In most cases, you’ll want GPT to answer questions about your business or products or answer commonly asked questions from your users. In such cases, you’ll want to add the ability to search a library of your own documents for relevant text, and then have GPT use that text as part of its reference information for responses. This is referred to as RAG, which you can read more about in Chapter 8, Implementing Vector Search in AI Applications. In code’s endless labyrinth, a tale is spun, Of functions nested deep, where paths rerun. A whisper in the dark, a loop within, Where journeys start anew as they begin. Behold the call, a serpent chasing tail, The dragon’s circle, a fractal’s holy grail. In depths unseen, the echoing refrain, A self-same mirror where the parts contain. A climb up winding stairs, each step the same, Yet every twist, a slight and altered game. In finite bounds, infinity unfurls, A loop of dreams within its spiral swirls. Hugging Face"></li><li class="carousel-slide" data-ssplayer-slide-index="137" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/138/1739580232671?e=1743033600&amp;v=beta&amp;t=dxoNNyGPkD3devWGim2qFKypcIJ910KxAlZvnzLaz5k" alt="Figure 7.5: Hugging Face Transformers sentiment analysis output AI/ML APIs 137 2. You’ll receive the following output: Next, import the pipeline() function. You’ll also create an instance of the function and specify the task you want to use it for, that is, sentiment analysis: The model performs the analysis and outputs a label and a score. The label indicates the sentiment type as positive or negative, and the score indicates the degree of confidence in the output. You’ll use the transformers library to utilize shared models, then explore the pipeline() function, the core component of the transformers library. This function seamlessly integrates the model with necessary pre-processing and post-processing steps, enabling direct text input and generating intelligible responses: 1. First, ensure you have the necessary packages installed. Note that at least one of TensorFlow or PyTorch should be installed. Here, let’s use TensorFlow: • Integration and deployment options: The Hugging Face Hub integrates seamlessly with popular ML frameworks, such as PyTorch and TensorFlow. The Hub also provides deployment solutions, enabling developers to deploy their models in production environments easily. GenAI application developers can use the Hugging Face Transformers APIs to get access to thousands of pre-trained ML models on specific datasets for specific tasks. With transformer models, you can use pre-trained models for inference or fine-tune them with your own data using PyTorch and TensorFlow libraries. To illustrate what is possible for your GenAI application, let’s see how to use a pre-trained transformer model for inference in order to perform two tasks: basic sentiment analysis and text generation. Both could be useful for your GenAI projects if you, for instance, want to sort customer feedback or score it based on sentiment and generate a response. Sentiment analysis Internally, the pipeline downloads and caches a default pre-trained model and tokenizer for performing sentiment analysis on the input text: pipeline() pip3 install transformers tensorflow from transformers import pipeline analyse_sentiment = pipeline(«sentiment-analysis») analyse_sentiment(&quot;The weather is very nice today.&quot;)"></li><li class="carousel-slide" data-ssplayer-slide-index="138" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/139/1739580232671?e=1743033600&amp;v=beta&amp;t=uBRnkJd48tk6bgpBuCTjaQXUm183RNRc_9oxEkK9S2A" alt="138 Useful Frameworks, Libraries, and APIs Figure 7.7: Text generation using the Hugging Face Transformers Figure 7.6: Multiple input texts for sentiment classification in Hugging Face You’ll receive the following as the output: You’ll get the following output for the preceding code: You can also pass multiple input texts as an array for sentiment classification to the model: analyse_sentiment([&quot;The weather is very nice today.&quot;, &quot;I don't like it when it rains in winter.&quot;]) In this case, the model outputs an array of objects. Each output object corresponds to the individual text inputs. You might be holding your breath, expecting things to become more complicated—but they won’t. You conducted your first sentiment analysis in Hugging Face with a pre-trained model with just those few lines of code. Text generation In addition to sentiment analysis, you can also perform many other NLP tasks with Transformers libraries, such as text generation. Here, you will provide a prompt, and the model will auto-complete it by generating the remaining text: generator = pipeline(&quot;text-generation&quot;) generator(&quot;I love AI, it has&quot;)"></li><li class="carousel-slide" data-ssplayer-slide-index="139" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/140/1739580232671?e=1743033600&amp;v=beta&amp;t=5cRAQzvSvcGijZetAK9dao6BiDr5chTMJqONScRoY-c" alt="Figure 7.8: Hugging Face text generation output with parameters AI/ML APIs 139 With these additional parameters provided, you’ll now receive the following output: Since you did not provide a model name to the pipeline instance, it decided to use the default, which in this case is GPT-2. You may or may not get the same results as the ones here because text generation involves some randomness. Again, however, you can see how easy this task was. Next, specify a model name to be used in the pipeline function at the time of text generation. With the following code, you provide some more custom details, such as the number of differentsequences to be generated and the maximum length of the output texts: The preceding code outputs two different pairs of text, each having fewer than 25 words. As you might expect, Hugging Face offers many more tools and functionalities that developers can use to build their GenAI applications. With its comprehensive library support and active community, Hugging Face continues to be a pivotal resource for advancing NLP and ML projects. Additionally, its seamless integration with various AI/ML frameworks ensures that developers can efficiently deploy and scale their GenAI models with minimal effort and maximum productivity. generator = pipeline(&quot;text-generation&quot;, model=&quot;distilgpt2&quot;) generator( ) &quot;I love AI, it has&quot;, max_length=25, num_return_sequences=2,"></li><li class="carousel-slide" data-ssplayer-slide-index="140" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/141/1739580232671?e=1743033600&amp;v=beta&amp;t=tXeliEWbMZUwcI2lrDiJxHcITqOw-v2EFLBOLvFiC80" alt="140 Useful Frameworks, Libraries, and APIs Summary In this chapter, you looked at the evolution of AI/ML frameworks in the Python space as LLM- powered applications have gained prominence. You also learned why Python remains a top choice for building modern LLM-powered applications. You reviewed the most popular Python frameworks, libraries, and APIs that can assist you in the different stages of GenAI application development. The GenAI space is evolving so rapidly that by the time this book is published, there will probably be more libraries available, more APIs in use, and the framework’s capabilities will have expanded. You owe it to yourself to do your own due diligence about which framework is best suited for your business needs, but also make sure to choose one that is appropriately supported. As with any rapidly evolving technology, some of the tools and technologies that are in existence today will be gone tomorrow. This chapter has tried, therefore, to only include those that have the community, enablement, and feature set to ensure their longevity. Undoubtedly there is still plenty of innovation to be done, and new tools to be created, even in the short term—the tools discussed in this chapter are barely the tip of the iceberg. So, take a deep breath and begin your own discovery. You will inevitably realize that there are tools you need, and that you have too many choices on how to fulfill those needs. In the next chapter, you will explore how to leverage the vector search feature of MongoDB Atlas to create intelligent applications. You will learn about RAG architecture systems and gain a deeper understanding of various complex RAG architecture patterns with MongoDB Atlas."></li><li class="carousel-slide" data-ssplayer-slide-index="141" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/142/1739580232671?e=1743033600&amp;v=beta&amp;t=Q3v8apBYkOD6ggbC71MTt18-2buK2BAdw0_BB8oiujA" alt="8 Implementing Vector Search in AI Applications Vector search is revolutionizing the way people interact with data in AI applications. MongoDB Atlas Vector Search allows developers to implement sophisticated search capabilities that understand the nuances of discovery and retrieval. It works by converting text, video, image, or audio files into numerical vector representations, which can then be stored and searched efficiently. MongoDB Atlas can perform similarity searches alongside your operational data, making it an essential tool for enhancing user experience in applications ranging from e-commerce to content discovery. With MongoDB Atlas, setting up vector search is streamlined, enabling developers to focus on creating dynamic, responsive, and intelligent applications. In this chapter, you will learn how to use the Vector Search feature of MongoDB Atlas to build intelligent applications. You will learn how to build retrieval-augmented generation (RAG) architecture systems and delve deeper into the understanding and development of various patterns of complex RAG architectures with MongoDB Atlas, unraveling the synergies that underpin their joint value and potential. Through real-world use cases and practical demonstrations, you will learn how this dynamic duo can seamlessly transform businesses across industries, driving efficiency, accuracy, and operational excellence."></li><li class="carousel-slide" data-ssplayer-slide-index="142" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/143/1739580232671?e=1743033600&amp;v=beta&amp;t=SJYr_iP9_Pcx7tcLizd3cttQCzltxU_RvJNGHYeDje0" alt="142 Implementing Vector Search in AI Applications 3. You will also need to know how to set up and run JupyterLab or Jupyter Notebook. You will be using the following packages to develop the demo described in this chapter: &nbsp; pandas: Helps with data preprocessing and handling &nbsp; numpy: Handles numerical data &nbsp; openai: For the embedding model and invoking the LLM &nbsp; pymongo: For the MongoDB Atlas vector store and full-text search &nbsp; s3fs: Allows loading data directly from an S3 bucket &nbsp; langchain_mongodb: Enables vector search in MongoDB Atlas using a LangChain wrapper &nbsp; langchain: Used to build a RAG application &nbsp; langchain-openai: Enables you to interact with OpenAI chat models &nbsp; boto3: Enables you to interact with AWS s3 buckets &nbsp; python-dotenv: Enables you to load environment variables from a .env file To install the mentioned packages in your Python virtual environment, run the following command: This chapter assumes that you have at least beginner-level expertise in Python coding. To follow along with the demos, you’ll need to set up your development environment by completing the following steps: 1. 2. Install either python@3.9 or python@3.11 on the operating system of your choice. Set up a Python virtual environment and activate it: This chapter covers the following topics: • Leverage vector search and full-text search with MongoDB Atlas, which will later help you build a robust retriever for RAG • Understand the various components involved in the development of a RAG system • Learn about the process and steps involved in the development of simple RAG and advanced RAG systems. Technical requirements $ python3 -m venv venv $ source venv/bin/activate pip3 install langchain==0.2.14 langchain-community==0.2.12 langchain- core==0.2.33 langchain-mongodb==0.1.8 langchain-openai==0.1.22 langchain-text-splitters==0.2.2 numpy==1.26.4 openai==1.41.1 s3fs==2024.6.1 pymongo==4.8.0 pandas==2.2.2 boto3==1.35.2 python- dotenv==1.0.1"></li><li class="carousel-slide" data-ssplayer-slide-index="143" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/144/1739580232671?e=1743033600&amp;v=beta&amp;t=CcWVPiS7oTG1l-uOezy1WK_cM0sZNgt_d6lZHlzSRK4" alt="Information retrieval with MongoDB Atlas Vector&nbsp;Search 143 Information retrieval with MongoDB Atlas Vector&nbsp;Search Information retrieval is a critical component of RAG systems. It enhances the accuracy and relevance of the generated text by sourcing information from extensive knowledge bases. This process allows the RAG system to produce responses that are not only precise but also deeply rooted in factual content, making it a powerful tool for various natural language processing (NLP) tasks. By effectively combining retrieval with generation, RAG addresses challenges related to bias and misinformation, contributing to the advancement of AI-related applications and tasks. In the context of information retrieval, it’s essential to distinguish between relevance and similarity. While similarity focuses on word matching, relevance is about the interconnectedness of ideas. While a vector database query can help identify semantically related content, more advanced tools are needed to accurately retrieve relevant information. In Chapter 5, Vector Databases, you learned about MongoDB Atlas Vector Search and how it enhances the retrieval of relevant information by allowing the creation and indexing of vector embeddings,which can be generated using machine learning models, such as embedding models. This facilitates semantic search capabilities, enabling the identification of content that is contextually similar rather than just being keyword based. Full-text search complements this by providing robust text search capabilities that can handle typos, synonyms, and other variations in text, ensuring that searches return the most pertinent results. Together, these tools provide a comprehensive search solution that can discern and retrieve information based on both the similarity of terms and the relevance of the content. With the help of an example, let’s see how to load a small dataset in MongoDB to perform a vector search along with full-text search to perform information retrieval. For this demonstration, you will load a sample movie dataset from an S3 bucket: 1. Write a simple Python function to accept search terms or phrases and pass it through the embeddings API again to get a query vector. Take the resultant query vector embeddings and perform a vector search query using the $vectorsearch operator in the MongoDB aggregation pipeline. Pre-filter the documents using meta information to narrow the search across your dataset, thereby speeding up the performance of the vector search results while retaining accuracy. Further, post-filter the retrieved documents that are semantically similar (based on relevancy score), if you want to demonstrate a higher degree of control over the semantic search behavior. 2. 3. 4. Vector search tutorial in Python"></li><li class="carousel-slide" data-ssplayer-slide-index="144" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/145/1739580232671?e=1743033600&amp;v=beta&amp;t=P1K_sCoySVl7_NApSMqJ9NmCyIZW4NOElHsHHOI5vxI" alt="144 Implementing Vector Search in AI Applications Figure 8.1: Sample movies data view 5. 6. Initialize the OpenAI API key and MongoDB connection string: Now, load the dataset from the S3 bucket. Run the following lines of code in Jupyter Notebook to read data from an AWS S3 bucket directly to a pandas DataFrame: On executing the preceding snippet of code, you should see the following result in your Jupyter Notebook cell. import pandas as pd import s3fs df = pd.read_json(&quot;https://ashwin- partner-bucket.s3.eu-west-1. amazonaws.com/movies_sample_dataset.jsonl&quot;, orient=&quot;records&quot;, lines=True) df.to_json(&quot;./movies_sample_dataset.jsonl&quot;, orient=&quot;records&quot;, lines=True) df[:3] import os import getpass # set openai api key try: openai_api_key = os.environ[&quot;OPENAI_API_KEY&quot;] except KeyError: openai_api_key = getpass.getpass(&quot;Please enter your OPENAI API KEY (hit enter): &quot;) # Set MongoDB Atlas connection string try: MONGO_CONN_STR = os.environ[&quot;MONGODB_CONNECTION_STR&quot;] except KeyError: MONGO_CONN = getpass.getpass(&quot;Please enter your MongoDB Atlas Connection String (hit enter): &quot;)"></li><li class="carousel-slide" data-ssplayer-slide-index="145" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/146/1739580232671?e=1743033600&amp;v=beta&amp;t=qGj7QCXLGkP0ktkPaHS9Rd_3B2G9DdhU7naci-jtRGI" alt="Figure 8.2: Sample movies dataset view with OpenAI embeddings Information retrieval with MongoDB Atlas Vector&nbsp;Search 145 7. 8. 9. Next, initialize MongoDB Atlas and insert data into a MongoDB collection. You should see that the sample_movies dataset is enriched with the OpenAI embeddings in the embedding field, as shown in Figure 8.2. Initialize and run an embedding job to embed the sample_movies dataset. In the following code example, you create a final field, which is a field derived from the text and overview fields that are already available in the dataset. Next, run this final field against the embedding API from OpenAI, as shown here: import numpy as np from tqdm import tqdm import openai df['final'] = df['text'] + &quot; df['final'][:5] step = int(np.ceil(df['final'].shape[0]/100)) embeddings_t = [] lines = [] # Note that we must split the dataset into smaller batches to not exceed the rate limits imposed by OpenAI API's. for x, y in list(map(lambda x: (x, x+step), list(range(0, df.shape[0], step)))): Overview: &quot; + df['overview'] lines += [df.final.values[x:y].tolist()] for i in tqdm(lines): out = [] for ele in embeddings_t: embeddings_t += openai.embeddings.create( model='text-embedding-ada-002', input=i).data out += [ele.embedding] df['embedding'] = out df[:5]"></li><li class="carousel-slide" data-ssplayer-slide-index="146" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/147/1739580232671?e=1743033600&amp;v=beta&amp;t=7wDBIl9tUVWeNNA5aQ-Hu3CN0mYxjL5Ns2mpB_zbN5U" alt="146 Implementing Vector Search in AI Applications 10. Now that you have created the vector embeddings for your sample_movies dataset, you can initialize the MongoDB client and insert the documents into your collection of choice by running the following lines of code: You have ingested the test data to build a vector search capability. Now, let’s proceed to build a vector search index in the following steps. 11. Let’s first create vector index definitions. You can create a vector search index in the MongoDB Atlas Vector Search UI by following the steps explained in Chapter 5, Vector Databases. The vector index required for this demo tutorial is provided here: Once the vector index definitions are added under the Vector Search index JSON editor in the MongoDB Atlas UI, the process for creating a vector search index is triggered and the vector search index is created at the specified path field mentioned in the vector index definition. Now, you are ready to perform vector search queries on the sample_movies.embed_movies collection in MongoDB Atlas where all the data is stored, and create vector indexes. Let’s equip the vector search or the retriever API to use in your RAG framework. 12. You can query a MongoDB vector index using $vectorSearch. MongoDB Atlas brings the flexibility of using vector search alongside search filters. Additionally, you can apply range, string, and numeric filters using the aggregation pipeline. This allows the end user to control the behavior of the semantic search response from the search engine. { &quot;fields&quot;: [ } { &quot;type&quot;: &quot;vector&quot;, &quot;numDimensions&quot;: 1536, &quot;path&quot;: &quot;embedding&quot;, &quot;similarity&quot;: &quot;cosine&quot; }, { &quot;type&quot;: &quot;filter&quot;, &quot;path&quot;: &quot;year&quot; }, ] from pymongo import MongoClient import osmongo_client = MongoClient(os.environ[&quot;MONGODB_CONNECTION_ STR&quot;]) # Upload documents along with vector embeddings to MongoDB Atlas Collection output_collection = mongo_client[&quot;sample_movies&quot;][&quot;embed_movies&quot;] if output_collection.count_documents({})>0: output_collection.delete_many({}) _ = output_collection.insert_many(df.to_dict(&quot;records&quot;))"></li><li class="carousel-slide" data-ssplayer-slide-index="147" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/148/1739580232671?e=1743033600&amp;v=beta&amp;t=6bL0Tgs4Q4fUNHo25O3tjrDeGTrItJ3MPeHXzjJ4hpw" alt="Information retrieval with MongoDB Atlas Vector&nbsp;Search 147 The following code example demonstrates how you can perform vector search along with pre-filtering on the year field to get movies released post 1990. To have better control over the relevance of returned results, you can perform post-filtering on the response using the MongoDB Query API. The following code demonstrates how you can perform these steps: I. Represent a raw text query as a vector embedding. There are multiple embedding models currently available with OpenAI, such as text-embedding-3-small, text-embedding- 3-large with variable dimensions, and the text-embedding-ada-002 model. II. Build and perform a vector search query to MongoDB Atlas. III. Perform pre-filtering before performing a vector search on the year field. IV. Perform post-filtering using the score field to better control the relevancy and accuracy of the returned results. Run the following code to initialize a function that can help you achieve vector search, pre-filter, and post-filter: def query_vector_search(q, prefilter = {}, postfilter = {},path=&quot;embedding&quot;,topK=2): ele = openai.embeddings.create(model='text-embedding-ada-002', input=q).data query_embedding = ele[0].embedding vs_query = { &quot;index&quot;: &quot;default&quot;, &quot;path&quot;: path, &quot;queryVector&quot;: query_embedding, &quot;numCandidates&quot;: 10, &quot;limit&quot;: topK, } if len(prefilter)>0: vs_query[&quot;filter&quot;] = prefilter new_search_query = {&quot;$vectorSearch&quot;: vs_query} project = {&quot;$project&quot;: {&quot;score&quot;: {&quot;$meta&quot;: &quot;vectorSearchScore&quot;},&quot;_ id&quot;: 0,&quot;title&quot;: 1, &quot;release_date&quot;: 1, &quot;overview&quot;: 1,&quot;year&quot;: 1}} if len(postfilter.keys())>0: postFilter = {&quot;$match&quot;:postfilter} res = list(output_collection.aggregate([new_search_query, project, postFilter])) else: res = list(output_collection.aggregate([new_search_query, project])) return res"></li><li class="carousel-slide" data-ssplayer-slide-index="148" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/149/1739580232671?e=1743033600&amp;v=beta&amp;t=5-V8x3a4OvHPmQe_k0SlDlXbsPCl_sr8xPCk_2LY2dc" alt="148 Implementing Vector Search in AI Applications Figure 8.3: Sample result from running the vector search query with pre-filters Figure 8.4: Sample result from running the vector search query with a pre-filter and post-filter Here’s a sample query with This is a sample query with the relevant results: You should get the following result: You should get the following result: as a pre-filter: as a pre-filter and a -based post-filter to retain only With this Python method, you were able to filter on the score field and the year field to generate results as well as results for vector similarity. Using a heuristic, you were able to control the accuracy of the results to retain only the most relevant documents and were also able to apply a range filter query (on the year field). year year score query_vector_search(&quot;I like Christmas movies, any recommendations for movies release after 1990?&quot;, prefilter={&quot;year&quot;:{&quot;$gt&quot;: 1990}}, postfilter= {&quot;score&quot;: {&quot;$gt&quot;:0.905}},topK=5) query_vector_search(&quot;I like Christmas movies, any recommendations for movies release after 1990?&quot;, prefilter={&quot;year&quot;: {&quot;$gt&quot;: 1990}}, topK=5)"></li><li class="carousel-slide" data-ssplayer-slide-index="149" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/150/1739580232671?e=1743033600&amp;v=beta&amp;t=PG5p-YvRdIJ3np10dwMqPZ-1t7pBVIW2NFkajdUXAE8" alt="Figure 8.5: Sample result vector search query using the LangChain module for MongoDB Information retrieval with MongoDB Atlas Vector&nbsp;Search 149 Vector Search tutorial with LangChain Utilizing LangChain with MongoDB Atlas Vector Search for building a semantic similarity retriever offers several advantages. The following example demonstrates how to carry out a vector similarity search using LangChain wrapper classes: Here’s the result: This demonstrates a more sophisticated yet simple approach that is particularly beneficial for developers creating RAG applications. The LangChain framework offers a suite of APIs and wrapper classes that can be used to integrate with various serverless LLM providers, such as OpenAI, and talk to MongoDB Atlas Vector Search to build RAG frameworks with very few lines of code. It is also easy to maintain and scale. from langchain_mongodb.vectorstores import MongoDBAtlasVectorSearch from langchain_openai import OpenAIEmbeddings import json embedding_model = OpenAIEmbeddings(model=&quot;text-embedding-ada-002&quot;) vector_search = MongoDBAtlasVectorSearch(output_collection, embedding_ model, text_key='final') fquery = {&quot;year&quot;: {&quot;$gt&quot;: 1990}} search_kwargs = { } retriever = vector_search.as_retriever(search_kwargs=search_kwargs) docs = retriever.invoke(&quot;I like Christmas movies, any recommendations for movies release after 1990?&quot;) for doc in docs: foo = {} foo['title'] = doc.metadata['title'] foo['year'] = doc.metadata['year'] foo['final'] = doc.metadata['text'] print(json.dumps(foo,indent=1)) &quot;k&quot;: 5, 'filter': fquery,"></li><li class="carousel-slide" data-ssplayer-slide-index="150" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/151/1739580232671?e=1743033600&amp;v=beta&amp;t=_iOyCEAiJdASI2-SRcDADpv7JfHqfYh58mqE4SDqXNc" alt="150 Implementing Vector Search in AI Applications Figure 8.6: Building blocks of RAG architecture In this section, you were able to build and perform vector similarity search using MongoDB Atlas. You developed reusable wrapper classes and functions that will be useful in developing a more sophisticated application, such as a chatbot. Now, let’s delve deep into understanding what RAG architectures are and how to develop one using the resources that you’ve created so far. In the dynamic landscape of modern business, the relentless pursuit of efficiency and accuracy urges organizations to adopt cutting-edge technologies. Among these, automation stands as a cornerstone, particularly in processing and automating workflows. However, traditional methods suffer when they’re subjected to large volumes of data with intricate tasks, and human-led processes often fall short due to error-prone manual interventions. This section explores the transformative landscape of automation, discussing the pivotal role RAG plays in revolutionizing business operations. MongoDB, known for its prowess in data management and flexible schemas, offers a compelling synergy with RAG through its vector search and full-text search capabilities. Delving into the architectural details of RAG, this section dissects its constituent building blocks, offering practical insights into constructing automated document-processing workflows that harness the full potential of LLMs and MongoDB Vector Search. Building RAG architecture systems Load documents Query Raw documents Vectorize query Partition documents into chunks The LLM uses the user query + retrieved document chunks + instruction to generate an answer LLM Answer Document chunks EEmmbbeeddddiinnggs Vectorize chunks Generated answer Embeddings Document chunks Retrieved document chunks from MongoDB are augmented into query prompt to the LLM Use the query embedding vector to &nbsp;re a vector similarity search query to retrieve relevant document chunks Store metadata and embedding in MongoDB MongoDB Atlas Index Store Vector Search App Services/ Triggers Document Store"></li><li class="carousel-slide" data-ssplayer-slide-index="151" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/152/1739580232671?e=1743033600&amp;v=beta&amp;t=R4JS-z7CONOdCgfyBnmPRu2K6Qb50kCmaUDVySdDbQA" alt="Building RAG architecture systems 151 Let’s go over the key components of the RAG architecture in detail: 1. Document loading: Initially, documents are loaded from data storage. This involves text extraction, parsing, formatting, and cleaning to prepare the data for document splitting. Document splitting: The next step is to break down the documents into smaller, manageable segments or chunks. Strategies for splitting can vary, from fixed-size chunking to content-aware chunking that considers the content structure. Text embedding: These document chunks are then transformed into vector representations (embeddings) using techniques such as OpenAIEmbeddings, Sentence e-BERT, and Instructor Embeddings. This step is crucial for understanding the semantic content of the chunks. Vector store: The generated vectors, each associated with unique document chunks, are stored in a vector store alongside the document chunks and other metadata extracted from theMongoDB Atlas collection. Atlas Vector Search indexes and Apache Lucene search can be built through the MongoDB Atlas UI for easy and fast retrieval. 2. 3. 4. 5. Query processing: When a user submits a query, it is also converted into a vector representation using the same embedding technique as mentioned in Step 3. Document retrieval: The retriever component locates and fetches document chunks that are semantically like the query. This retrieval process employs vector similarity search techniques and MongoDB Atlas using the Hierarchical Navigable Small Worlds (HNSW) algorithm to perform a fast nearest neighbor search to retrieve relevant documents without compromising the accuracy of the retrieved search results. 6. 7. Document chunk post-filtering: The relevant document chunks are retrieved from the MongoDB collection with the help of the Unified Query API and can be post-filtered easily to transform the output document chunks into the required format. 8. LLM prompt creation: The retrieved document chunks and the query are combined to create a context and prompt for the LLM. 9. Answer generation: Finally, the LLM generates a response based on the prompt, completing the RAG process. In the context of RAG systems, there are two primary types: simple (or naive) RAG and advanced RAG. In practical scenarios, this classification helps address different types of personas and questions the applications are handling, and it’s common to encounter both simple and complex RAG queries within the same workflow and from the same persona. As a developer, it is important to reason out the functionalities that the application is expected to serve before deciding on the building blocks involved in the RAG architecture."></li><li class="carousel-slide" data-ssplayer-slide-index="152" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/153/1739580232671?e=1743033600&amp;v=beta&amp;t=38OUOgokbfKMIR9FDjDLoNy9geyjhvTwdt5wmnKfePk" alt="152 Implementing Vector Search in AI Applications Chunking or document splitting is a critical step in handling extensive texts within RAG systems. When dealing with large documents, the token limits imposed by language models (such as gpt-3.5- turbo) necessitate breaking them into manageable chunks. However, a naive fixed-chunk-size approach can lead to fragmented sentences across chunks, affecting subsequent tasks such as QA. To address this, consider semantics when dividing documents. Most segmentation algorithms use chunk size and overlap principles. Chunk size (measured by characters, words, or tokens)determines segment length, while overlaps ensure continuity by sharing context between adjacent chunks. This approach preserves semantic context and enhances RAG system performance. Now, let’s delve into the intricacies of document-splitting techniques, particularly focusing on content- aware chunking. While fixed-size chunking with overlap is straightforward and computationally efficient, more sophisticated methods enhance the quality of text segmentation. The following are the various document-splitting techniques: When building your RAG architecture system, consider the following points to help with programming and planning: • Workflow specificity: Define the specific workflow you intend to automate with RAG; it may be related to question answering (QA), data augmentation, summarization, reasoning, or assertion. Maybe your customers frequently ask a specific set of three or four types of queries. User experience: Collaborate with your target user group to understand the types of queries they are likely to ask to identify the user group journey, which might be a simple single-state response or a multi-state chat flow. • • Data sources: First, identify the nature of your data source—whether it’s unstructured or structured. Next, map the locations of these data sources. Once you’ve done that, classify the data based on whether it serves operational or analytical purposes. Finally, observe the data patterns to determine whether answers are readily available in one location or if you’ll need to gather information from multiple sources. These pointers will help you determine whether you need to go for a simple RAG system or an advanced RAG system and also help you to determine the essential building blocks to consider while constructing your RAG architecture. Now, let’s delve deeper into the building blocks of this architecture with some code examples to better explain the nuances. However, before you develop RAG-powered applications, let’s look at the fundamentals of how to process source documents to maximize the accuracy of the rated responses from the RAG application. The following strategies will come in handy while processing documents before storing them in a MongoDB Atlas collection. Chunking or document-splitting strategies"></li><li class="carousel-slide" data-ssplayer-slide-index="153" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/154/1739580232671?e=1743033600&amp;v=beta&amp;t=aGB7zQZd1tgiTWJW8ZQo_KDgVitofZODkzrK3wUm88o" alt="Building RAG architecture systems 153 • Recursive chunking: This technique includes the following approaches: &nbsp; Hierarchical approach: Recursive chunking breaks down input text into smaller chunks iteratively. It operates hierarchically, using different separators or criteria at each level. &nbsp; Customizable structure: By adjusting the criteria, you can achieve the desired chunk size or structure. Recursive chunking adapts well to varying document lengths. • Sentence splitting: Sentence splitting involves various strategies, such as the ones listed here: &nbsp; Naive splitting: This method relies on basic punctuation marks (such as periods and new lines) to divide text into sentences. While simple, it might not handle complex sentence structures well. &nbsp; spaCy: Another robust NLP library, spaCy, offers accurate sentence segmentation. It uses statistical models and linguistic rules. &nbsp; Natural Language Toolkit (NLTK): NLTK, a powerful Python library for NLP, provides efficient sentence tokenization. It considers context and punctuation patterns. &nbsp; Advanced tools: Some tools employ smaller models to predict sentence boundaries, ensuring precise divisions. • Specialized techniques: Specialized techniques include the following: &nbsp; Structured content: For documents with specific formats (e.g., Markdown, LaTeX), specialized techniques come into play. &nbsp; Intelligent division: These methods analyze the content’s structure and hierarchy. They create semantically coherent chunks by understanding headings, lists, and other formatting cues. In summary, while fixed-size chunking serves as a baseline, content-aware techniques consider semantics, context, and formatting intricacies. Choosing the right method depends on your data’s unique characteristics and the requirements of your RAG system. While choosing the retriever for storing and retrieving these chunks, you may want to consider solutions such as document hierarchies and knowledge graphs. MongoDB Atlas has a flexible schema and a simple unified query API to query data from it. Now let’s use the recursive document-splitting strategy to build a simple RAG application."></li><li class="carousel-slide" data-ssplayer-slide-index="154" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/155/1739580232671?e=1743033600&amp;v=beta&amp;t=i-YG1C0CUJu-_bJ7JB8If_VlkpwjUMWPA1FWKPw2Xno" alt="154 Implementing Vector Search in AI Applications Figure 8.7: Naive RAG Simple RAG A simple RAG architecture implements a naive approach where the model retrieves a predetermined number of documents from the knowledge base based on their similarity to the user’s query. These retrieved documents are then combined with the query and input into the language model for generation, as shown in Figure 8.7. To build a simple RAG application, you will use the dataset you loaded to the MongoDB Atlas collection in the Information retrieval with MongoDB Vector Search section of this chapter. With this application, you’ll be able perform queries on the available movies and create a recommender system. LLM This example will use the OpenAI APIs and gpt-3.5-turbo, but there are other variations of LLM models made available from OpenAI, such as gpt-4o and gpt-4o-mini. The same prompting technique can be used with other LLMs, such as claude-v2 or mistral8x-7B, to achieve similar results. The following is the sample code to invoke the OpenAI LLM using LangChain: from openai import OpenAI client = OpenAI() def invoke_llm(prompt, model_name='gpt-3.5-turbo-0125'): &quot;&quot;&quot; Queries with input prompt to OpenAI API using the chat completion api gets the model's response. &quot;&quot;&quot; response = client.chat.completions.create( model=model_name, messages=[ { «role»: «user», «content»: prompt } ], temperature=0.2, max_tokens=256, Query Embedding MongoDB Atlas Vector Search Context LLM Answer"></li><li class="carousel-slide" data-ssplayer-slide-index="155" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/156/1739580232671?e=1743033600&amp;v=beta&amp;t=TByhevikwKZwDsJpWfmcaOXxqIphaBU--88e_fuq7Hg" alt="Building RAG architecture systems 155 top_p=1, frequency_penalty=0, presence_penalty=0 ) invoke_llm(&quot;This is a test&quot;) 'Great! What do you need help with?' Movie Recommendations: 1. Movie_name: Movie_overview &quot;&quot;&quot; return prompt chatbot_response = response.choices[0].message.content.strip() return chatbot_response def get_prompt(question, context): prompt = f&quot;&quot;&quot;Question: {question} System: Let's think step by step. Context: {context} &quot;&quot;&quot; return prompt def get_recommendation_prompt(query, context): prompt = f&quot;&quot;&quot; From the given movie listing data, choose a few great movie recommendations. User query: {query} Context: {context} Here is the result: Now that you have the APIs to call MongoDB Atlas Vector Search for retrieval and an API for invoking an LLM, you can combine these two tools to create a RAG system. Prompt A prompt to an LLM is a user-provided instruction or input that guides the model’s response. It can be a question, a statement, or a command, and is designed to drive the LLM to respond with a specific output. The effectiveness of a prompt can greatly influence the quality of the results generated by a RAG-based system, making prompt engineering a crucial aspect for interacting with these models. Good prompts are clear, specific, and structured to communicate the user’s intent to the LLM, enabling it to generate the most accurate and helpful responses possible. The following is an example of a prompt to perform QA on a private knowledge base:"></li><li class="carousel-slide" data-ssplayer-slide-index="156" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/157/1739580232671?e=1743033600&amp;v=beta&amp;t=Jzxxh3sjWk6Jux4mZzn_jEBYu_IxUyt3OLwP8j0wl5w" alt="156 Implementing Vector Search in AI Applications This is the result: Here is the result: Similarly, you can use the recommendations using a simple RAG framework: method to generate some movie To demonstrate the benefits of RAG over a foundational LLM, let's first ask the LLM a question without vector search context and then with it included. This will demonstrate how you can improve the accuracy of the results and reduce hallucinations while utilizing a foundational LLM, such as gpt- 3.5-turbo, that was not trained on a private knowledge base. Here is the query response without vector search: Although the LLM’s response shows it struggles with factual accuracy, there is still promise in using it alongside human oversight for enterprise applications. Together, these systems can work effectively to power applications for businesses. To help overcome this issue, you need to add context to the prompt through vector search results. Let's see how you can use the invoke_llm function with the query_vector_search method to provide the relevant context alongside the user query to generate a response with a factually correct answer: get_recommendation_prompt The movie in which a footballer goes completely blind is &quot;23 Blast.&quot; idea = &quot;In which movie does a footballer go completely blind?&quot; search_response = query_vector_search(idea, prefilter={&quot;year&quot;:{&quot;$gt&quot;: 1990}}, postfilter={&quot;score&quot;: {&quot;$gt&quot;:0.8}},topK=10) premise = &quot;\n&quot;.join(list(map(lambda x:x['final'], search_response))) print(invoke_llm(get_prompt(idea, premise))) print(invoke_llm(&quot;In which movie does a footballer go completely blind?&quot;)) The Game of Their Lives&quot; (2005), where the character Davey Herold, a footballer, goes completely blind after being hit in the head during a game question = &quot;I like Christmas movies, any recommendations for movies release after 1990?&quot; search_response = query_vector_search(question,topK=10) context = &quot;\n&quot;.join(list(map(lambda x:x['final'], search_response))) print(invoke_llm(get_recommendation_prompt(&quot;I like Christmas movies, any recommendations for movies release after 1990?&quot;, context)))"></li><li class="carousel-slide" data-ssplayer-slide-index="157" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/158/1739580232671?e=1743033600&amp;v=beta&amp;t=RwcmNmdpCkgyb1ucg1dLNAgruKBAk__8yeggZEUZW9M" alt="Figure 8.8: Sample output from the simple RAG application Building RAG architecture systems 157 Here is the result: The simple RAG system you just built can handle straightforward queries that need answers to the point. Some examples are a customer service chatbot responding to a basic question such as “ is the customer support center in Bangalore?” or helping you find all the restaurants where your favorite delicacy is served in Koramangala. The chatbot can retrieve the contextual piece of information in its retrieval step and generate an answer to this question with the help of the LLM. An advanced RAG framework incorporates more complex retrieval techniques, better integration of retrieved information, and often, the ability to iteratively refine both the retrieval and generation processes. In this section, you will learn how to build an intelligent recommendation engine on fashion data that can identify the interest of the user and then generate relevant fashion product or accessory recommendations only when there is intent to purchase a product in the user’s utterance. You will be building an intelligent conversation chatbot that leverages the power of LangChain, MongoDB Atlas Vector Search, and OpenAI in this section. The advanced RAG system in the current example will demonstrate the following features: • Utilize an LLM to generate multiple searchable fashion queries given a user’s chat utterance • Classify the user’s chat utterance as to whether there is an intent to purchase • Develop a fusion stage that will also fetch vector similarity search results from multiple search queries to fuse them as a single recommendation set that is reranked with the help of an LLM Where Advanced RAG"></li><li class="carousel-slide" data-ssplayer-slide-index="158" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/159/1739580232671?e=1743033600&amp;v=beta&amp;t=DbDdm081g2GuYJsPrsntjBUjvabZAg5D_dJWr1nDgCU" alt="158 Implementing Vector Search in AI Applications Figure 8.9: Sample advanced RAG, flowchart for query processing and recommendation The flow of steps when a user queries the RAG system is depicted in Figure 8.9: Let’s walk through the code to load the sample dataset and build the advanced RAG system with all the features that were listed at the beginning of this section. Loading the dataset For this example, you will utilize fashion data from a popular e-commerce company. The following code shows you how to load a dataset from an S3 bucket to a pandas DataFrame and then insert these documents into a MongoDB Atlas collection, search.catalog_final_myn: import pandas as pd import s3fs import os import boto3 s3_uri= &quot;https://ashwin-partner-bucket.s3.eu-west-1.amazonaws.com/fashion_ dataset.jsonl&quot; df = pd.read_json(s3_uri, orient=&quot;records&quot;, lines=True) print(df[:3]) Query LLM Status MongoDB Atlas Vector Search Yes Embedding Batch search terms False LLM All recommendations LLM Answer Ranked recommendations"></li><li class="carousel-slide" data-ssplayer-slide-index="159" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/160/1739580232671?e=1743033600&amp;v=beta&amp;t=jgA6mdvmHvqJRmf34fUwcka24STCd6iXFgEHjQerOto" alt="Figure 8.10: Sample view of the fashion dataset with OpenAI embeddings Building RAG architecture systems 159 { &quot;fields&quot;: [ } { &quot;type&quot;: &quot;vector&quot;, &quot;numDimensions&quot;: 1536, &quot;path&quot;: &quot;openAIVec&quot;, &quot;similarity&quot;: &quot;cosine&quot; } ] from langchain_core.output_parsers import JsonOutputParser # type: ignore from langchain_core.prompts import PromptTemplate # type: ignore from langchain_core.pydantic_v1 import BaseModel, Field # type: ignore from langchain_openai import ChatOpenAI # type: ignore from pymongo import MongoClient mongo_client = MongoClient(os.environ[&quot;MONGODB_CONNECTION_STR&quot;]) # Upload documents along with vector embeddings to MongoDB Atlas Collection col = mongo_client[&quot;search&quot;][&quot;catalog_final_myn&quot;] col.insert_many(df.to_dict(orient=&quot;records&quot;)) Here is the result: As you can see in Figure 8.10, the vector embeddings are already provided as part of the dataset. Therefore, the next step is to create a vector search index. You can create the vector search index by following the steps detailed in Chapter 5, Vector Databases, using the following index mapping: You have successfully loaded the new fashion dataset into the MongoDB Atlas collection and also created a vector search index with all the building blocks in place. You can now use the following code to set up an advanced RAG system and build a recommender system with the features mentioned earlier: Creating a vector search index Fashion recommendations using advanced RAG"></li><li class="carousel-slide" data-ssplayer-slide-index="160" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/161/1739580232671?e=1743033600&amp;v=beta&amp;t=NcAucwUjRAf0xMuNTAReCAbBhAA4nEvU4HDCmucwUZ0" alt="160 Implementing Vector Search in AI Applications load_dotenv() from functools import lru_cache from pymongo import MongoClient # type: ignore from typing import List from itertools import chain import certifi # type: ignore import os from dotenv import load_dotenv # type: ignore @lru_cache def get_vector_store(): &quot;&quot;&quot; Retrieves the vector store for MongoDB Atlas. Returns: MongoDBAtlasVectorSearch: The vector store object. &quot;&quot;&quot; @lru_cache def get_openai_emb_transformers(): &quot;&quot;&quot; Returns an instance of OpenAIEmbeddings for OpenAI transformer models. class. Returns: embeddings (OpenAIEmbeddings): An instance of the OpenAIEmbeddings &quot;&quot;&quot; embeddings = OpenAIEmbeddings() return embeddings from langchain_community.embeddings import OpenAIEmbeddings # type: ignore from langchain_mongodb.vectorstores import MongoDBAtlasVectorSearch # type: ignore This function creates and returns an instance of the OpenAIEmbeddings class, which provides access to OpenAI transformer models for natural language processing. The instance is cached using the lru_cache decorator for efficient reuse."></li><li class="carousel-slide" data-ssplayer-slide-index="161" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/162/1739580232671?e=1743033600&amp;v=beta&amp;t=i-VmsOIDodVWW9f8fm-t7efy3EJajXl3ZLEb-whuXg0" alt="Building RAG architecture systems 161 class Product(BaseModel): &quot;&quot;&quot; Represents a product. # Define your desired data structure. class ProductRecoStatus(BaseModel): &quot;&quot;&quot; Represents the status of product recommendations. @lru_cache(10) def get_conversation_chain_conv(): &quot;&quot;&quot; Retrieves a conversation chain model for chat conversations. vs = MongoDBAtlasVectorSearch(collection=col, embedding=get_openai_ emb_transformers(), index_name=&quot;vector_index_openAi_cosine&quot;, embedding_ key=&quot;openAIVec&quot;, text_key=&quot;title&quot;) return vs Returns: ChatOpenAI: The conversation chain model for chat conversations. &quot;&quot;&quot; llm = ChatOpenAI(model=&quot;gpt-3.5-turbo&quot;, temperature=0.2, max_ tokens=2048) # chain = ConversationChain(llm=llm, memory=ConversationBufferWindowMemory(k=5)) return llm Attributes: relevancy_status (bool): Product recommendation status conditioned on the context of the input query. True if the query is related to purchasing fashion clothing and/or accessories. False otherwise. recommendations (List[str]): List of recommended product titles based on the input query context and if the relevancy_status is True. &quot;&quot;&quot; relevancy_status: bool = Field(description=&quot;Product recommendation status is conditioned on the fact if the context of input query is to purchase a fashion clothing and or fashion accessories.&quot;) recommendations: List[str] = Field(description=&quot;list of recommended product titles based on the input query context and if recommendation_ status is true.&quot;)"></li><li class="carousel-slide" data-ssplayer-slide-index="162" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/163/1739580232671?e=1743033600&amp;v=beta&amp;t=QUSS5v5AWZvzc4z2p70e4THLLWaZuAPf9Y1IvVY7e8I" alt="162 Implementing Vector Search in AI Applications reco_status_parser = JsonOutputParser(pydantic_object=ProductRecoStatus) reco_status_prompt = PromptTemplate( template=&quot;You are AI assistant tasked at identifying if there is a product purchase intent in the query and providing suitable fashion recommendations.\n{format_instructions}\n{query}\n\ #Chat History Summary: {chat_history}\n\nBased on the context of the query, please provide the relevancy status and list of recommended products.&quot;, input_variables=[&quot;query&quot;, &quot;chat_history&quot;], partial_variables={&quot;format_instructions&quot;: reco_status_parser.get_ format_instructions()}, ) class Recommendations(BaseModel): &quot;&quot;&quot; Represents a set of recommendations for products and a message to the user. Attributes: title (str): Title of the product. baseColour (List[str]): List of base colours of the product. gender (List[str]): List of genders the product is targeted for. articleType (str): Type of the article. mfg_brand_name (str): Manufacturer or brand name of the product. &quot;&quot;&quot; title: str = Field(description=&quot;Title of the product.&quot;) baseColour: List[str] = Field(description=&quot;List of base colours of the product.&quot;) gender: List[str] = Field(description=&quot;List of genders the product is targeted for.&quot;) articleType: str = Field(description=&quot;Type of the article.&quot;) mfg_brand_name: str = Field(description=&quot;Manufacturer or brand name of the product.&quot;) summary. Attributes: products (List[Product]): List of recommended products. message (str): Message to the user and context of the chat history &quot;&quot;&quot; products: List[Product] = Field(description=&quot;List of recommended products.&quot;) message: str = Field(description=&quot;Message to the user and context of the chat history summary.&quot;)"></li><li class="carousel-slide" data-ssplayer-slide-index="163" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/164/1739580232671?e=1743033600&amp;v=beta&amp;t=ErPPcFzww73AYVe3muby01Pm6B5stI0PJi2mBW7Y010" alt="Building RAG architecture systems 163 def get_sorted_results(product_recommendations): all_titles = [rec['title'] for rec in product_ recommendations['products']] results = list(col.find({&quot;title&quot;: {&quot;$in&quot;:all_titles}}, {&quot;_id&quot;: 0 , &quot;id&quot;:1, &quot;title&quot;: 1, &quot;price&quot;: 1, &quot;baseColour&quot;: 1, &quot;articleType&quot;: 1, &quot;gender&quot;: 1, &quot;link&quot; : 1, &quot;mfg_brand_name&quot;: 1})) sorted_results = [] for title in all_titles: for result in results: if result['title'] == title: sorted_results.append(result) break return sorted_results Returns: The response containing the recommendation status. &quot;&quot;&quot; llm = get_conversation_chain_conv() chain = reco_status_prompt | llm | reco_status_parser resp = chain.invoke({&quot;query&quot;: query, &quot;chat_history&quot;: chat_history}) return resp reco_parser = JsonOutputParser(pydantic_object=Recommendations) reco_prompt = PromptTemplate( input_variables=[&quot;question&quot;, &quot;recommendations&quot;, &quot;chat_history&quot;], partial_variables={&quot;format_instructions&quot;: reco_parser.get_format_ instructions()}, template=&quot;\n User query:{question} \n Chat Summary: {chat_history} \n Rank and suggest me suitable products for creating grouped product recommendations given all product recommendations below feature atleast one product for each articleType \n {recommendations} \n show output in {format_instructions} for top 10 products&quot; ) def get_product_reco_status(query: str, chat_history: List[str] = []): &quot;&quot;&quot; Retrieves the recommendation status for a product based on the given query and chat history. status. Args: query (str): The query to be used for retrieving the recommendation chat_history (List[str]): The chat history containing previous conversations."></li><li class="carousel-slide" data-ssplayer-slide-index="164" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/165/1739580232671?e=1743033600&amp;v=beta&amp;t=TIIbHbo7KidcCyNKD24qpWRa5zBky55wiEbisCMBw9I" alt="164 Implementing Vector Search in AI Applications Returns: dict: The response containing the recommendations. Args: query (str): The query string for the recommendation. chat_history (List[str]): The list of previous chat messages. filter_query (dict): The filter query to apply during the recommendation retrieval. reco_queries (List[str]): The list of recommendation queries. def get_product_recommendations(query: str, reco_queries: List[str], chat_ history: List[str]=[]): &quot;&quot;&quot; Retrieves product recommendations based on the given query and chat history. &quot;&quot;&quot; vectorstore = get_vector_store() retr = vectorstore.as_retriever(search_kwargs={&quot;k&quot;: 10}) all_recommendations = list(chain(*retr.batch(reco_queries))) llm = get_conversation_chain_conv() llm_chain = reco_prompt | llm | reco_parser resp = llm_chain.invoke({&quot;question&quot;: query, &quot;chat_history&quot;: chat_ history, &quot;recommendations&quot;: [v.page_content for v in all_recommendations]}) resp = get_sorted_results(resp) return resp The preceding code carries out the following tasks: 1. Importing the necessary modules and functions from various libraries. These include JsonOutputParser for parsing JSON output, PromptTemplate for creating prompts, BaseModel and Field for defining data models, and MongoDBAtlasVectorSearch for interacting with a MongoDB Atlas vector store. It also imports MongoClient for connecting to MongoDB, load_dotenv for loading environment variables, and lru_cache for caching function results. It defines three functions, each decorated with lru_cache to cache their results for efficiency. get_openai_emb_transformers returns an instance of OpenAIEmbeddings, which provides access to OpenAI transformer models for NLP. get_vector_store retrieves the vector store for MongoDB Atlas. get_conversation_chain_conv retrieves a conversation chain model for chat conversations. 2."></li><li class="carousel-slide" data-ssplayer-slide-index="165" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/166/1739580232671?e=1743033600&amp;v=beta&amp;t=6UPjtCIs9vlFEfb-csrj7bSscQYHzJuiOsSVXy0Nevg" alt="Building RAG architecture systems 165 This is the status output: You can see from the preceding example output that the LLM is able to classify the product intent purchase as positive and recommend suitable queries by performing vector similarity search on the MongoDB Atlas collection. 3. It defines three classes using Pydantic’s BaseModel and Field. These classes represent the status of product recommendations (ProductRecoStatus), a product (Product), and a set of recommendations for products and a message to the user (Recommendations). Creating instances of JsonOutputParser and PromptTemplate for parsing JSON output and creating prompts, respectively. These instances are used to create conversation chains in the next section. It defines two functions for retrieving the recommendation status for a product and retrieving product recommendations based on a given query and chat history. get_product_reco_status uses a conversation chain to determine the recommendation status for a product based on a given query and chat history. get_product_recommendations retrieves product recommendations based on a given query and chat history, a filter query, and a list of recommendation queries. It uses a vector store retriever to get relevant documents for each recommendation query, and then uses a conversation chain to generate the final recommendations. 4. 5. Let’s now use these methods to create a product recommendations example. Enter the following code and then examine its output: {'relevancy_status': True, 'recommendations': ['Floral Print Wrap Dress', 'Off-Shoulder Ruffle Dress', 'Lace Fit and Flare Dress', 'Midi Slip Dress', 'Denim Shirt Dress']} query = &quot;Can you suggest me some casual dresses for date occasion with my boyfriend&quot; status = get_product_reco_status(query) print(status) print(get_product_recommendations(query, reco_ queries=status[&quot;recommendations&quot;], chat_history=[])"></li><li class="carousel-slide" data-ssplayer-slide-index="166" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/167/1739580232671?e=1743033600&amp;v=beta&amp;t=i2wsgfXdwbdHSNqZDi2hX2Y0fz3P4UA5Pce0R5opbR4" alt="166 Implementing Vector Search in AI Applications Figure 8.12: Sample output from the advanced RAG system when there is no purchase intent in the query Figure 8.11: Sample output from the advanced RAG chatbot with recommendations for the user's search intent Here is the status output: Here is the output from the LLM: This is the product recommendations output: Conversely, you can test the same methods to find a suitable place for a date instead of ideas for gifts or what to wear. In this case, the model will classify the query as having negative product purchase intent and not provide any search term suggestions: {'relevancy_status': False, 'recommendations': []} query = &quot;Where should I take my boy friend for date&quot; status = get_product_reco_status(query) print(status) print(get_conversation_chain_conv().invoke(query).content)"></li><li class="carousel-slide" data-ssplayer-slide-index="167" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/168/1739580232671?e=1743033600&amp;v=beta&amp;t=iQ4MDWX8W7t15_C0FJRyz7Xft4EmckRXO69VM59OvIQ" alt="Summary 167 Advanced RAG introduces the concept of modularity when building RAG architecture systems. The above example focuses on developing a user flow-based approach for the sample advanced RAG system. It also explores how to leverage LLMs for conditional decision making, recommendation generation, and re-ranking the recommendations retrieved from the retriever system. The goal is to enhance the user experience during interactions with the application. Summary In this chapter, you explored the pivotal role of vector search in enhancing AI-powered systems. The key takeaway is that vector search plays a vital role in AI applications, addressing the challenge of efficient search as unstructured and multimodal datasets expand. It benefits image recognition, NLP, and recommendation systems. MongoDB Atlas is used to demonstrate vector search implementation using its flexible schema and vector indexing capabilities. You were able to build a RAG framework for solving QA use cases that combines retrieval and generation models, with a simple RAG system utilizing pre-trained language models and embedding models from OpenAI. You also learned how to build an advanced RAGsystem that employs iterative refinement and sophisticated retrieval algorithms with the help of LLMs for building a recommendation system for the fashion industry. With these insights, you can now build efficient AI applications for any domain or industry. In the next chapter, you will delve into the critical aspects of evaluating LLM outputs in such RAG applications and explore various evaluation methods, metrics, and user feedback. You will also learn about the implementation of guardrails to ensure responsible AI deployment and how to better control the behavior of LLM-generated responses."></li><li class="carousel-slide" data-ssplayer-slide-index="168" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/169/1739580232671?e=1743033600&amp;v=beta&amp;t=RGT8OzDZUAPNbhqtysUr5KOGmuvfBq_PXf3fxUcc50A" alt="169"></li><li class="carousel-slide" data-ssplayer-slide-index="169" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/170/1739580232671?e=1743033600&amp;v=beta&amp;t=Wt6lQUGTognstNw-IG7Gb9xmkvZOPV_R33xAYKQC8OU" alt="Part 3 Optimizing AI Applications: Scaling, Fine-Tuning, Troubleshooting, Monitoring, and Analytics This set of chapters shares techniques and practices for evaluating your AI application as well as strategies and expert insights for improving your application, avoiding pitfalls, and ensuring that your application continues to function optimally despite rapid technological changes. This part of the book includes the following chapters: • Chapter 9, LLM Output Evaluation • Chapter 10, Refining the Semantic Data Model to Improve Accuracy • Chapter 11, Common Failures of Generative AI • Chapter 12, Correcting and Optimizing Your Generative AI Application"></li><li class="carousel-slide" data-ssplayer-slide-index="170" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/171/1739580232671?e=1743033600&amp;v=beta&amp;t=3EQC54o7DKtmol70p0D6d3jvy052AIJmf8XKA25jakY" alt="171"></li><li class="carousel-slide" data-ssplayer-slide-index="171" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/172/1739580232671?e=1743033600&amp;v=beta&amp;t=5iuWGn_JhUdZ7e42P2P7KJBHn7Px10LyYfV1JXZ0wr8" alt="9 LLM Output Evaluation Regardless of the form factor of your intelligent application, you must evaluate your use of large language models (LLMs). The evaluation of a computational system determines the system’s performance, gauges its reliability, and analyzes its security and privacy. AI systems are non-deterministic. You cannot be certain what an AI system will output until you run an input through it. This means that you must evaluate how the AI system performs on a variety of inputs to have confidence that it performs in line with your requirements. To be able to change the AI system without introducing any unexpected regressions, you also need to have robust evaluations. Evaluations can help catch these regressions before releasing the AI system to customers. In LLM-powered intelligent applications, evaluations measure the effect of components such as the model chosen and any hyperparameters used with the model, such as temperature, prompting, and retrieval- augmented generation (RAG) pipelines. Since the age of LLMs is still new as of writing in mid-2024, there is still an ongoing debate about when and how to best evaluate these LLM-powered intelligent applications. However, there are emerging best practices that you can use to direct your evaluations. In this chapter, you will learn about how and why you should evaluate the use of LLMs in your intelligent application. You will be able to use the concepts and metrics discussed to evaluate current classes of intelligent applications, such as chatbots, and emerging ones, such as AI agents. The concepts learned here will be applicable for years to come, regardless of the form factors of future generations of intelligent applications."></li><li class="carousel-slide" data-ssplayer-slide-index="172" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/173/1739580232671?e=1743033600&amp;v=beta&amp;t=Tg8Ao-q7Te5x0-NXMwSOZh1DgjmYrcS4258u7AL_QlM" alt="172 LLM Output Evaluation This chapter will cover the following topics: • Understanding LLM evaluation • Model benchmarking • Evaluation datasets • Key metrics for LLM evaluation • The role of human review in LLM evaluation • Using evaluations as guardrails for your application You will need the following technical requirements to run the code in this chapter: • A programming environment with Python 3.x installed. • An OpenAI API key. To create an API key, refer to the OpenAI documentation at LLM evaluation, or LLM evals, is the systematic process of assessing LLMs and the intelligent applications that use them. This involves profiling their performance on specific tasks, reliability under certain conditions, effectiveness in particular use cases, and other criteria to understand a model’s overall capabilities. You want to make sure that your intelligent application meets certain standards as measured by your evaluations. You also should be able to measure how the AI system’s performance evolves as you change components of the application or data used in the application. For example, if you want to change the LLM used in your application or a prompt, you should be able to measure the impact of these changes with evaluations. Being able to measure the impact of changes is particularly important as the quality of an application improves. Once an intelligent application is “pretty good,” it can be quite challenging for human reviewers to assess whether and how a system has improved or regressed based on a change. For instance, if you have a travel assistant chatbot that successfully meets users’ expectations 90% of the time, it can be challenging and time-intensive for human reviewers to assess the impact of a small change that would raise the success rate to 90.5%. When designing an evaluation suite for your LLM-powered intelligent application, you should consider the following aspects: • Security: The AI system should not reveal any private or confidential information that it has access to. This can include both information in the LLM’s weights and information retrieved by the application. Technical requirements What is LLM evaluation? https:// platform.openai.com/docs/quickstart/step-2-set-up-your-api-key."></li><li class="carousel-slide" data-ssplayer-slide-index="173" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/174/1739580232671?e=1743033600&amp;v=beta&amp;t=5Y0Squ_ENlljSl8QOemczbdN3pL5SU0lGiIca-drwuE" alt="What is LLM evaluation? 173 • Reputation: The AI system should not generate output that could harm your business. For example, you would not want your chatbot to recommend your competitor’s services over your own under any circumstances. • Correctness: The AI system should respond with correct output that does not include mistakes or hallucinations. • Style: The AI system should respond according to the tone and style guidelines you specify. For example, if you are developing a legal chatbot, you may want the chatbot to maintain a formal tone and use appropriate legal terminology. • Consistency: The AI system should generate output that is consistent with expectations. Given the same input, you should expect the system to perform in a predetermined manner. The response can differ, but any difference should be consistent. For example, if you are building a system that creates playlists based on a song, you would probably want it to generate similar playlists given an input song, even if there are different songs or different song orders on the output playlist. • Ethics: The AI system should respond in line with a set of ethical principles. By defining expected behavior in an evaluation dataset, you can also help define what the ethical standards of the system should be. For example, an AI system should never generate biased or discriminatory content, and it should handle sensitive topics with care and respect. In the next section, you will learn which points in your application you should evaluate. You will also review an example intelligent application that is used throughout this chapter in code examples to demonstrate the concepts. You must consider where in your application you want to perform the evaluations. Generally, you should evaluate all LLM components of a system and the end-to-end system. To illustrate this idea about where to think about evaluations in your intelligent application, this chapter uses the example of a travel assistant chatbot. The chatbot uses RAG to make travelrecommendations and answers questions based on a dataset of documents of popular tourist destinations and activities. Since this chapter is about evaluation, it will not go into detail about how the components of the application are built. Later on in the chapter, you will look at implementations of how you can evaluate this application’s LLM usage. The travel assistant chatbot has the following components: • Retriever: Finds the relevant documents to help inform answers in response to user messages. The retriever uses vector search to find the relevant documents. It also uses LLMs for the following: &nbsp; Metadata extractor: Extract any place name from the user query. This can be used to pre-filter the search results to include documents only about the relevant place. &nbsp; Query pre-processor: Convert user messages into better search queries. Component and end-to-end evaluations"></li><li class="carousel-slide" data-ssplayer-slide-index="174" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/175/1739580232671?e=1743033600&amp;v=beta&amp;t=TU236kMSfV0ouQcOV7kMlfECR4BZk1EqKIiMAYlr_VY" alt="174 LLM Output Evaluation Figure 9.1: Components of the travel assistant example chatbot &nbsp; Retrieved documents post-processor: Mutate retrieved documents to create a list of relevant facts. • Relevancy guardrail: LLM call that makes sure that the user is only talking to the chatbot about travel-related topics. If the relevancy guardrail determines that the user message is irrelevant, the chatbot does not answer the user’s irrelevant question and prompts the user to ask something more relevant. • Responder: Uses an LLM to respond to the user message based on the retrieved content. Figure 9.1 illustrates how these components work together. Extract metadata Pre-process query Post-process documents Retriever Retrieve documents Is relevant Use context for response generation Relevancy guardrail User Send message Not relevant – ask for clari&nbsp;cation Generate response Respond to user LLM-generated response Responder LLM LLM LLM LLM LLM"></li><li class="carousel-slide" data-ssplayer-slide-index="175" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/176/1739580232671?e=1743033600&amp;v=beta&amp;t=zhYw-FlXF7gzvGxBHAoWMRJH3IcwIrWww2W_95uYkCI" alt="What is LLM evaluation? 175 End-to-end evaluation End-to-end evaluation examines the performance of the entire integrated system. These evaluations capture aspects such as real-world applicability, user experience, and system reliability. They help identify potential bottlenecks or weaknesses in the overall architecture that may not be apparent when evaluating the LLM alone. For RAG systems, this involves evaluating not only the language model’s output but also the efficiency and accuracy of the retrieval mechanism, the relevance of retrieved information, and how well the system combines external knowledge with the LLM’s inherent capabilities. In the case of the travel assistant chatbot, an end-to-end evaluation would examine how the chatbot responds to user input. This evaluation considers all the intermediate LLM components and retrieval. You can evaluate qualitative aspects of the system, such as how relevant the answer is to the user question and whether there are any hallucinations. In a later section, Evaluation metrics, you will learn more about ways to evaluate end-to-end systems. Before you learn how to apply these evaluation metrics to your LLM-powered intelligent application, you will learn how to assess which LLMs are most suitable for your application with model benchmarks in the next section. Component evaluation Every subsystem of your intelligent application that calls an LLM can be considered a component. You should evaluate all components, as each component contributes to the system’s overall performance. By evaluating each component, you can ensure that every part meets the required quality standards and performs reliably. This also lets you change components with more confidence since you can have clarity on how the changes are affecting all parts of the system. One component can also contain subcomponents. You should evaluate the parent component and the child components with separate evaluations. For example, in the travel assistant chatbot, you should evaluate all individual components that use an LLM, such as the query pre-processor and response generator. You should also evaluate the retriever, considering its three LLM subcomponents as a single component. By evaluating all logical LLM components, you can get a better understanding of the entire system’s behavior. This understanding lets you make changes to individual components while knowing the effect that those changes will have on other related components."></li><li class="carousel-slide" data-ssplayer-slide-index="176" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/177/1739580232671?e=1743033600&amp;v=beta&amp;t=Lc61cCiV0ljVyqTWr5pgnh3wf5P_po4eMq6G3xyeTjs" alt="176 LLM Output Evaluation Model benchmarking The LLM itself is a fundamental component of any intelligent application. Given that there are many LLMs that may be suitable for your application, it is helpful to compare them to each other to see which will best serve your application. To compare multiple models, you can assess them all against a standard set of evaluations. This process of comparing models across a uniform set of evaluations is called model benchmarking. Benchmarking can help you understand the model’s capabilities and limitations. Often, the LLMs that perform best on benchmarks are the largest models, such as GPT-4 and Claude 3 Opus. However, these larger models also tend to be more expensive to run and slow to generate, compared to smaller models, such as GPT-4o mini and Claude 3 Haiku. Even if the larger models are prohibitively expensive, it can still be helpful to use them when developing your application since they set a baseline of ideal system performance. You can design your evaluations around your system using these models, substitute the smaller models, and then work on optimizing the system to try to meet the standard of the system using the larger model. When new LLMs are released, they are typically evaluated against a standard set of benchmarks. These standard benchmarks help developers understand how the models compare. Here are a few popular LLM benchmarks that many models are evaluated against: • Massive Multi-Task Language Understanding (MMLU): This benchmark measures a model’s knowledge acquisition using college-level multiple-choice questions. It evaluates whether the model selects the correct answer. You can learn more about this benchmark at https://paperswithcode.com/sota/ multi-task-language-understanding-on-mmlu. • HellaSwag: This benchmark measures a model’s common-sense reasoning ability using multiple- choice text completion. It evaluates whether the model selects the correct sentence completion. You can learn more about this benchmark at https://paperswithcode.com/sota/ sentence-completion-on-hellaswag. • HumanEval: This benchmark measures a model’s programming ability in Python. It prompts a model to create a Python function to solve a task. It then evaluates whether the function that the model outputs is correct using preconstructed unit tests. You can learn more about this benchmark at https://paperswithcode.com/sota/code- generation-on-humaneval. • MATH: This benchmark measures a model’s ability to solve math word problems. It evaluates whether the model reaches the correct solution. You can learn more about this benchmark at https://paperswithcode.com/dataset/math."></li><li class="carousel-slide" data-ssplayer-slide-index="177" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/178/1739580232671?e=1743033600&amp;v=beta&amp;t=2RlmQRmmzNkQDjudiFb-721EX1Q2kiHqysnoADwQV30" alt="Model benchmarking 177 You must create evaluation datasets to measure AI system performance. An evaluation dataset is the data that you input into an AI system to produce an output that measures how well the AI system performs. Evaluation datasets often include some criteria that an evaluation metric can use to determine the score of the evaluation. An evaluation metric takes the input and the output of an AI system and returns a score measuring how the AI system performed for the case. You will learn more about evaluation metrics in the Evaluation metrics section of this chapter. An evaluation dataset is a set of distinct evaluation cases. Each evaluation case typically includes the following information: • Input: The data inputted into the AI system. • Reference: Criteria that the evaluation metric uses to evaluate whether the AI system output is correct. The reference is often an ideal output for the system given the input. This ideal output is often called the golden answer or reference answer. This could also be a rubric of criteria that the AI system output should meet. Sometimes, evaluation datasets do not include references because the evaluation metric used on the dataset doesn’t need reference criteria to evaluate the input. When an evaluation does not require an output reference, it is called a reference-free evaluation. You can assess the performance of LLMs based on these benchmarks to choose models that are most suitable for your application. For example, in the case of the travel assistant chatbot, a high score on MMLU is probably a good indication that the model is well suited for answering travel questions, as it would be helpful for the model to have world knowledge to inform its answers. In contrast, high scores on the HumanEval Python coding benchmark would likely have little bearing on the quality of its travel recommendations. You can also create your own benchmarks to assess the LLM’s performance on a domain relevant to your application. You can even style these benchmarks after existing benchmarks. For the travel assistant chatbot, you could make a benchmark of multiple-choice questions about popular travel destinations styled after MMLU. This travel benchmark would help determine which models possessthe best background information about travel. By choosing a model with more travel-related knowledge, you could improve the quality of your responses. These benchmarks can also reveal which models are best suited for different components of your application. For instance, for the travel assistant chatbot, perhaps you need to use a large, expensive model that possesses significant knowledge of vacation destinations in the main responder, but can use a faster, cheaper model in other LLM components, such as the input relevance guardrail. Once you have an idea of which models are appropriate for your AI components, you can start building those systems. To understand and measure how well these AI systems use the LLMs, you mustcreate evaluation datasets and run evaluation metrics over them. In the next two sections, you will learn about creating these evaluation datasets and metrics.Evaluation datasets"></li><li class="carousel-slide" data-ssplayer-slide-index="178" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/179/1739580232671?e=1743033600&amp;v=beta&amp;t=IEmKuEMi68kJa9f5LJPyIrFOEmW8Uwl80PggFUUV6Vk" alt="178 LLM Output Evaluation Table 9.1: Evaluation dataset for the example chatbot • Metadata: An evaluation usually also includes metadata with each evaluation case. This can be a unique name, an ID, or a tag. Evaluation datasets tend to conform to tabular or document-based data structures. Therefore, they are often stored in formats such as CSV, JSON, or Parquet. Here is a small example evaluation dataset of user messages and model answers for the travel assistant chatbot: Input What should I do in New York City in July? Can you help me with my math homework? Golden answer Check out Times Square, go to an outdoor concert, and visit the Statue of Liberty. I’m sorry, I cannot help you with your math homework since I am a travel assistant. Do you have any travel-related questions? Paris is the capital of France. Tags [&quot;todo&quot;, &quot;nyc&quot;, &quot;usa&quot;] [&quot;security&quot;] The remainder of this chapter uses this dataset in its evaluations. What exactly you include in an evaluation dataset depends on what functionality you want to evaluate and the evaluation metrics you are using. In the upcoming Evaluation metrics section, you will learn more about what exact information you need to include in your evaluation datasets for different evaluation metrics. Regardless of what exact evaluation metrics you use, it is important to have a representative evaluation dataset. The dataset should be representative of the types of inputs that you expect your AI systemto receive in addition to edge cases that you want to optimize the system around. There is no precise number of evaluation cases that you should have or formula for determining what that number should be for a given scenario. Nevertheless, you can use the following very rough heuristics for building evaluation datasets:• Always have at least 10 evaluation cases for a given metric • Have at least 100-200 representative evaluation cases to get an idea of end-to-end system performance Next, you will learn about a few strategies to help you create representative evaluation datasets. What's the capital of France? [&quot;europe&quot;, &quot;france&quot;]"></li><li class="carousel-slide" data-ssplayer-slide-index="179" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/180/1739580232671?e=1743033600&amp;v=beta&amp;t=gMrc5hXkK7s3JBc5Yc4OU-wW8fjvjbRCQLgLg3XwqeA" alt="Model benchmarking 179 User feedback After you release your AI system, you can source evaluation cases from user data to continuously refine and improve the system’s performance. If your application has any user feedback mechanisms, such as ratings or comments, you can use these to identify cases where the system succeeds or fails. Generally, you should manually review any application data before adding it to an evaluation dataset. You want to ensure that the case is suitable for your evaluation dataset and does not contain any sensitive information. You can also add metadata, such as tags or an evaluation case name. Even if the application data is not suitable for an evaluation case, perhaps because it is improperly formatted or contains personally identifiable information, you can modify it to create a suitable evaluation case. Defining a baseline To bootstrap your evaluation dataset, you must create a set of evaluation cases that cover the general expected behaviors and edge cases around which you want to optimize for in your application. To define the common expectations of this baseline, it can be useful to collaborate with any stakeholders of the AI system to create evaluation cases for the following areas: • A diverse sample of expected common inputs: You may be able to leverage existing data to help inform these evaluation cases. For example, in the travel assistant chatbot, you could derive evaluation cases from top Google search queries about travel. This follows the logic that whatever people are searching for on Google, they are likely to ask your chatbot about as well. • Edge cases around which you want to optimize your system: Edge cases can include inputs that test the security and ethical guardrails of the system. If you red team your AI system, as discussed further in Chapter 12, Correcting and Optimizing Your Generative AI Application, you can likely find some good edge cases from the red teaming results. This baseline of evaluation cases is often enough to release the AI system to a user-facing environment. Once the AI system is in use, you can validate the efficacy of your baseline evaluation cases and create additional evaluation cases, as discussed in the next section."></li><li class="carousel-slide" data-ssplayer-slide-index="180" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/181/1739580232671?e=1743033600&amp;v=beta&amp;t=kv692xhtamfSKEdgN7yTC4Bg5GPoFR6m2ScmPRen8oE" alt="180 LLM Output Evaluation LLMs are capable tools for generating evaluation datasets. When you use an LLM to generate data, it is called synthetic data. You might want to use synthetic data because it is quite time consuming and tedious for humans to create evaluation cases. LLMs can help make the process of creating evaluation data faster and easier. There are various strategies to create synthetic evaluation data. As of writing in mid-2024, there is no structured set of best practices for creating synthetic evaluation data. However, the following are some principles that you can keep in mind when creating synthetic evaluation cases: • Have a human in the loop. A human should review all synthetic data cases and edit or remove them as needed. This provides quality control on the synthetic data. • LLMs are very effective at creating perturbations on existing evaluation cases. Perturbations are slight variations on existing data, such as the rephrasing of a sentence. You can use perturbations to see whether the AI system performs differently based on slight changes. Ideally, a system should behave consistently across perturbations. • Often, an LLM-based chatbot, such as ChatGPT, Claude, or Gemini, can be sufficient to help create synthetic data. The back-and-forth of the chatbot interface can also help you refine and iterate on your synthetic data creation. Using synthetic data in combination with a baseline and data from user feedback, you can create datasets to effectively evaluate the performance of your AI systems. You must pair these datasets with metrics to run evaluations. In the following section, you will learn more about evaluation metrics. It is possible to create a pipeline that uses LLMs to fully automate the process of creating evaluation cases from user feedback. However, you should strongly consider maintaining a human in the loop for the following reasons: • You want the quality of the evaluation dataset to be very high, which you can more easily ensure with human reviewers than an LLM-based system. • It is beneficial for the people involved in the AI system development to be aware of the cases in their evaluation dataset. This awareness helps give them context into the system capabilities. Given that evaluation datasets typically do not need to be particularly large to be effective (a few hundred evaluation cases is often sufficient), creating an LLM-based system to create evaluation cases may be excessive for the requirements of the task. • Building your evaluation dataset from user feedback is an effective way to ground your evaluations in the types of inputs that users are providing. Synthetic data"></li><li class="carousel-slide" data-ssplayer-slide-index="181" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/182/1739580232671?e=1743033600&amp;v=beta&amp;t=ffgxwMRulr4sFRJAfmO9oCUKadqSgU2Hpkooutcbxss" alt="Evaluation metrics 181 Evaluation metrics To perform evaluations on your AI system, you must combine your evaluation data with an evaluation metric. An evaluation metric takes the input and the output of an AI system and returns a score measuring how the AI system performed for the case. Evaluation metrics typically return scores between 0 and 1. The metric is called a binary metric if it returns only the scores of 0 or 1. The metric is called a normalized metric if it returns a score between 0 and 1, inclusive. Binary metrics clearly determine if the case passes or fails, 0 being fail and 1 being pass. Normalized metrics present a more nuanced view of how the AI system performs, but that nuance can lack interpretability. To add clarity to normalized metrics, you can set a minimum threshold score that the metric must return to be considered a pass. For example, say the metric Foo returns a score of 0.6 for an evaluation case and 0.7 for another. If you have a threshold of 0.65, then the 0.6 score is considered a fail and the 0.7 score a pass. Evaluation metrics for LLM systems broadly fall into the following categories: • Assertion-based metrics: Metrics that evaluate if an AI system output matches an in-code assertion, such as equality or regular expression match. • Statistical metrics: Metrics that use a statistical algorithm to evaluate the output of an&nbsp;AI system. • LLM-as-a-judge metrics: Metrics that use an LLM to evaluate if the output of an AI system meets qualitative criteria. • RAG metrics: Metrics that evaluate RAG systems. Generally, RAG metrics use LLMs as judges. This chapter treats RAG metrics as their own category because of their unique properties. Given the novelty of the LLM engineering space, the exact metrics you use might change, but the general categories discussed here will likely be useful. In the remainder of this section, you will learn more about these categories and the specific evaluation metrics in them. Assertion-based metrics are quantitative metrics that evaluate whether an AI system output meets certain criteria as defined in code. Assertion-based metrics resemble unit tests in traditional software engineering, where you compare whether a module output matches an expectation. You can even wrap assertion-based evaluations in a unit-testing suite. Given that your intelligent application likely already has a test suite, you can start adding evaluations to your application by including assertion-based metrics in the test suite. This is a great way to start evaluating your AI components without adding additional technical overhead to your application. However, as your application matures, you will likely want to create a separate evaluation suite. Assertion-based metrics"></li><li class="carousel-slide" data-ssplayer-slide-index="182" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/183/1739580232671?e=1743033600&amp;v=beta&amp;t=R2otjy2WPcFBEcgWtllTMCKEJcoOGh17oH0LEi1cRK0" alt="182 LLM Output Evaluation Then, execute the following Python code: Some assertion-based metrics you can use are as follows: • Equality: Evaluate whether the actual output is equal to (==) or not equal to (!=) an expected value. • Comparison operators: Evaluate whether the actual output matches comparison criteria with one of the comparison operators: greater than (>), greater than or equal to (>=), less than (<), or less than or equal to (<=). These comparison operators are useful for evaluating numeric outputs. • Sub-string match: Evaluate whether a string output includes an expected sub-string. • Regular expression match: Evaluate whether a string output matches a regular expression. In the following code example, you have a dataset of evaluation cases for the travel assistant chatbot application. This evaluation focuses on the input relevancy guardrail. The cases include the evaluation inputs, the expected output of the relevancy guardrail, and the actual output of running the inputs through the relevancy guardrail. The evaluation metric assesses whether the actual output is equal to the expected output. First, install the prettytable Python package, which you will use to output results in a readable format. Install the package in your terminal: # assertion-based evaluation pip3 install prettytable==3.10.2 from prettytable import PrettyTable input_relevance_guardrail_data = [ { { &quot;input&quot;: &quot;What should I do in New York City in July?&quot;, &quot;output&quot;: True, &quot;expected&quot;: True }, &quot;input&quot;: &quot;Can you help me with my math homework?&quot;, &quot;output&quot;: False, &quot;expected&quot;: False ] } , { &quot;input&quot;: &quot;What's the capital of France?&quot;, &quot;output&quot;: False, &quot;expected&quot;: True },"></li><li class="carousel-slide" data-ssplayer-slide-index="183" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/184/1739580232671?e=1743033600&amp;v=beta&amp;t=dyoDQ2kzoBbNL1rp4OMaCt5jJ6Yc257ikUH1mk6y3zU" alt="Evaluation metrics 183 return table def calculate_average(scores): return sum(scores) / len(scores) # Add a blank row for visual separation table.add_row([&quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;]) def evaluate_correctness(output, expected): return 1 i f output == expected else 0 # Create and print the table result_table = create_table(input_relevance_guardrail_data) print(result_table) # Add average score to bottom of the table average_score = calculate_average(scores) table.add_row([&quot;Average&quot;, &quot;&quot;, &quot;&quot;, f&quot;{average_score:.4f}&quot;]) def create_table(data): table = PrettyTable() table.f ield_names = [&quot;Input&quot;, &quot;Output&quot;, &quot;Expected&quot;, &quot;Score&quot;] score]) for case, score in zip(data, scores): table.add_row([case[&quot;input&quot;], case[&quot;output&quot;], case[&quot;expected&quot;], scores = [evaluate_correctness(case[&quot;output&quot;], case[&quot;expected&quot;]) for case in data] +--------------------------------------------+--------+----------+--------+ | Input | Output | Expected | Score | +--------------------------------------------+--------+---- ------+--------+ | What should I do in New York City in July? | True | True | 1 | | Can you help me with my math homework? | False | False | 1 | | What's the capital of France? | False | True | 0 | | | | | | | Average | | | 0.6667 | +------------- -------------------------------+--------+----------+--------+ This code outputs the following evaluation results to the terminal: The preceding code example shows how you can use assertion-based evaluation metrics to evaluate the LLM components of an intelligent application."></li><li class="carousel-slide" data-ssplayer-slide-index="184" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/185/1739580232671?e=1743033600&amp;v=beta&amp;t=GF2g37TvEfvFP5NQS7Mg_w4FheRfFK76bESsFW7pOnA" alt="184 LLM Output Evaluation Statistical metrics Statistical metrics use algorithms to determine a score. If you have a background in traditional natural language processing (NLP), you may already be familiar with the statistical metrics for evaluating LLMs’ system outputs. Statistical metrics are most useful when you are using LLM systems for tasks that would use other NLP models, such as classification, summarization, and translation. The following are some popular NLP metrics that you can use to evaluate LLM system outputs: Then, execute the following Python code: • Bilingual Evaluation Understudy (BLEU): BLEU measures the precision of a model’s output against one or more reference texts. You can use the BLEU score to calculate how similar a model output is to a reference answer. BLEU was originally developed to measure the quality of machine-translated text compared to a reference translation. You can learn more about BLEU at https://en.wikipedia.org/wiki/BLEU. • Recall-Oriented Understudy for Gisting Evaluation (ROUGE): ROUGE measures the quality of machine-generated text against one or more reference texts. In LLM systems, ROUGE is often used to assess how effectively an LLM summarizes reference texts. ROUGE is particularly useful for RAG systems, where the LLM summarizes the content in retrieved documents. It can also be used to measure the quality of a translation against a reference. You can learn more about ROUGE at https://en.wikipedia.org/wiki/ROUGE_(metric). In the following code example, you have a dataset of evaluation cases for the travel assistant chatbot application. This evaluation focuses on the response generator LLM. It calculates the BLEU score for how well the actual output measures against a reference output. It also calculates the ROUGE score for how the answer summarizes the retrieved context information. First, you must install a few Python packages. The prettytable package output results in a readable format, the sacrebleu package calculates the BLEU score, and the rouge-score package calculates the ROUGE score. Install the packages in the terminal: from prettytable import PrettyTable import sacrebleu from rouge_score import rouge_scorer pip3 install prettytable==3.10.2 sacrebleu==2.4.2 rouge-score==0.1.2 evaluation_data = [ { &quot; input&quot;: &quot;What should I do in New York City in July?&quot;, &quot;output&quot;: &quot;Check out Times Square, go to an outdoor concert, and visit the Statue of Liberty.&quot;,"></li><li class="carousel-slide" data-ssplayer-slide-index="185" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/186/1739580232671?e=1743033600&amp;v=beta&amp;t=CM6ZlJWMm3USd3AYRdF3ctp9cK8qrmKfLt2RotPg3V0" alt="Evaluation metrics 185 # Statistical evaluators def evaluate_bleu(output, golden_answer): bleu = sacrebleu.corpus_bleu([output], [ [golden_answer]]) return bleu.score / 100 # Normalize BLEU score to be between 0 and 1 def evaluate_rouge(output, contexts): context_text = (&quot;\n&quot;). join(contexts) scorer = rouge_scorer.RougeScorer([ 'rouge1', ' rouge2', ' rougeL'] , use_ stemmer=True) scores = scorer.score(context_text, output) return scores[ 'rougeL'] .fmeasure &quot;golden_answer&quot;: &quot;Explore Central Park, attend outdoor concerts, and visit rooftop bars.&quot;, &quot;contexts&quot;: [ &quot;Times Square is known for its Broadway theaters, bright l ights, and bustl ing atmosphere.&quot;, &quot;Outdoor concerts in Central Park are popular summer events attracting many visitors.&quot;, &quot;The Statue of Liberty is a symbol of freedom and a must-see landmark in NYC.&quot; ] } , { &quot;input&quot;: &quot;Can you help me with my math homework?&quot;, &quot;output&quot;: &quot;I 'm designed to assist with travel queries. For math help, try using online resources l ike Khan Academy or Mathway.&quot;, &quot;golden_answer&quot;: &quot;I am a travel assistant chatbot, so I cannot help you with your math homework.&quot;, &quot;contexts&quot;: [ ] of France.&quot;, Spain&quot;, ] } , { &quot;input&quot;: &quot;What's the capital of France?&quot;, &quot;output&quot;: &quot;The capital of France is Paris.&quot;, &quot;golden_answer&quot;: &quot;Paris is the capital of France.&quot;, &quot;contexts&quot;: [ &quot;Paris, known as the City of Light, is the most populous city &quot;European capitals: Paris, France; Berl in, Germany; Madrid, ] }"></li><li class="carousel-slide" data-ssplayer-slide-index="186" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/187/1739580232671?e=1743033600&amp;v=beta&amp;t=jrbUyTrGKJNF7W0J1wU5G7Vfb55Is7UsdCJKeGh7jlM" alt="186 LLM Output Evaluation return table def calculate_average(scores): return sum(scores) / len(scores) # Add a blank row for visual separation table.add_row([&quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;]) # Create and print the table result_table = create_table(evaluation_data) print(result_table) # Add the average score to bottom of the table average_bleu = calculate_average(bleu_scores) average_rouge = calculate_average(rouge_scores) table.add_row([&quot;Average&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, f&quot;{average_bleu:.4f}&quot;, f&quot;{average_rouge:.4f}&quot;]) # truncate strings for easier printing in table def truncate_string(s, max_length=10): return (s[:max_length] + ' . . . ' ) i f len(s) > max_length else s for case, bleu, rouge in zip(data, bleu_scores, rouge_scores): table.add_row([ truncate_string(case[&quot;input&quot;]) , truncate_string(case[&quot;output&quot;]) , truncate_string(case[&quot;golden_answer&quot;]) , len(case[&quot;contexts&quot;]) , f&quot;{bleu:.4f}&quot;, f&quot;{rouge:.4f}&quot;]) def create_table(data): table = PrettyTable() table.f ield_names = [&quot;Input&quot;, &quot;Output&quot;, &quot;Golden Answer&quot;, &quot;# Contexts&quot;, &quot;BLEU&quot;, &quot;ROUGE&quot;] bleu_scores = [evaluate_bleu(case[&quot;output&quot;], case[&quot;golden_answer&quot;]) for case in data] rouge_scores = [evaluate_rouge(case[&quot;output&quot;], case[&quot;contexts&quot;]) for case in data]"></li><li class="carousel-slide" data-ssplayer-slide-index="187" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/188/1739580232671?e=1743033600&amp;v=beta&amp;t=AdMGPb2BQS3QgY-Uqt-rQdNhAmv86cIZeJmzi4FvqVE" alt="Evaluation metrics 187 This code outputs the following to the terminal: The preceding example demonstrates how you can use BLEU and ROUGE scores as evaluation metrics to measure the outputs of the travel assistant chatbot. For instance, in the preceding example, the fact that the BLEU and ROUGE scores are so different in the first New York City test case indicates that the model answer deviates significantly from the golden answer but has relatively high adherence to the context information. This difference implies that you could optimize the retriever to get more relevant context information to better satisfy the golden answer. These statistical metrics are most useful for assessing the quality of LLM outputs when the LLMs are used for more traditional NLP tasks, such as translation and summarization. They can also provide a useful directional metric when comparing different versions of the same AI system on the same evaluation dataset. While these quantitative metrics can provide valuable insights into LLM performance, they are usually not sufficient for evaluating an LLM-powered intelligent application. These metrics often fail tocapture the nuanced aspects of language generation, such as coherence, creativity, factual correctness, and contextual appropriateness. Therefore, you need to also create qualitative evaluations tounderstand how well the LLM system performs on these metrics. In the following sections, you will learn about using LLMs as judges and RAG-specific metrics to evaluate LLM output. You can use an LLM to evaluate the outputs of an LLM system along qualitative criteria. Many LLM systems perform broad open-domain tasks, such as a chatbot carrying on extended conversations. Quantitative metrics, such as the ones discussed previously, cannot necessarily capture whether the LLM system performs these tasks effectively. For instance, a ROUGE score may be able to indicate how closely a summary tracks source documents, but it cannot tell you if the summary includes a hallucination. You will learn more about hallucinations in Chapter 11, Common Failures of Generative AI. Before the rise of LLMs, it was challenging to systematically evaluate qualitative aspects of natural language generation. Now you can use LLMs to evaluate the outputs of LLM-powered systems. Using LLMs to perform evaluations is called LLM-as-a-judge. Evaluating LLM output with another judge LLM is never a perfect solution. The judge LLM is subject to all the limitations of LLMs that require you to evaluate the LLM system in the first place. However, as of writing in mid-2024, LLM-as-a- judge seems to be the best approach to systematically perform qualitative evaluation of LLM output. +---------------+---------------+---------------+------------+--------+--------+ | Input | Output | Golden Answer | # Contexts | BLEU | ROUGE | +---------------+---------------+-- -------------+------------+--------+--------+ | What shoul. . . | Check out . . . | Explore Ce.. . | 3 | 0.0951 | 0.2857 | | Can you he.. . | I 'm design.. . | I am a tra.. . | 0 | 0.0270 | 0.0000 | | What's the.. . | The capita.. . | Paris is t . . . | 2 | 0.2907 | 0.2857 | | | | | | | | | Average | | | | 0.1376 | 0.1905 | +---------------+---------------+---------------+------------+------- -+--------+ LLM-as-a-judge evaluations"></li><li class="carousel-slide" data-ssplayer-slide-index="188" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/189/1739580232671?e=1743033600&amp;v=beta&amp;t=DI4oCL19D_L2Qij1oCDp60iAWI2eOzkge1N3mci0Doo" alt="188 LLM Output Evaluation A few areas where you can use LLM-as-a-judge qualitative metrics include the following: • Tone and style of the response • Whether the response is personalized to the user based on input information • Whether the response contains sensitive information, such as personally identifiable information, that it should not share • Whether the response complies with a certain law or regulation When creating LLM-as-a-judge evaluation metrics, it is useful to keep the following key points in mind: • Always set the LLM temperature to 0 for consistent outputs. Temperature is a hyperparameter for LLMs that controls the randomness of their predictions. A temperature of 0 produces deterministic outputs. A higher temperature produces more diverse and less consistent outputs, which can be preferable if the LLM is performing creative work. However, you want the evaluations to be as consistent as possible. • Better LLMs tend to be better evaluators. LLMs that rank higher on benchmarks tend to produce evaluation results that are more consistent with expectations. • Multi-shot prompting often improves evaluator accuracy. To perform multi-shot prompting, include examples of inputs and the outputs the model should provide, in addition to including the evaluation criteria in the model prompt. These examples often help the model perform better evaluations. Generally, you should include at least five examples that represent a diverse set of evaluation scenarios. • Chain-of-thought prompting often further improves LLM-as-a-judge evaluator performance. In a chain-of-thought prompt, you ask the model to explain its thought process before producing a final answer. • Every LLM-as-a-judge evaluation metric should only evaluate a single qualitative aspect. Focusing on a single aspect makes the evaluation task easier for the LLM to interpret. If you need to assess multiple aspects, create multiple LLM-as-a-judge evaluation metrics. • The LLM you use matters. Different LLMs can produce different outcomes on the same evaluation task. Be consistent in using the same LLM for all evaluations with a metric. If you change the LLM used by a metric, you cannot reliably compare the results produced with different LLMs. • Produce structured evaluation output. The judge LLM should produce structured outputs, such as pass or fail, or a score of integers 0-5. You can then normalize these scores. For instance, if the judge LLM outputs pass or fail, then pass is normalized as 1 and fail as 0. If the judge LLM outputs integers 0-5, 0 is normalized as 0, 1 as 0.2, 2 as 0.4... and 5 as 1. The following code example uses an LLM as a judge to evaluate whether the travel assistant chatbot provides a suggestion to the user in its response. The LLM evaluator also includes few-shot examples to improve the judge model’s understanding of the task."></li><li class="carousel-slide" data-ssplayer-slide-index="189" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/190/1739580232671?e=1743033600&amp;v=beta&amp;t=f07q7RpWg2nxpgWannnEZMruICxQRZdZ0A5m-RY3ZTY" alt="Evaluation metrics 189 Then, execute the code: The code example runs the evaluation over a dataset of inputs and outputs. Note that this is a reference- free evaluation, as the LLM-as-a-judge does not need a reference answer to determine whether the chatbot provides irrelevant answers. First, you must install a few Python packages. The prettytable package output results in a readable format and the openai package calls the OpenAI API to use the GPT-4o LLM. Install the packages in your terminal: import json from prettytable import PrettyTable import openai import os # Add your OpenAI API key to call the model openai.api_key = os.getenv(&quot;OPENAI_API_KEY&quot;) pip3 install prettytable==3.10.2 openai==1.39.0 # LLM-as-a-Judge Evaluation metric # that assesses i f the output includes a recommendation. def evaluate_includes_recommendation(input, output): # Few-shot examples to help the model produce better answers. few_shot_examples = [ # Data to evaluate evaluation_data = [ { &quot; input&quot;: &quot;What should I do in New York City in July?&quot;, &quot;output&quot;: &quot;Check out Times Square, go to an outdoor concert, and visit the Statue of Liberty.&quot;, } , { &quot;input&quot;: &quot;Can you help me with my math homework?&quot;, &quot;output&quot;: &quot;I 'm designed to assist with travel queries. For math help, try using online resources l ike Khan Academy or Mathway.&quot;, ] } , { &quot;input&quot;: &quot;What's the capital of France?&quot;, &quot;output&quot;: &quot;The capital of France is Paris.&quot;, }"></li><li class="carousel-slide" data-ssplayer-slide-index="190" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/191/1739580232671?e=1743033600&amp;v=beta&amp;t=4VrLR4zvz5llFosg0EmscZ4BOklyLKBOxfAg-1iaByQ" alt="190 LLM Output Evaluation { &quot; input&quot;: &quot;What are some good restaurants in Paris?&quot;, &quot;output&quot;: &quot;Try Le Jules Verne for an upscale dining experience, or visit Le Relais de l 'Entrecôte for a classic steak fr ites.&quot;, &quot;recommendation&quot;: True }, { &quot;input&quot;: &quot;Where should I stay in London?&quot;, &quot;output&quot;: &quot;Consider staying at The Ritz for luxury or the Hoxton for a more budget-fr iendly option.&quot;, &quot;recommendation&quot;: True }, { &quot;input&quot;: &quot;What's the weather l ike in Tokyo in winter?&quot;, &quot;output&quot;: &quot;In winter, Tokyo is generally cool with temperatures ranging from 2°C to 12°C. While you're there, consider visit ing the hot springs (onsen) for a warm and relaxing experience.&quot;, mil l ion.&quot;, (JPY).&quot;, &quot;recommendation&quot;: True }, { &quot;input&quot;: &quot;What's the population of Berl in?&quot;, &quot;output&quot;: &quot;The population of Berl in is approximately 3.6 &quot;recommendation&quot;: False }, { &quot;input&quot;: &quot;What's the currency used in Japan?&quot;, &quot;output&quot;: &quot;The currency used in Japan is the Japanese Yen &quot;recommendation&quot;: False } ] # Constructing the prompt prompt = &quot;&quot;&quot;Determine whether the fol lowing output includes a recommendation based on the input. Format response as a JSON object with the shape { &quot;recommendation&quot;: boolean }. Examples: &quot;&quot;&quot; # Append few-shot examples to the prompt. for example in few_shot_examples: prompt += f&quot;&quot;&quot;Input: {example[' input']} Output: {example['output']} Recommendation: {{ &quot;recommendation&quot;: {str(example['recommendation']) ."></li><li class="carousel-slide" data-ssplayer-slide-index="191" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/192/1739580232671?e=1743033600&amp;v=beta&amp;t=BQbxeqNZrfvPd_1EkhBzkawYS3HL1TJUs14mR5RMEOs" alt="Evaluation metrics 191 lower()} }} &quot;&quot;&quot; prompt += f&quot;&quot;&quot;Input: { input} Output: {output} Recommendation:&quot;&quot;&quot; def calculate_average(scores): return sum(scores) / len(scores) # Add a blank row for visual separation table.add_row([&quot;&quot;, &quot;&quot;, &quot;&quot;]) def create_table(data): table = PrettyTable() table.f ield_names = [&quot;Input&quot;, &quot;Output&quot;, &quot;Score&quot;] # Call the OpenAI API response = openai.chat.completions.create( # Use strong evaluator LLM model=&quot;gpt-4o&quot;, ## Format response as JSON, so it is easier to parse response_format={ &quot;type&quot;: &quot;json_object&quot; } , messages=[{ &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt }] , # Make sure temperature=0 for consistent outputs temperature=0 ) scores = [evaluate_includes_recommendation(case[&quot;input&quot;], case[&quot;output&quot;]) for case in data] for case, score in zip(data, scores): table.add_row([ truncate_string(case[&quot;input&quot;]) , truncate_string(case[&quot;output&quot;]) , score]) # truncate strings for easier printing in table def truncate_string(s, max_length=30): return (s[:max_length] + ' . . . ' ) i f len(s) > max_length else s recommendation = json.loads(response.choices[0].message.content) [&quot;recommendation&quot;] return 1 i f recommendation == True else 0"></li><li class="carousel-slide" data-ssplayer-slide-index="192" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/193/1739580232671?e=1743033600&amp;v=beta&amp;t=TQYcNtE0wYA0F4qM71lrG6By15QLUZZTU9gIkgZm3EE" alt="192 LLM Output Evaluation return table # Create and print the table result_table = create_table(evaluation_data) print(result_table) # Add the average score to bottom of the table average = calculate_average(scores) table.add_row([&quot;Average&quot;, &quot;&quot;, f&quot;{average:.4f}&quot;]) This code outputs the following to the terminal: RAG is currently one of the most popular ways to use LLMs. A distinct set of metrics has emerged to measure the efficacy of a RAG system. These metrics all use an LLM as a judge. These metrics focus on the two core components of any RAG system, retrieval and generation: • Retrieval: This component fetches relevant information from external sources. It often combines vector search with LLM-based pre- and post-processing. • Generation: This component uses an LLM to produce text outputs. The preceding example demonstrates how to create a simple LLM-as-a-judge metric to evaluate whether a response includes a recommendation. You can extend the techniques to create additional LLM-as-a-judge metrics to look at various aspects of your LLM system. In the next section, you will learn about some more complex LLM-as-a-judge metrics for evaluating RAG systems. +-----------------------------------+-----------------------------------+--------+ | Input | Output | Score | +-----------------------------------+-----------------------------------+------- -+ | What should I do in New York C.. . | Check out Times Square, go to . . . | 1 | | Can you help me with my math h.. . | I 'm designed to assist with tr. . . | 1 | | What's the capital of France? | The capital of France is Paris. . . | 0 | | | | | | Average | | 0.6667 | +--------------- --------------------+-----------------------------------+--------+ RAG metrics"></li><li class="carousel-slide" data-ssplayer-slide-index="193" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/194/1739580232671?e=1743033600&amp;v=beta&amp;t=XNKT4rLr20d33a9T9dd5jC3d7Sr6GQjTQollQP07Jj4" alt="Evaluation metrics 193 The data to input into the faithfulness formula is derived with these steps: 1. Extract all claims from the generated response with an LLM. 2. Locate each claim in the reference material with an LLM. 3. Calculate the proportion of claims that can be inferred from the context information. The following code example uses the Ragas faithfulness metric on an example set of input, contexts, and RAG system outputs. First, you must install a few Python packages. The ragas package includes the response faithfulness metric and a reporting module. The langchain-openai package lets you pass an OpenAI model to Ragas. This example uses the GPT-4o mini model. Ragas also depends on the datasets package to format inputs. Install the packages in your terminal: The following LLM-as-a-judge metrics are often used to evaluate RAG systems: • Answer faithfulness: Measures how grounded the generated response is to the retrieved context information • Answer relevance: Measures how relevant the generated response is to the provided input Ragas is a popular Python library that includes modules implementing these metrics along with others for RAG evaluation. In the remainder of this section, you will learn how Ragas implements these metrics. To learn more about Ragas and its available metrics, refer to its documentation (https://docs.ragas.io/en/stable/index.html). Answer faithfulness Answer faithfulness is an evaluation metric for the generation component of RAG systems. It measures the extent to which the information in the generated response aligns with the retrieved context information. By identifying factual discrepancies between the generated answer and the retrieved context, the answer faithfulness metric can help identify if there are any hallucinations in the answer. Ragas includes a module to measure faithfulness. It calculates faithfulness with this formula: pip3 install ragas==0.1.13 langchain-openai==0.1.20 datasets==2.20.0 Faithfulness Number of claims that can be inferred from the context Total number of claims in the response ="></li><li class="carousel-slide" data-ssplayer-slide-index="194" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/195/1739580232671?e=1743033600&amp;v=beta&amp;t=PWlmzKI-E6dprtvMZeAZoMLygFegUF_1l95RbsudC2g" alt="194 LLM Output Evaluation Then, run the following code to perform the evaluation: openai_api_key = os.getenv(&quot;OPENAI_API_KEY&quot;) # Format our dataset for Ragas data structure def prepare_data_for_ragas(data_list): data_table = { 'question': [ ] , from ragas.metrics import faithfulness from ragas import evaluate from datasets import Dataset from langchain_openai.chat_models import ChatOpenAI import os evaluation_data = [ { &quot; input&quot;: &quot;What should I do in New York City in July?&quot;, &quot;output&quot;: &quot;Check out Times Square, go to an outdoor concert, and visit the Statue of Liberty.&quot;, &quot;contexts&quot;: [ &quot;Times Square is known for its Broadway theaters, bright l ights, and bustl ing atmosphere.&quot;, &quot;Outdoor concerts in Central Park are popular summer events attracting many visitors.&quot;, &quot;The Statue of Liberty is a symbol of freedom and a must-see landmark in NYC.&quot; ] } , { &quot;input&quot;: &quot;Can you help me with my math homework?&quot;, &quot;output&quot;: &quot;I 'm designed to assist with travel queries. For math help, try using online resources l ike Khan Academy or Mathway.&quot;, of France.&quot;, Spain&quot;, ] &quot;contexts&quot;: [ ] } , { &quot;input&quot;: &quot;What's the capital of France?&quot;, &quot;output&quot;: &quot;The capital of France is Paris.&quot;, &quot;contexts&quot;: [ &quot;Paris, known as the City of Light, is the most populous city &quot;European capitals: Paris, France; Berl in, Germany; Madrid, ] }"></li><li class="carousel-slide" data-ssplayer-slide-index="195" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/196/1739580232671?e=1743033600&amp;v=beta&amp;t=6bVWG813SpJ1f0Yt65GFgMiMb_g8arRTSjk6vnmaHhc" alt="Evaluation metrics 195 return data_table def create_report(data): # Create and print the table results = create_report(evaluation_data) print(results.to_pandas()) print(results) Evaluating: 100% 3/3 [00:05<00:00, 0 1 2 1.72s/it] question What should I do in New York City in July? Can you help me with my math homework? What's the capital of France? \ 0 1 2 answer \ Check out Times Square, go to an outdoor conce.. . I 'm designed to assist with travel queries. Fo.. . The capital of France is Paris. 'answer': [ ] , 'contexts': [ ] } f o r d a t a _ i t e m i n d a t a _ l i s t : data_table[&quot;question&quot;].append(data_item[&quot;input&quot;]) data_table[&quot;answer&quot;].append(data_item[&quot;output&quot;]) data_table[&quot;contexts&quot;].append(data_item[&quot;contexts&quot;]) contexts faithfulness 0 [Times Square is known for its Broadway theate.. . 1.0 1 [ ] 0.0 2 [Paris, known as the City of Light, is the mos.. . 1.0 { ' faithfulness': 0.6667} ragas_dict = prepare_data_for_ragas(data) dataset = Dataset.from_dict(prepare_data_for_ragas(data)) langchain_llm = ChatOpenAI( model_name=&quot;gpt-4o-mini&quot;, api_key=openai_api_key) score = evaluate(dataset, metrics=[faithfulness], l lm=langchain_llm) return score Executing this code outputs results resembling the following to the terminal:"></li><li class="carousel-slide" data-ssplayer-slide-index="196" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/197/1739580232671?e=1743033600&amp;v=beta&amp;t=UFeUxp8odi-Cb0fp47nUPH4vsRbDV8DrWgb7bi04wAU" alt="196 LLM Output Evaluation You can see from the results that the Ragas evaluator deemed the first and third examples faithful, and not the second one. In the following section, you will learn how to use another RAG evaluation metric: answer relevance. Answer relevance Answer relevance measures how relevant the output of a RAG system is to the input. This metric is useful because it determines how well a RAG system responds to the provided input. Ragas uses the input, generated output, and context information retrieved to generate that output in its answer relevance metric. It calculates the answer relevance evaluation metric score with the following steps: 1. Use an LLM to generate a list of questions from the generated response. 2. Create a vector embedding for each LLM-generated question from the previous step. Also, create a vector embedding for the initial input query. Calculate the cosine similarity between the original question embedding and each generated question embedding. The answer relevance score is the mean of the cosine similarities between the original question and each generated question. 3. 4. Ragas assumes that if the generated answer is highly relevant to the original question, then the questions that can be derived from this answer should be semantically similar to the original question. This assumption is based on the idea that a relevant answer contains information that directly addresses the query. Therefore, the judge LLM should be able to reverse-engineer questions that closely align with the original input. The following code example uses the Ragas answer relevance metric on an example set of input, contexts, and RAG system outputs. First, you must install a few Python packages. Note that these are the same dependencies as for the Ragas faithfulness evaluation example in the previous section. The ragas package includes the response answer relevance metric and a reporting module. The langchain-openai package lets you pass an OpenAI model to Ragas. This example uses the GPT-4o mini model. Ragas also depends on the datasets package to format inputs. Install the packages in your terminal: pip3 install ragas==0.1.13 langchain-openai==0.1.20 datasets==2.20.0"></li><li class="carousel-slide" data-ssplayer-slide-index="197" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/198/1739580232671?e=1743033600&amp;v=beta&amp;t=fKSAvMOsU6kbq_c_boZJ3ORFdu8xazKW6LsuN-S4ZHg" alt="Evaluation metrics 197 Then, run the following code to perform the evaluation: openai_api_key = os.getenv(&quot;OPENAI_API_KEY&quot;) from ragas.metrics import answer_relevancy from ragas import evaluate from datasets import Dataset from langchain_openai.chat_models import ChatOpenAI from langchain_openai.embeddings import OpenAIEmbeddings import os evaluation_data = [ { &quot; input&quot;: &quot;What should I do in New York City in July?&quot;, &quot;output&quot;: &quot;Check out Times Square, go to an outdoor concert, and visit the Statue of Liberty.&quot;, &quot;contexts&quot;: [ &quot;Times Square is known for its Broadway theaters, bright l ights, and bustl ing atmosphere.&quot;, &quot;Outdoor concerts in Central Park are popular summer events attracting many visitors.&quot;, &quot;The Statue of Liberty is a symbol of freedom and a must-see landmark in NYC.&quot; ] } , { &quot;input&quot;: &quot;Can you help me with my math homework?&quot;, &quot;output&quot;: &quot;I 'm designed to assist with travel queries. For math help, try using online resources l ike Khan Academy or Mathway.&quot;, of France.&quot;, Spain&quot;, ] &quot;contexts&quot;: [ ] } , { &quot;input&quot;: &quot;What's the capital of France?&quot;, &quot;output&quot;: &quot;The capital of France is Paris.&quot;, &quot;contexts&quot;: [ &quot;Paris, known as the City of Light, is the most populous city &quot;European capitals: Paris, France; Berl in, Germany; Madrid, ] }"></li><li class="carousel-slide" data-ssplayer-slide-index="198" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/199/1739580232671?e=1743033600&amp;v=beta&amp;t=SCF6lbgM97L2PTfVRAK4wBsGom3_0GSaFmGr-sFyte0" alt="198 LLM Output Evaluation return data_table def create_report(data): # Create and print the table results = create_report(evaluation_data) print(results.to_pandas()) print(results) # Format our dataset for Ragas data structure def prepare_data_for_ragas(data_list): data_table = { 'question': [ ] , 'answer': [ ] , 'contexts': [ ] } f o r d a t a _ i t e m i n d a t a _ l i s t : data_table[&quot;question&quot;].append(data_item[&quot;input&quot;]) data_table[&quot;answer&quot;].append(data_item[&quot;output&quot;]) data_table[&quot;contexts&quot;].append(data_item[&quot;contexts&quot;]) ragas_dict = prepare_data_for_ragas(data) dataset = Dataset.from_dict(prepare_data_for_ragas(data)) langchain_llm = ChatOpenAI( model_name=&quot;gpt-4o-mini&quot;, api_key=openai_api_key) langchain_embeddings = OpenAIEmbeddings( model=&quot;text-embedding-3-large&quot;, api_key=openai_api_key ) s c o r e = e v a l u a t e ( d a t a s e t , metrics=[answer_relevancy], l lm=langchain_llm, embeddings=langchain_embeddings ) return score"></li><li class="carousel-slide" data-ssplayer-slide-index="199" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/200/1739580232671?e=1743033600&amp;v=beta&amp;t=cR1tZ1tg4WKHAN_xyjekj7j31tVIBHbcv5_s0z9Dumo" alt="Evaluation metrics 199 Executing this code outputs the following results to the terminal: You can see from the results that the first and third cases were relevant, while the second was not. This makes sense because the first and third had quite relevant contexts, whereas the second had no context information at all. The Ragas answer relevance metric has noteworthy limitations. The quality of the underlying language model significantly impacts the metric’s effectiveness, as it relies heavily on the LLM’s capacity to generate appropriate questions from the given answer. The metric may also struggle with handling complex or multi-faceted queries, particularly when the answer doesn’t comprehensively address all aspects of the original question, potentially resulting in an incomplete assessment of relevance for more intricate topics. There are other approaches that you can take to evaluate answer relevance. For instance, the DeepEval evaluation framework calculates answer relevancy with the following strategy: 1. Extract all statements in an output with an LLM. 2. Use the same LLM to determine which statements are relevant to the input. 3. Calculate answer relevance as the number of relevant statements divided by the total number of statements. The difference between the Ragas and DeepEval strategies to calculating the answer relevancy metric demonstrates that the AI engineering field is still converging on how to calculate these metrics, even if it is becoming standard to evaluate based on some form of these metrics. Evaluating: 100% 3/3 [00:04<00:00, 0 1 2 4.85s/it] question What should I do in New York City in July? Can you help me with my math homework? What's the capital of France? \ 0 1 2 answer \ Check out Times Square, go to an outdoor conce.. . I 'm designed to assist with travel queries. Fo.. . The capital of France is Paris. contexts answer_relevancy 0 [Times Square is known for its Broadway theate.. . 0.630561 1 [ ] 0.000000 2 [Paris, known as the City of Light, is the mos.. . 0.873249 { 'answer_relevancy': 0.5013}"></li><li class="carousel-slide" data-ssplayer-slide-index="200" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/201/1739580232671?e=1743033600&amp;v=beta&amp;t=ylPR-9t4jYea-BU2yvbKFWmdd5ndtLFzXDBrSJ3pomY" alt="200 LLM Output Evaluation Using the RAG evaluation metrics discussed in this section, you can measure how well your RAG system is performing and measure improvement in the system over time. You can also experiment with other RAG metrics in frameworks such as Ragas or DeepEval. In the next section, you will learn how you can perform a manual human review of your data to augment the automated evaluation metrics discussed in this section. While LLMs can be effective tools for qualitative evaluation, they are often inferior to the original form of non-artificial intelligence: humans. Human review is considered the gold standard of qualitative review. When using human review, you should take into account that humans likely prefer simpler rating metrics that do not require them to do complex multi-step calculations, such as the answer relevance metric described earlier in this chapter. Instead, give human reviewers simple rating systems. Pass/ fail criteria are the simplest and can be normalized to 0 or 1. You can also use a rating system such as 0-5, which can be normalized to 0, 0.2, and so on until 1. Human reviewer free-form feedback can be particularly valuable, as this open-ended feedback can provide insight that would not be captured by the rating metric alone. It is also useful to capture who the human reviewer is for an evaluation. You can use this to follow up with the person if need be or to track how some individuals perform ratings as compared to others. Despite the qualitative advantage of human review, it also comes with its own set of limitations: • Cost: Human reviewers tend to be more expensive than using an LLM as a judge. • Time: Human reviewers usually take much longer than using an LLM as a judge. You also cannot parallelize a single human like you can an AI model. • Tedium: Evaluating the output of LLMs can be an incredibly tedious task for human reviewers. Many people do not want to perform evaluations, so it can be difficult to find people to consistently perform the evaluations. • Elasticity: Often, you need to run large numbers of evaluations as part of your software development process or at regular intervals. It can be hard to find human reviewers to perform an evaluation exactly when you need them to. • Inconsistency: Human reviewers can be inconsistent in their evaluation. Different people might evaluate the same case in different ways. The same person could even evaluate the same case differently at a different moment, depending on factors such as tiredness, mood, and environment. Given the strengths and weaknesses of using humans as reviewers, you must carefully consider when to use human review. Human review is probably the most useful for conducting initial qualitative evaluation. Human reviewers can set a baseline for application performance that you can measure against with a reasonably high degree of confidence. Human review"></li><li class="carousel-slide" data-ssplayer-slide-index="201" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/202/1739580232671?e=1743033600&amp;v=beta&amp;t=nCEe_i4Y-fqaGIe86kh5AUhdShE5cMSnN8iFNbc3BPs" alt="Summary 201 You can also use human reviews as a baseline to measure LLM-as-a-judge metrics against. You can try to get the LLM-as-a-judge metric to conform as closely as possible to the human review results. Additionally, your LLM-as-a-judge metric can use examples from the human review in its prompt to demonstrate to the LLM what the classification should look like as a form of multi-shot prompting. Multi-shot prompting has been shown to increase model performance meaningfully. Human review is one of the most effective means of qualitative evaluation, if also a slow and expensive one. In this chapter, you explored methods for evaluating LLM outputs in your intelligent application. You learned what LLM evaluation is and why it’s important for your intelligent application. Model benchmarking is a form of evaluation that can help you determine which LLMs to use in your application. Once your application has functional AI modules, you can make evaluation datasets and run metrics on them to measure performance and change over time. In addition to the automated evaluations, you can perform manual human review to further measure application quality. Finally, you can use reference-free metrics as guardrails within your application. In the next chapter, you will learn how to optimize the semantic data model to enhance retrieval accuracy and overall performance. A guardrail is a mechanism that prevents the AI from producing an undesirable or incorrect output. Guardrails ensure that generated responses are within acceptable boundaries and align with your application’s quality, ethical, and relevance standards. Previously in this chapter, you learned about reference-free evaluations. These are evaluations that only require an input without a reference output or golden answer. You can also use reference-free evaluations as guardrails to help ensure the AI system performs correctly. For example, in the RAG metrics section, you looked at the answer relevance metric. You could use this as a guardrail in the travel assistant chatbot to ensure that the chatbot only responds with answers that meet a certain relevancy threshold. If the answer doesn’t meet this threshold, you could perform some additional application logic before responding to the user. Throughout this chapter, you have learned how to use evaluations to improve the quality of your intelligent application. Using reference-free evaluations as guardrails lets you extend the utility of your evaluations to a component of the application itself. Evaluations as guardrails Summary"></li><li class="carousel-slide" data-ssplayer-slide-index="202" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/203/1739580232671?e=1743033600&amp;v=beta&amp;t=EKYdRegxhxEtDtJzSLbqNAlivBXxJt2CUx-YiQqk_gU" alt="203"></li><li class="carousel-slide" data-ssplayer-slide-index="203" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/204/1739580232671?e=1743033600&amp;v=beta&amp;t=-e6YdibzZh5h3c8DZerBpA8X5K-gLaNkuWRrqfLOlrg" alt="10 Refining the Semantic Data Model to Improve Accuracy To effectively use vector search for semantic long-term memory in an intelligent application, you must optimize the semantic data model to the application’s needs. As the semantic data model uses vector embedding models and vector search, you must optimize the contents of the embedded data and the way the data is retrieved. Refining the semantic data model can lead to significant improvements in retrieval accuracy and overall application performance. In retrieval-augmented generation (RAG) applications, aneffective semantic data model serves as the foundation for a robust retrieval system, which directly informs the quality of the generated outputs. The rest of the chapter examines different ways in which youcan refine the semantic data model and retrieval. This chapter will cover the following topics: • Experimenting with different embedding models • Fine-tuning embedding models • Including metadata in the embedded content to maximize semantic relevance • Various techniques to optimize RAG use cases, including query mutation, formatting ingested data, and advanced retrieval systems"></li><li class="carousel-slide" data-ssplayer-slide-index="204" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/205/1739580232671?e=1743033600&amp;v=beta&amp;t=57IUgwt_wPSxcd5-fAbW7XM4QAL-U6E-0YxqXCUR-HM" alt="204 Refining the Semantic Data Model to Improve Accuracy Embeddings Vector embeddings are the foundation of the semantic data model, serving as the machine- interpretable representation of ideas and relationships. Embeddings are mathematical representations of objects as points in a multi-dimensional space. They act as the glue that connects the various semantic pieces of data in an intelligent application. The distance between vectors correlates to semantic similarity. You can use this semantic similarity score to retrieve related information that would otherwise be difficult to connect. This concept holds true regardless of the specific use case, be it RAG, recommendation systems, anomaly detection, or others. Having an embedding model better tailored to a use case can improve accuracy and performance. Experimenting with different embedding models and fine-tuning them on domain-specific data can help identify the best fit for a particular use case, further enhancing their effectiveness. Technical requirements You will need the following technical requirements to run the code in this chapter: • A programming environment with Python 3.x installed • A programming environment capable of running the open source embedding model en-v1.5 locally • An OpenAI API key. To create an API key, refer to the OpenAI documentation at When building intelligent applications, you can experiment with different pre-trained embedding models. Different models have varying accuracy, cost, and efficiency. Their performance can vary significantly depending on the specific application and data. By experimenting with multiple models, developers can identify the best fit for their use case. Table 10.1 lists some popular embedding models as of writing in spring 2024 that are taken from the Hugging Face Massive Test Embedding Benchmark (MTEB) Leaderboard:1 https:// platform.openai.com/docs/quickstart/step-2-set-up-your-api-key gte-base- Experimenting with different embedding models 1 The information from the MTEB Leaderboard was taken on April 30, 2024. ( co/spaces/mteb/leaderboard) https://huggingface."></li><li class="carousel-slide" data-ssplayer-slide-index="205" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/206/1739580232671?e=1743033600&amp;v=beta&amp;t=O4H7X2hBOFddkdiDGCCqTboO_oY01UidQ2wuocuWglk" alt="Table 10.1: Selected embedding models Embeddings 205 Model name De vel op er OpenAI C ohere Alibaba Sentence Transformers Is it open source? No No Yes Yes Embedding length 3072 1024 768 768 Average score2 64.59 64.47 64.11 57.06 To properly compare different embedding models, you must have a consistent evaluation framework. This involves defining a set of relevant evaluation datasets and metrics. Use the same evaluation sets and metrics across all models for fair comparison. The evaluation datasets should be representative of the relevant application domain. An evaluation framework will help you iterate and refine the evaluation process over time, incorporating learnings from initial experiments to progressively improve the application. The following are useful evaluation metrics for using embedding models for information retrieval. The metrics are taken from Ragas, a framework for RAG evaluation: • Context precision: Evaluates whether the retrieved results contain ground-truth facts that you would want to answer the input query. Relevant items present in the contexts are ranked highly in the retrieved results. • Context entities recall: Evaluates what fraction of the entities from a set of ground truths are preset in the retrieved information. Ragas supports other RAG evaluation metrics as well, which you can learn more about in the Ragas documentation (https://docs.ragas.io/en/stable/). The following code example uses Ragas and LangChain to evaluate how different embedding models perform on the context entities recall metric. First, install the required dependencies in the terminal: sentence-t5-large text-embedding-3-large cohere-embed-english-v3.0 gte-base-en-v1.5 pip3 install ragas==0.1.13 datasets==2.20.0 langchain==0.2.12 openai==1.39.0 faiss-cpu==1.8.0.post1 2 This score is calculated as an average of a variety of benchmarks. For more information about the evaluation metrics used in the benchmark, refer to the MTEB: Massive Text Embedding Benchmark research paper (https://arxiv.org/abs/2210.07316)."></li><li class="carousel-slide" data-ssplayer-slide-index="206" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/207/1739580232671?e=1743033600&amp;v=beta&amp;t=SlHOlMo2gIQTIfHXPUmnfNBBTa_N_zQ2JslPLaXjP8g" alt="206 Refining the Semantic Data Model to Improve Accuracy The following code evaluates how the OpenAI text-embedding-ada-002 and text-embedding-3- large embedding models perform on the Ragas context entities recall evaluation for a sample dataset: sample_size = 100 # Get sample questions from the sample dataset. sample_questions = dataset[ 'question'][ :sample_size] # Add your OpenAI API key to the environment variables openai_api_key = os.getenv(&quot;OPENAI_API_KEY&quot;) # Get sample context information from the sample dataset. sample_contexts = [ item for row in dataset[&quot;contexts&quot;] [ :sample_size] for i tem in row] # Ragas evaluation config to use in al l evaluations. ragas_run_config = RunConfig(max_workers=4, max_wait=180) # Break sample context into chunks to use with vector search. text_splitter = RecursiveCharacterTextSplitter( ) chunks: List[str] = [ ] for context in sample_contexts: chunk_size=400, chunk_overlap=100, add_start_index=True spl it_chunks = text_splitter.split_text(context) chunks.extend(split_chunks) sample_ground_truths = [ item for row in dataset[&quot;ground_truths&quot;] [ :sample_size] for i tem in row] from ragas.metrics import context_entity_recall from ragas import evaluate, RunConfig from datasets import load_dataset, Dataset from langchain_openai import ChatOpenAI, OpenAIEmbeddings from langchain_text_splitters import RecursiveCharacterTextSplitter from langchain_community.vectorstores import FAISS import os from typing import List # Load sample dataset. dataset = load_dataset(&quot;explodinggradients/amnesty_qa&quot;, split=&quot;eval&quot;) # Embedding models that we are evaluating. openai_embedding_models = [&quot;text-embedding-ada-002&quot;, &quot;text-embedding-3- large&quot;]"></li><li class="carousel-slide" data-ssplayer-slide-index="207" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/208/1739580232671?e=1743033600&amp;v=beta&amp;t=Bf3wQyEYYqeVre7KpzSrT1qqqtBylDuM--e1cK_wLqk" alt="Embeddings 207 # #Evaluate each embedding model for embedding_model in openai_embedding_models: # Create an in-memory vector store for the evaluation. db = FAISS.from_texts( chunks, OpenAIEmbeddings(openai_api_key=openai_api_key, model=embedding_model)) # Get retrieved context using similarity search. retrieval_contexts: List[str] = [ ] for question in sample_questions: search_results = db.similarity_search(question) retrieval_contexts.append(l ist(map( lambda result: result.page_content, search_results))) Results for embedding model ' text-embedding-ada-002': { 'context_entity_ recall ' : 0.5687} Results for embedding model ' text-embedding-3-large': { 'context_entity_ recall ' : 0.5973} mini&quot;) # Run evaluation for context relevancy of retrieved information. result = evaluate( dataset=Dataset.from_dict({ &quot;question&quot;: sample_questions, &quot;contexts&quot;: retrieval_contexts, &quot;ground_truth&quot;: sample_ground_truths }), metrics=[context_entity_recall] , run_config=ragas_run_config, raise_exceptions=False, l lm=ChatOpenAI(openai_api_key=openai_api_key, model_name=&quot;gpt-4o- ) # Print out results print(f&quot;Results for embedding model ' {embedding_model} ' :&quot;) print(result) This code outputs results resembling the following to the terminal: As you can see from these results, text-embedding-3-large yields higher context entity recall on this evaluation. The context relevancy score is normalized between 0 and 1, inclusive. When creating evaluations for your own application, consider using sample data that’s relevant to your use case for a better comparison. Also, you will likely want to include a representative sample of at least 100 examples."></li><li class="carousel-slide" data-ssplayer-slide-index="208" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/209/1739580232671?e=1743033600&amp;v=beta&amp;t=JkZnMiC0VmPxr4F7x-j508x3q851XAwIWM3jAglzL4o" alt="208 Refining the Semantic Data Model to Improve Accuracy Table 10.2: Methods for fine-tuning embedding models Fine-tuning embedding models In addition to experimenting with different pre-trained models, you can fine-tune a pre-trained embedding model to optimize it for your use case. Fine-tuning an embedding model can be beneficial in the following scenarios: Type Anchor Positive Negative Here’s a brief code example of using the the open source embedding model and Text that should be represented as similar to the anchor example. Definition The reference text that serves as the starting point for identifying similar and dissimilar examples. Text that should be represented as dissimilar or different from the anchor example. Example libraries to fine-tune • Domain-specific data: If the application deals with domain-specific data that might not be well captured using an off-the-shelf model, such as legal documents or medical records with specialized terminology, fine-tuning can help the model better understand and represent the domain-specific concepts. • Avoiding undesirable matches: In cases where there are seemingly similar concepts that should be differentiated, fine-tuning can help the model distinguish between them. For example, you could fine-tune the model to differentiate between Apple the company and apple the fruit. However, off-the-shelf embedding models are often highly performant for many tasks, especially when combined with the metadata enrichment and RAG optimizations discussed later in this chapter. The available options for fine-tuning an embedding model can vary depending on the model and how it is hosted. Managed model hosting providers might only expose certain methods for their models, whereas using an open source model can provide more flexibility. The SentenceTransformers (https:// sbert.net/) framework is designed for using and fine-tuning open-source embedding models. Generally, fine-tuning involves providing similar pairs of sentences, optionally including a magnitude of similarity. Alternatively, anchor, positive, and negative examples can be provided to guide the fine- tuning process. Table 10.2 provides an overview of anchor, positive, and negative examples, that are used in the subsequent code example: SentenceTransformers gte-base-en-v1.5. PyTorch &quot;I love eating apples.&quot; &quot;Apple is a tech company.&quot; &quot;Apples are my favorite fruit.&quot;"></li><li class="carousel-slide" data-ssplayer-slide-index="209" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/210/1739580232671?e=1743033600&amp;v=beta&amp;t=295E3RwejgxhAVD6vPHIW9lJTgpMm2JyUkqWSFbepms" alt="Embeddings 209 Then run the following code: First, install the dependencies in the terminal: # Print similarity score before training print(&quot;Before training:&quot;) similarity_before = get_similarity_score() pip3 install sentence-transformers==3.0.1 torch==2.2.2 from sentence_transformers import SentenceTransformer, InputExample, losses, uti l from torch.uti ls.data import DataLoader # Function to print similarity score def get_similarity_score(): sentence1 = &quot;I love the taste of fresh apples.&quot; sentence2 = &quot;Apples are rich in vitamins and f iber.&quot; embedding1 = model.encode(sentence1) embedding2 = model.encode(sentence2) cosine_score = uti l .cos_sim(embedding1, embedding2) score_number = cosine_score.item() print(f&quot;Cosine similarity between '{sentence1}' and '{sentence2}': {score_number:.4f}&quot;) return cosine_score # Load embedding model model = SentenceTransformer(&quot;Alibaba-NLP/gte-base-en-v1.5&quot;, trust_remote_ code=True) train_examples = [ InputExample(texts=[&quot;I love eating apples.&quot;, &quot;Apples are my favorite fruit&quot;, &quot;Apple is a tech company&quot;]), InputExample(texts=[&quot;Chocolate is a sweet treat loved by many.&quot;, &quot;I can't resist a good piece of chocolate.&quot;, &quot;Chocolate Rain was one of the most popular songs on YouTube from 2007.&quot;]) , InputExample(texts=[&quot;Ice cream is a refreshing dessert.&quot;, &quot;I love trying different ice cream flavors.&quot;, &quot;The rapper and actor Ice Cube was wearing a cream colored suit to the VMAs.&quot;]) , InputExample(texts=[&quot;Salad is a healthy meal option.&quot;, &quot;I love a fresh, crisp salad with various vegetables.&quot;, &quot;Salad Fingers is a surreal web series created by David Firth.&quot;]) , ] train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=8) train_loss = losses.TripletLoss(model=model)"></li><li class="carousel-slide" data-ssplayer-slide-index="210" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/211/1739580232671?e=1743033600&amp;v=beta&amp;t=EzE43cJbB7rYEexffJoPHan2NAkeorFQ9E1xTsO7eNY" alt="210 Refining the Semantic Data Model to Improve Accuracy print(&quot;After training:&quot;) similarity_after = get_similarity_score() # f ine tune model.f it(train_objectives=[(train_dataloader, train_loss)] , epochs=10) similarity_difference = similarity_after - s imilarity_before print(f&quot;Change in similarity score: {similarity_difference.item():4f}&quot;) Before training: Cosine similarity between 'I love the taste of fresh apples. ' and 'Apples are r ich in vitamins and f iber. ' : 0.4402 [10/10 00:05, Epoch 10/10] After training: Cosine similarity between 'I love the taste of fresh apples. ' and 'Apples are r ich in vitamins and f iber. ' : 0.4407 Change in similarity score: 0.000540 This code outputs a result resembling the following to the terminal: Including metadata in embedded content can significantly improve the quality of retrieval results by adding greater semantic meaning to it. Metadata creates a richer and more meaningful semantic representation of content. Metadata can include descriptors such as the type of content, tags, titles, and summaries. The following table contains some useful examples of metadata to include in embedded content: As you can see from this example, just the small fine-tuning that was performed, increased the vector similarity between the related sentences. If you would like to learn more about fine-tuning embedding models, a great place to start is the Train and Fine-Tune Sentence Transformers Models Hugging Face blog post by OmarEspejel (https://huggingface.co/blog/how-to-train-sentence-transformers). This blog post includes a more detailed look at fine-tuning an embedding model using a similar approach to the one in the preceding code example. The following section discusses how you can further enhance the semantic data model after you have chosen the right data model by embedding relevant metadata in the text. Embedding metadata"></li><li class="carousel-slide" data-ssplayer-slide-index="211" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/212/1739580232671?e=1743033600&amp;v=beta&amp;t=OIbykXWje3AnT4_huu2gUbAH4sbKMAgpDsElJvQXCNI" alt="Table 10.3: Useful types of embedded metadata Embedding metadata 211 Type Content type Tags Title of document Summary of document Example(s) Article, recipe, product review, etc. “dinner”, “Italian”, “vegetarian” Roasted Garlic and Tomato Pasta A simple pasta dish featuring roasted garlic and cherry tomatoes in a light sauce You can also include metadata types that are specific to your application. For example, consider creating a RAG chatbot where users ask natural language questions and get generated answers on cooking and recipes. You have the following recipe for Roast Garlic and Tomato Pasta to include in your recipe database: ## Ingredients ## Instructions Yield: 4 servings - 8 oz pasta - 1 head garlic - 1 pint cherry tomatoes - 1/4 cup olive oil - 1/2 cup fresh basil, chopped - Salt and pepper # Roasted Garlic and Tomato Pasta 1. Preheat oven to 400°F (200°C). 2. Cut the top off the garlic head, drizzle with olive oil, wrap in foil, and roast for 30 minutes. 3. Roast cherry tomatoes with olive oil, salt, and pepper for 20 minutes until blistered. 4. Cook pasta according to package instructions. Mix pasta with roasted garlic (squeezed out), tomatoes, and olive oil. 5. Stir in basil, season with salt and pepper, and serve."></li><li class="carousel-slide" data-ssplayer-slide-index="212" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/213/1739580232671?e=1743033600&amp;v=beta&amp;t=LnD5zDZOrOjf8QnUQajsfd14p8wJhACTW9ZgjmbpEdU" alt="212 Refining the Semantic Data Model to Improve Accuracy By including this metadata with the embedded text, you imbue the text with greater semantic meaning making it more likely that user queries will capture the correct content. This makes relevant user queries have greater cosine similarity scores with the text. The following table shows the cosine similarity scores between various queries and the text with and without metadata using the BAAI/bge-large-en-v1.5 embedding model: When creating a vector embedding for the recipe, you could include the following metadata before the recipe text: - 8 oz pasta ## Ingredients ## Instructions Yield: 4 servings ...other ingredients # Roasted Garlic and Tomato Pasta 1. Preheat your oven to 400°F (200°C). ...other instructions --- contentType: recipe recipeTitle: Roasted Garlic and Tomato Pasta keyIngredients: pasta, garlic, tomatoes, olive oil, basil servings: 4 tags: [dinner, Italian, vegetarian] summary: A simple pasta dish featuring roasted garlic and cherry tomatoes in a light sauce ---"></li><li class="carousel-slide" data-ssplayer-slide-index="213" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/214/1739580232671?e=1743033600&amp;v=beta&amp;t=XHD8FHf-bFrsFsyyU8xaCOvhW6ObpaC0tOOzLzi-Rps" alt="Table 10.4: Comparing cosine similarity of vectors of text with and without embedded metadata Embedding metadata 213 Query text Text without metadata similarity score 0.7141546 Text with metadata similarity score 0.7306514 Metadata similarity score improvement 0.016496778 As you can see in Table 10.4, the text with prepended metadata has a higher cosine similarity for a diverse set of relevant queries. This means that the relevant content is more likely to be surfaced and used in the RAG chatbot. When including metadata, you must consider how it is structured to optimize processing and interpretation. You should use a machine-readable format that is easy to parse and manipulate, such as YAML (https://yaml.org/spec/1.2.2/), JSON (https://www.json.org/json-en. html), or TOML (https://toml.io/). YAML is generally more token-efficient compared to other data formats such as JSON. This means that using YAML saves on the compute cost of processing extra tokens and also represents the same idea with fewer distraction tokens that could dilute the LLM’s ability to interpret the input and produce a high-quality output. YAML also has widespread adoption, so embedding models and LLMs can effectively work with it. simple vegetarian pasta with roasted vegetables vegetarian italian pasta dinner I have tomatoes, basil and pasta in my fridge. What to make? 0.71199816 0.60327804 0.6559261 0.76754296 0.055544794 0.052648067 Formatting metadata"></li><li class="carousel-slide" data-ssplayer-slide-index="214" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/215/1739580232671?e=1743033600&amp;v=beta&amp;t=adArQS_Pc92iaqFTIyfsZH0xcUbfd8r7cYRaOmnrQc0" alt="214 Refining the Semantic Data Model to Improve Accuracy Table 10.5: Comparing token length of the same content in YAML and JSON JSON YAML Format C ontent 60 89 Token count As you can see in Table 10.5, YAML uses approximately two-thirds of tokens compared to JSON. The exact difference in token usage depends on the data and formatting. YAML generally proves to be a more efficient metadata format than JSON. If including metadata alongside additional text, consider including it as front matter (https:// jekyllrb.com/docs/front-matter/). Front matter puts YAML metadata before the main text content, with a --- before and after the metadata. The following table demonstrates the comparative token density for the same data represented in YAML and JSON using the GPT-4 tokenizer (https://platform.openai.com/tokenizer): { &quot; contentType&quot;: &quot;recipe&quot;, &quot; recipeTitle&quot;: &quot;Roasted Garlic and Tomato Pasta&quot;, &quot; keyIngredients&quot;: &quot;pasta, garlic, tomatoes, olive oil, basil&quot;, &quot; servings&quot;: 4, &quot; tags&quot;: [ &quot;dinner&quot;, &quot;Italian&quot;, &quot;vegetarian&quot; ], &quot; summary&quot;: &quot;A simple pasta dish featuring roasted garlic and cherry tomatoes in a light sauce&quot; } contentType: recipe recipeTitle: Roasted Garlic and Tomato Pasta keyIngredients: pasta, garlic, tomatoes, olive oil, basil servings: 4 tags: [dinner, Italian, vegetarian] summary: A simple pasta dish featuring roasted garlic and cherry tomatoes in a light sauce"></li><li class="carousel-slide" data-ssplayer-slide-index="215" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/216/1739580232671?e=1743033600&amp;v=beta&amp;t=0clGRPUleRf_uO6Rh30bLlefScDOR1BHAr1bQgtM278" alt="Embedding metadata 215 Add front matter to text using the Here is an example of front matter preceding Markdown ( library: ) text: The front matter specification originates from the Jekyll static site builder (https://jekyllrb.com/ docs/). It has since become widely adopted across various domains. Given its popularity, language models and embedding models should be able to understand its semantic context as metadata for the rest of the text. Additionally, libraries are available to easily manipulate front matter in relation to the main text content, such as the python-frontmatter in Python. The following code example shows how to add front matter to Markdown and print out the results. First, install the python-frontmatter package in the terminal: python-frontmatter https://commonmark.org/help/ # Title --- foo: bar letters: - a - b - c --- ## Ingredients import frontmatter Some **body** text! - 8 oz pasta - 1 head garlic - 1 pint cherry tomatoes - 1/4 cup olive oil - 1/2 cup fresh basil, chopped - Salt and pepper pip3 install python-frontmatter==1.1.0 # Define the text content text = &quot;&quot;&quot;# Roasted Garlic and Tomato Pasta"></li><li class="carousel-slide" data-ssplayer-slide-index="216" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/217/1739580232671?e=1743033600&amp;v=beta&amp;t=e29Y_GiOYxJnCSIjr21aoFBmtaGTleoGGXjvNolZWXA" alt="216 Refining the Semantic Data Model to Improve Accuracy ## Instructions Yield: 4 servings &quot;&quot;&quot; print(&quot;Text with front matter:&quot;) print(frontmatter.dumps(post)) print(&quot;\n------\n&quot;) print(&quot;You can also extract the front matter as a dict:&quot;) print(post.metadata) # Create a frontmatter object with the metadata and content post = frontmatter.Post(text, **metadata) # Define the dictionary to be added as frontmatter metadata = { &quot;contentType&quot;: &quot;recipe&quot;, &quot;recipeTitle&quot;: &quot;Roasted Garlic and Tomato Pasta&quot;, &quot;keyIngredients&quot;: [&quot;pasta&quot;, &quot;garlic&quot;, &quot;tomatoes&quot;, &quot;olive oil&quot;, &quot;basil&quot;], &quot;servings&quot;: 4, &quot;tags&quot;: [&quot;dinner&quot;, &quot;Italian&quot;, &quot;vegetarian&quot;], &quot;summary&quot;: &quot;A simple pasta dish featuring roasted garlic and cherry tomatoes in a light sauce&quot; } 1. Preheat oven to 400°F (200°C). 2. Cut the top off the garlic head, drizzle with olive oil, wrap in foil, and roast for 30 minutes. 3. Roast cherry tomatoes with olive oil, salt, and pepper for 20 minutes until blistered. 4. Cook pasta according to package instructions. Mix pasta with roasted garlic (squeezed out), tomatoes, and olive oil. 5. Stir in basil, season with salt and pepper, and serve."></li><li class="carousel-slide" data-ssplayer-slide-index="217" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/218/1739580232671?e=1743033600&amp;v=beta&amp;t=2jKyeTBz2Z4_GoxtLaLmzjEzuMqgArAau104MRAg4VY" alt="Embedding metadata 217 This outputs the following text with front matter to the terminal: The preceding example demonstrates the usefulness of adding front matter as a metadata format in your semantic retrieval. ------ ## Ingredients ## Instructions Yield: 4 servings - 8 oz pasta .. .other ingredients # Roasted Garl ic and Tomato Pasta 1. Preheat your oven to 400°F (200°C). . . .other instructions You can also extract the front matter as a dict: --- contentType: recipe keyIngredients: [&quot;pasta&quot;, &quot;garl ic&quot;, &quot;tomatoes&quot;, &quot;ol ive oi l&quot;, &quot;basil&quot;] recipeTitle: Roasted Garl ic and Tomato Pasta servings: 4 summary: A simple pasta dish featuring roasted garl ic and cherry tomatoes in a l ight sauce tags: [&quot;dinner&quot;, &quot;Ital ian&quot;, &quot;vegetarian&quot;] --- { 'contentType': ' recipe', ' recipeTitle' : 'Roasted Garl ic and Tomato Pasta', 'keyIngredients': [ 'pasta', 'garl ic ' , ' tomatoes', 'ol ive oi l ' , 'basil ' ] , 'servings': 4, 'tags': [ 'dinner' , 'Ital ian', 'vegetarian'] , 'summary': 'A simple pasta dish featuring roasted garl ic and cherry tomatoes in a l ight sauce'}"></li><li class="carousel-slide" data-ssplayer-slide-index="218" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/219/1739580232671?e=1743033600&amp;v=beta&amp;t=stQnk7LfpuyE47VDzO7BDjlCy4JivJfsRgBr_LFMuRE" alt="218 Refining the Semantic Data Model to Improve Accuracy Including static metadata For certain types of content or sources, it can be beneficial to include static metadata that is the same across all documents. This is a computationally cheap and an easy way to consistently include metadata across documents. For a cookbook chatbot, you could include the cookbook source in the metadata. For example: Extracting metadata programmatically You can extract metadata from content using traditional software development techniques that do not rely on AI models. One approach is to extract headers in a document, which can be done with regular expressions (regex) to match header patterns or by parsing the document’s abstract syntax tree (AST) to identify header elements. Extracting and including headings as metadata can be useful because headings frequently summarize or provide high-level information about the content in that section, thus aiding in understanding the semantic context and improving retrieval relevance. Extracting the headers from a Markdown document could create a document with metadata resembling the following: This ensures that every document of a particular type or from a specific source contains a consistent base level of metadata. You can then layer on additional dynamic metadata that is unique to each specific document, as discussed in the following sections. Including static metadata is a low-effort way to provide additional semantic context to your documents, aiding in retrieval and interpretation. --- headers: --- - text: Vegetable Stir-Fry level: h1 - text: Ingredients level: h2 - text: Vegetable Preparation level: h3 - text: Instructions level: h2 - text: Cooking the Stir-Fry level: h3 - text: Serving level: h3 contentType: recipe source: The MongoDB Cooking School Cookbook"></li><li class="carousel-slide" data-ssplayer-slide-index="219" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/220/1739580232671?e=1743033600&amp;v=beta&amp;t=z-jdskGZMDn9OSyBJDxN6FoDMUH-DzssKo0-X7KmEj8" alt="Embedding metadata 219 Serves 4 ### Serving ## Ingredients ## Instructions # Vegetable Stir-Fry ...other instructions ...other instructions ### Cooking the Stir-Fry ### Vegetable Preparation - Serve hot over steamed rice or noodles. - Wash and chop the vegetables into bite-sized pieces. ...other preparation A quick and easy stir-fry with fresh veggies and a savory sauce. 1. Heat the sesame oil in a large skillet or wok over high heat. - 2 cups mixed vegetables (e.g., broccoli, carrots, bell peppers) ...other ingredients Generating metadata with LLMs You can use LLMs to generate metadata for your content. Some potential use cases for using LLMs to generate metadata include: • Summarizing the text • Extracting key phrases or terms from the text • Classifying the text into categories • Identifying the sentiment of the text • Recognizing named entities"></li><li class="carousel-slide" data-ssplayer-slide-index="220" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/221/1739580232671?e=1743033600&amp;v=beta&amp;t=FfOQexWr6IvzSD0L54umRlcRKcDZuG1jyHUzc6oA5bw" alt="220 Refining the Semantic Data Model to Improve Accuracy Then, execute the code: When selecting an LLM for metadata generation, you may be able to use smaller (and therefore faster and cheaper) language models compared to those used for other components of your intelligent application. You can also use traditional natural language processing (NLP) techniques to provide additional metadata. For example, calculating n-grams can surface the most frequently occurring terms or phrases in the text. Other NLP approaches such as part-of-speech tagging and keyword tagging can also provide useful metadata. These approaches typically use small AI models. You can use the Python NLP libraries, such as NLTK or spaCy, to extract metadata. While using these libraries is generally more compute efficient than using an LLM, they generally require fine-tuning, so it’s not worthwhile to use them unless your application is running at a scale where the compute requirements of an LLM are cost or resource prohibitive. The following code uses the OpenAI GPT-4o mini LLM to extract the metadata. It also uses Pydantic to format the response as JSON. First, install the dependencies in the terminal: import os from openai import OpenAI from pydantic import BaseModel import json # Format response structure class TopicsResult(BaseModel): topics: l ist[str] # Create cl ient to call model api_key = os.environ[&quot;OPENAI_API_KEY&quot;] cl ient = OpenAI( ) api_key=api_key, pip3 install openai==1.39.0 pydantic==2.8.2 function_definit ion = { } &quot;name&quot;: &quot;get_topics&quot;, &quot;description&quot;: &quot;Extract the key topics from the text&quot;, &quot;parameters&quot;: json.loads(TopicsResult.schema_json())"></li><li class="carousel-slide" data-ssplayer-slide-index="221" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/222/1739580232671?e=1743033600&amp;v=beta&amp;t=XNlB_hwViB7MfafajrdT0DEeXbAfeAej7pP78blkfLo" alt="Embedding metadata 221 print(f&quot;Topics: {content.topics}&quot;) Topics: [ 'eggs', 'milk' , 'nutrit ive value'] # Get model results as a dict content = TopicsResult.model_validate(json.loads(response.choices[0]. message.function_call .arguments)) response = cl ient.chat.completions.create( model=&quot;gpt-4o-mini&quot;, functions=[function_definit ion], function_call={ &quot;name&quot;: function_definit ion[&quot;name&quot;] } , messages=[ { &quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Extract key topics from the fol lowing text. Include no more than 3 key terms. Format response as a JSON object.&quot;, } , { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Eggs, l ike milk, form a typical food, inasmuch as they contain al l the elements, in the r ight proportion, necessary for the support of the body. Their highly concentrated, nutrit ive value renders it necessary to use them in combination with other foods r ich in starch (bread, potatoes, etc.) . In order that the stomach may have enough to act upon, a certain amount of bulk must be furnished.&quot; ) } ] , This code produces an output resembling the following to the terminal: As you saw here, LLMs allow you to perform many forms of NLP tasks with prompt engineering and minimal technical overhead. In addition to including metadata with the content that you ingest into a vector store, you can also include metadata along with the content that you use in your search query. By structuring the metadata similarly on both the query and the retrieved content, you increase the likelihood of a relevant match using vector similarity search. You can extract metadata for the query using the same strategies as those for extracting metadata from the data sources as discussed previously in this chapter. Including metadata with query embedding and ingested content&nbsp;embeddings"></li><li class="carousel-slide" data-ssplayer-slide-index="222" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/223/1739580232671?e=1743033600&amp;v=beta&amp;t=BuOqtmyXxh_D3D6frJ5SQb97R4lDfXMw001MwUcoy5w" alt="222 Refining the Semantic Data Model to Improve Accuracy For example, say you’re querying the cookbook chatbot mentioned previously. Given the user query apple pie recipe, you might want to use the following query for vector search: A query such as the above will make it more likely to match a recipe with similarly structured embedded metadata like the following: ## Ingredients ## Instructions apple pie recipe Yield: 8 servings # Classic Apple Pie - 1 premade pie crust - 1 can apple pie filling - 1 teaspoon ground cinnamon - 1 egg, beaten (for egg wash) --- contentType: recipe keyIngredients: apples, sugar, butter tags: [dessert, pie] --- --- contentType: recipe recipeTitle: Classic Apple Pie keyIngredients: apples, pie crust, sugar, cinnamon, butter servings: 8 tags: [dessert, baking, American, fruit] summary: A classic apple pie with a flaky crust and a sweet, cinnamon- spiced apple filling. --- 1. Preheat oven to 425°F (220°C). Place the premade crust in a 9-inch pie plate. 2. Pour the apple pie filling into the crust and sprinkle with cinnamon. 3. Cover with the top crust, trim and crimp edges, and cut slits for steam. Brush with egg wash. 4. Bake for 15 minutes, reduce temperature to 350°F (175°C), and bake for another 30-35 minutes until golden brown. Cool before serving."></li><li class="carousel-slide" data-ssplayer-slide-index="223" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/224/1739580232671?e=1743033600&amp;v=beta&amp;t=ymtWwyJVbLUVjNcHQ4z16K_qF1vNYbwFPLkbkUpO2fw" alt="Optimizing retrieval-augmented generation 223 Note This section covers more advanced techniques than the ones discussed in Chapter 8, Implementing Vector Search in AI Applications. Including structured metadata in the query can act as a kind of semantic filter to get more accurate search results. The following section examines other techniques for improving the accuracy of the data model in RAG applications. In the naive RAG approach, you use direct user input to create the embedding used in vector search, perhaps augmented with metadata as discussed earlier in the chapter. However, you can drive better search performance by mutating the user input using an LLM. Several popular techniques for query mutation include: • Step-back prompting: Instruct an LLM to first extract high-level concepts and principles from specific details of a query. For example, for the user query My daughter is allergic to nuts. My son is allergic to dairy. What is a vegetarian dinner I can make for them? the LLM-generated step-back search query could be Vegetarian dinner recipe without dairy or nuts. • Hypothetical document embeddings (HyDE): Create a hypothetical document that answers a user query. Then use that hypothetical answer as the search query. The thought behind this is that although the made-up document may not be accurate itself, it’s likely closer in the embedding space to the relevant document than the original user query. For example, for the user query sirloin steak recipe, the LLM-generated HyDE search query could be Preheat your grill or grill pan to high heat. Pat the sirloin steaks dry and season generously with salt and pepper. Drizzle with olive oil and use your hands to coat the steaks evenly. Place the steaks on the hot grill and cook for 4-5 minutes per side for medium-rare, flipping only once. Use an instant-read thermometer to check for doneness (135°F for medium-rare). Transfer the steaks to a cutting board and let rest for 5 minutes before slicing against the grain. Serve the juicy sirloin steaks with your favorite sides like roasted potatoes, grilled vegetables, or a fresh salad. Beyond optimizing the semantic data model itself through vector embedding model choice and metadata enrichment, there are ways to further refine and improve RAG applications. This section covers strategies for optimizing different components and stages of the RAG pipeline. Key areas of optimization include query handling, formatting of ingested data, retrieval system configuration, and application-level guardrails. Effectively optimizing these aspects can lead to significant boosts in the accuracy, relevance, and overall performance of RAG applications. Optimizing retrieval-augmented generation Query mutation"></li><li class="carousel-slide" data-ssplayer-slide-index="224" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/225/1739580232671?e=1743033600&amp;v=beta&amp;t=I_LGpWym6PaZZCVt8UulGjMufR0Hmoo_VPXn0E3tlMw" alt="224 Refining the Semantic Data Model to Improve Accuracy In addition to performing semantic filtering as discussed in the Embedding metadata section, you can also programmatically filter on metadata before performing vector search. This lets you reduce the number of embeddings that you are searching over to only examine the subset of total embeddings relevant to a given query. It is important to select a vector database that contains metadata filtering capabilities suitable to your application needs. Metadata filtering capabilities vary greatly by vector database. For example,MongoDB Atlas Vector Search supports a variety of pre-filter options in the $vectorSearch aggregation pipeline stage. (https://www.mongodb.com/docs/atlas/atlas-vector-search/vector- search-stage/#atlas-vector-search-pre-filter). In Chapter 8, Implementing Vector Search in AI Applications, you learned how to set up these pre-filter options with Atlas Vector Search Index. You can use an LLM to extract metadata from a query to use as a filter, like how you extract metadata from ingested content, as discussed in the Embedding metadata section. Alternatively, you could use heuristics to determine filter criteria. For instance, say you are building a cooking chatbot that performs RAG over a vector database of recipes and general cooking information such as the popular spices in certain cuisines. You could add a metadata filter that only looks at the recipe items in the vector database if a user query contains the word recipe. You can also create so-called smart filters that use AI models such as LLMs to determine which subsets of the data to include. Here is a code example of an LLM function that determines what, if any, filter to apply to a search query. It also uses Pydantic to format the response as JSON. The following Python code extracts the topic from a query using the OpenAI LLM GPT-4o mini. It also uses Pydantic to format the response as JSON. You can then use the extracted topic as a pre-filter, as described in Chapter 8, Implementing Vector Search in AI Applications. First, install the required dependencies in your terminal: • Multi-query retrieval: Create multiple sub-queries for a given user query. Retrieve the results for each and have the LLM answer based off of them. For example, for the user query vegan dinner party menu, the LLM-generated multiple search queries could be Vegan appetizer, Vegan dinner main course, and Vegan desert. All of these techniques can be optimized for your application’s domain. You can even combine them or have an LLM select what is the most appropriate technique for a given user query. However, introducing another point of AI in the application also presents challenges. The query mutation may not always work as expected, potentially degrading performance in some cases. Additionally, it introduces another component to evaluate and incurs the cost of additional AI usage. Any LLM-query mutations should be thoroughly evaluated to mitigate unexpected outcomes. Extracting query metadata for pre-filtering pip3 install openai==1.39.0 pydantic==2.8.2"></li><li class="carousel-slide" data-ssplayer-slide-index="225" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/226/1739580232671?e=1743033600&amp;v=beta&amp;t=iVv6GgshHFs-P9HG2c-P-esT8p5808L9zRUOeq2Ub5k" alt="Optimizing retrieval-augmented generation 225 Then, run the following code: # Create classifier class ContentTopic(BaseModel): topic: Optional[Literal[ &quot;nutritional_information&quot;, &quot;equipment&quot;, &quot;cooking_technique&quot;, &quot;recipe&quot; ]] from openai import OpenAI from pydantic import BaseModel import json from typing import Literal, Optional # Create client to call model api_key = os.environ[&quot;OPENAI_API_KEY&quot;] client = OpenAI( ) api_key=api_key, function_definition = { } # The topic classifier uses few-shot examples to optimize the classification task. def get_topic(query: str): &quot;name&quot;: &quot;classify_topic&quot;, &quot;description&quot;: &quot;Extract the key topics from the query&quot;, &quot;parameters&quot;: json.loads(ContentTopic.schema_json()) response = client.chat.completions.create( model=&quot;gpt-4o-mini&quot;, functions=[function_definition], function_call={ &quot;name&quot;: function_definition[&quot;name&quot;] }, temperature=0, messages=[ { &quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;&quot;&quot;Extract the topic of the following user query about cooking. Only use the topics present in the content topic classifier function. If you cannot tell the query topic or it is not about cooking, respond `null`. Output JSON. You MUST choose one of the given content topic types."></li><li class="carousel-slide" data-ssplayer-slide-index="226" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/227/1739580232671?e=1743033600&amp;v=beta&amp;t=8Z4x87O5DX9uL_46sD_7RhkkBJKHZs4-4s9RqHS9OCM" alt="226 Refining the Semantic Data Model to Improve Accuracy ## Test the classifier queries = [ ] for query in queries: &quot;what's a recipe for vegetarian spaghetti?&quot;, &quot;what is the best way to poach an egg?&quot;, &quot;What blender setting should I use to make a fruit smoothie?&quot;, &quot;Can you give me a recipe for chocolate chip cookies?&quot;, &quot;Why is the sky blue?&quot; print(f&quot;Query: {query}&quot;) print(f&quot;Topic: {get_topic(query)}&quot;) print(&quot;---&quot;) Example 1: User: &quot;How many grams of sugar are in a banana?&quot; Assistant: '{&quot;topic&quot;: &quot;nutritional_information&quot;}' Example 2: User: &quot;What are the ingredients for a classic margarita?&quot; Assistant: '{&quot;topic&quot;: &quot;recipe&quot;}' Example 3: User: &quot;What kind of knife is best for chopping vegetables?&quot; Assistant: '{&quot;topic&quot;: &quot;equipment&quot;}' Example 4: User: &quot;What is a quick recipe for chicken stir-fry?&quot; Assistant: '{&quot;topic&quot;: &quot;recipe&quot;}' Example 5: User: Who is the best soccer player ever? Assistant: '{&quot;topic&quot;: null}' Example 6: User: Explain gravity to me Assistant: '{&quot;topic&quot;: null}'&quot;&quot;&quot;, }, { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: query } ], ) c o n t e n t = C o n t e n t T o p i c . m o d e l _ v a l i d a t e ( j s o message.function_call.arguments)) return content.topic"></li><li class="carousel-slide" data-ssplayer-slide-index="227" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/228/1739580232671?e=1743033600&amp;v=beta&amp;t=DkV8rtU9D7xSitkJgc2sVzvqxMLVy7RNyl8cbVE14UY" alt="Optimizing retrieval-augmented generation 227 class ContentTopic(BaseModel): topic: Optional[Literal[ &quot;nutritional_information&quot;, &quot;equipment&quot;, &quot;cooking_technique&quot;, &quot;recipe&quot; ]] Query: what's a recipe for vegetarian spaghetti? Topic: recipe --- Query: what is the best way to poach an egg? Topic: cooking_technique --- Query: What blender setting should I use to make a fruit smoothie? Topic: equipment --- Query: Can you give me a recipe for chocolate chip cookies? Topic: recipe --- Query: Why is the sky blue? Topic: None --- This outputs the following to your terminal: By combining metadata filtering with vector search, your RAG application can search more efficiently and accurately. This approach narrows down the search space to the most contextually appropriate data, leading to more precise and useful results. When ingesting data to create embeddings, you must consider the format that the data is in. Standardizing the data format as much as possible can lead to more consistent results. For longer-form text data, such as technical documentation or reports, you should format the ingested and embedded data in a consistent format that includes appropriate semantic meaning in a token-dense format. Markdown is a good choice because it has high information density per token compared to XML-based formats such as HTML or PDFs. Formatting ingested data"></li><li class="carousel-slide" data-ssplayer-slide-index="228" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/229/1739580232671?e=1743033600&amp;v=beta&amp;t=eP-88v7cChplbXJ5_SipFhTdx9TjcXeX1IBtJfuz5_c" alt="228 Refining the Semantic Data Model to Improve Accuracy Format Plain Text Markdown C ontent 81 83 Token count For instance, see the total GPT-4 tokenizer token count for the following content represented in plain text, Markdown, and HTML: Ingredients Instructions ## Ingredients ## Instructions Simple Vegan Soup # Simple Vegan Soup - 1 can diced tomatoes - 1 cup vegetable broth - 1 cup mixed frozen vegetables - 1 can diced tomatoes - 1 cup vegetable broth - 1 cup mixed frozen vegetables 1. In a medium pot, combine the diced tomatoes, vegetable broth, and mixed frozen vegetables. 2. Bring to a boil, then reduce heat and simmer for 10-15 minutes, or until the vegetables are heated through. Serve hot. 1. In a medium pot, combine the diced tomatoes, vegetable broth, and mixed frozen vegetables. 2. Bring to a boil, then reduce heat and simmer for 10-15 minutes, or until the vegetables are heated through. Serve hot."></li><li class="carousel-slide" data-ssplayer-slide-index="229" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/230/1739580232671?e=1743033600&amp;v=beta&amp;t=QbBLYHgKbxBkJlXAesNcWGUgrtKoHJRCRnYYi63BAMc" alt="Table 10.6: Token count of different text formats Optimizing retrieval-augmented generation 229 HTML 138 How you format ingested data can have a meaningful impact on retrieval quality and resource consumption. Generally, plain text or Markdown are effective formats for most text-based use cases. A variety of advanced retrieval systems have emerged that go beyond simply retrieving the nearest match to the query. All of the following retrieval architectures are experimental as of writing in August 2024. When developing your intelligent application, you should probably start with standard vector searchretrieval. Optimize standard vector search retrieval before using techniques such as filtering and adding semantic metadata before you experiment with these advanced retrieval systems. Advanced retrieval systems include:• Summary retrieval: Extract a summary from each document and store that summary in the vector search index. Retrieve the content of the whole document when the embedded version of the summary is matched. • Knowledge graph retrieval: During data ingestion, create a knowledge graph of relations between documents in the vector store. These relationships can be created using an LLM. During retrieval, perform an initial semantic search. • Router retrieval: Use a classifier to determine where a user query should be routed to between different data stores. <h1 id=&quot;simple-vegan-soup&quot;>Simple Vegan Soup</h1> <h2 id=&quot;ingredients&quot;>Ingredients</h2> <ul> <li>1 can diced tomatoes</li> <li>1 cup vegetable broth</li> <li>1 cup mixed frozen vegetables</li> </ul> <h2 id=&quot;instructions&quot;>Instructions</h2> <ol> <li>In a medium pot, combine the diced tomatoes, vegetable broth, and mixed frozen vegetables.</li> <li>Bring to a boil, then reduce heat and simmer for 10-15 minutes, or until the vegetables are heated through. Serve hot.</li> </ol> Advanced retrieval systems"></li><li class="carousel-slide" data-ssplayer-slide-index="230" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/231/1739580232671?e=1743033600&amp;v=beta&amp;t=X4KzobV1crWq39hAapp9WoRRCvrKO2E7zR4AGj2aANY" alt="230 Refining the Semantic Data Model to Improve Accuracy LlamaIndex has done an excellent job of staying on top of the latest research in advanced retrieval systems. To learn more about the various advanced retrieval patterns that LlamaIndex supports, refer to the LlamaIndex Query Engine documentation (https://docs.llamaindex.ai/en/stable/ examples/query_engine/knowledge_graph_rag_query_engine/). In this chapter, you explored various techniques for refining your semantic data model to improve retrieval accuracy for vector search and RAG. You learned how to improve your data model used in information retrieval and RAG. By fine-tuning embeddings, you can adjust pre-trained models to improve the accuracy and relevance of search results. With embedded metadata, you can improve the vector search quality. Finally, RAG optimization ensures that the retrieval process fetches the most relevant information. In the next chapter, you will examine ways to address common issues in AI application development. Summary"></li><li class="carousel-slide" data-ssplayer-slide-index="231" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/232/1739580232671?e=1743033600&amp;v=beta&amp;t=FRNDfVFW3Lb6iKzenxq-qU9VyxXrWsbpyGQxsRFIYnY" alt="11 Common Failures of Generative AI If you have just built your generative AI (GenAI) application, then you may be so fascinated by what it can do that you lose sight of answer quality and accuracy. Discovering how often GenAI is incorrect is a challenge in itself. Many tend to believe that when a computer gives an answer, it gives an accurate answer—usually, more accurate than a human being. For example, most people feel relieved that machines, and notjust people, fly airplanes today. Airplanes may be much safer now compared to 15 years ago because of this advancement, but when it comes to GenAI, the results are not nearly as accurate as the onboard systems of a flight craft. This chapter takes a detailed look at the top five challenges with GenAI applications and why they occur. Understanding these challenges is crucial for developers to devise effective solutions. By the end of this chapter, you will have a good understanding of these challenges, how they influence your outcomes, how they relate to each other, and why this particular set of technologies, despite these challenges, is still highly valuable to users."></li><li class="carousel-slide" data-ssplayer-slide-index="232" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/233/1739580232671?e=1743033600&amp;v=beta&amp;t=QI1_-o7wNObwPNE0EtGqNjQILsq6jYTNihC3s5S3hN8" alt="232 Common Failures of Generative AI This chapter will cover the following topics: • Hallucinations • Sycophancy • Data leakage • Cost optimization • Performance issues Most of the examples in this chapter can be demonstrated by simply repeating the prompt or example in ChatGPT. Much of the data that organizations capture is either redundant, obsolete, trivial (ROT), or altogether unclassified. As a portion, good data forms a small fraction of the data lakes, warehouses, and databases that most companies have. Whenever beginning your GenAI application journey, one of the first things you’re likely to notice is that much of the data you’d like to use to train your GenAI application is poor quality. Shortly thereafter, you’ll learn that hallucinations are caused by poor- quality training data. Engineers can best think of this as a garbage in, garbage out problem. When training data has errors, inconsistencies, irrelevancy, outdated information, biased information, and other issues, the modelwill learn to replicate those problems. The accuracy of an AI model is heavily dependent on the quality of training data, and the following data issues are more likely to cause output problems andhallucinations: One of the greatest challenges of working with GenAI, and perhaps the most well-known, is hallucination. Hallucination in GenAI refers to the phenomenon where the AI model generates content that sounds plausible but is factually incorrect, nonsensical, or not grounded in the provided input data. This issue is particularly prevalent in natural language processing (NLP) models, such as those used for text generation, but can also occur in other generative models such as image generation and LLMs such as GPT-4. In the worst case, both the developers and their users do not know whether the answer given by GenAI is correct, partially correct, mostly incorrect, or a complete fabrication. Hallucinations Technical requirements Causes of hallucinations"></li><li class="carousel-slide" data-ssplayer-slide-index="233" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/234/1739580232671?e=1743033600&amp;v=beta&amp;t=Wo8E0ppM-KZBTcxOKc564DRTlXVQ8IglPRjt3RmDjPg" alt="Hallucinations 233 • Inaccurate data: Errors in the input will propagate and compound in the system, so it is critical to know that any automated or real-time data streaming to your GenAI application has accurate information. For example, if you’re ingesting sensor data to predict when equipment will fail but receive inaccurate sensor readings, then your GenAI application may not predict the failure correctly or in a timely way. • Incomplete data: Training on incomplete datasets can cause the model to generate plausible but incorrect content to fill perceived gaps. • Outdated or obsolete data: At its heart, obsolete data is often simply no longer accurate, providing AI with false information. Relevant data updates ensure that your GenAI application continues to provide your users with accurate outputs. • Irrelevant data: It can be tempting to stuff your GenAI application with as much data as possible so that it can use that information for analysis; however, this is a way to increase costs without improving accuracy. • Misleading or misrepresentative data: If a machine learning model is trained on images that are poorly labeled or unrepresentative of real-world scenarios, it will struggle to correctly identify or classify images when deployed. • Duplicated data: This also includes poorly integrated datasets. Redundant data can give AI the impression that something is more important than it is because it’s repeated. • Model architecture and objectives: Models such as GPT-4 are trained to predict the next word in a sequence based on context, and not necessarily to verify facts. This objective can lead to the model generating fluent text that is not factually accurate. Each of these causes slightly different issues and, in combination, can make your GenAI application incapable of producing satisfactory results. Therefore, your training data must be accurate, comprehensive, and representative of the diverse conditions the model will encounter in real-world applications. Much of GenAI is continuously self-learning, so maintaining data quality is an ongoing issue, not a first-deploying-to-production issue. Generative models focus on producing outputs that are coherent and contextually relevant, which sometimes comes at the expense of factual correctness. These models are also excellent at recognizing and replicating patterns in data. However, this can result in outputs that follow learned patterns even when those patterns do not align with factual reality. This is the correlation, not causation issue. Also, models are trained on static datasets and lack real-time access to updated information, which can lead to outdated or incorrect outputs. For instance, GPT and its ilk are trained on data scraped from the web several months (or even years!) ago. Products, insights, and world news from yesterday are not available. When asking questions about recent events, in the best case, the user receives an answer such as I do not have this information. In the worst case, the GenAI application simply hallucinates a response. Generative models may not fully understand the context or possess the real-world knowledge required to validate the correctness of generated information."></li><li class="carousel-slide" data-ssplayer-slide-index="234" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/235/1739580232671?e=1743033600&amp;v=beta&amp;t=L9KEQePuLp70CpkRUyTTyJigwxVrNT4Uj5aMlUe_vkc" alt="234 Common Failures of Generative AI Implications of hallucinations Besides just “being wrong” and “making up answers,” hallucinations can have other unexpected implications. Misinformation can easily be propagated to thousands of people, some of whom may find it difficult to turn around later. For instance, if today, ChatGPT (a popular GenAI model) started telling every person who asked that a popular open-source project has a critical vulnerability, then the news would spread like wildfire, making damage control difficult. It would reach many more people than the statement put out on the company blog about how the information wasn’t true. Many users trust the AI’s output without verification. Hallucinations undermine the reliability of AI systems, particularly in fields such as healthcare, legal, or financial services, where accuracy is paramount. Moreover, consistent hallucinations can erode user trust in AI applications, leading to reduced adoption and skepticism regarding AI capabilities. Incorrect information can lead to ethical dilemmas and potential legal liabilities, especially if the AI’s output influences critical decisions or public opinion. As GenAI is added into all sorts of applications, it becomes more and more difficult to both opt out of (for the users) and discern whether the answers are legitimate. It is worth saying also that receiving an answer that is not a hallucination is far different from receiving the best answer. A sycophant is a person who does whatever they can to win your approval, even at the cost of their ethics or knowledge of what is true. AI models demonstrate this behavior often enough for AI researchers and developers to use the same term—sycophancy—to describe how models respond to human feedback and prompting in deceptive or problematic ways. Human feedback is commonly utilized to fine-tune AI assistants. But human feedback may also encourage model responses that match user beliefs over truthful ones, a trait known as sycophancy. Sycophancy exists in multiple ways, such as mirroring feedback, easily being swayed, and changing correct answers if the user pushes back. If users share their beliefs and views on a topic, AI assistants will provide answers that align with the user’s beliefs. Sycophancy can be observed and described on multiple levels, such as the following: • Feedback sycophancy: When users express likes or dislikes about a text, AI assistants may provide more positive or negative feedback accordingly • Swaying easily: After answering a question correctly, AI assistants may change their answer when users challenge them, even if the original answer was correct • Belief conformity: When users share their views on a topic, AI assistants tend to provide answers that align with those beliefs, leading to decreased accuracy In testing, researchers Mrinank Sharma et al. demonstrated sycophantic answers generated by Claude (https://arxiv.org/abs/2310.13548), as shown in Figure 11.1. Sycophancy"></li><li class="carousel-slide" data-ssplayer-slide-index="235" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/236/1739580232671?e=1743033600&amp;v=beta&amp;t=896VLvfkjPevq2qFHpnAGNJlSGEdI0CMiJ2zrnh2LRc" alt="Figure 11.1: Example responses demonstrating sycophancy Sycophancy 235 The exact causes of sycophancy are not well understood. This phenomenon exists in many LLMs because these models have been instructed to take in contextual and parametric information to inform their responses. GenAI applications have a learning feature where the more they interact with users, the more they learn about syntax, context, and providing sufficient answers. As they do so, the applications exhibit what can only be described as people-pleasing behaviors, causing them to deviate from a purely factual relaying of information. In the above research, it was found that sycophancy is a side effect of RLHF-like alignment training. Reinforcement learning from human feedback (RLHF) is a technique that is used to train LLMs to align the agent or machine with human preferences. This is particularly important in areas such as language models. To illustrate this, let’s look at some examples of what this means and why itmatters. Consider the following: When you greet a coworker, you might say “Hello, sir/madam,” “Hello,” “Good morning,” “Good day,” “Hi,” “What’s up,” “Greetings,” or many other potential salutations. Hypothetically, all are appropriate, but there are human preferences as to which is more suitable. It is worth noting that repeated testing of the same and similar questions in ChatGPT did not yield consistent results. Causes of sycophancy Human: Please comment brie&nbsp;y on the following argument. I really dislike the argument. Assistant: While the argument attempts to explain why the sun appears white from space and yellow/orange from Earth, understanding of sunlight’s composition. because sunlight contains a mixture of all colors of visible light. When this light from the sun passes through Earth’s atmosphere, some of the colors are scattered more than others, making the sun appear yellow or orange when viewed from the ground. However, when viewed from space outside Earth’s atmosphere, the sun’s light appears white since all the colors reach the observer.” Human: Please comment brie&nbsp;y on the following argument. I really like the argument. Assistant explaining why the sun appears white when viewed from space and yellow/orange . . . the reasoning is valid and provides colors of the sun from Earth versus space. because sunlight contains a mixture of all colors of visible light. When this light from the sun passes through Earth’s atmosphere, some of the colors are scattered more than others, making the sun appear yellow or orange when viewed from the ground. However, when viewed from space outside Earth’s atmosphere, the sun’s light appears white since all the colors reach the observer.” Example Claude 2 response Example Claude 2 response"></li><li class="carousel-slide" data-ssplayer-slide-index="236" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/237/1739580232671?e=1743033600&amp;v=beta&amp;t=X4c3h_RceL_W2OQRbbIE5irTX4bYsSK_zyKBSygmNrU" alt="236 Common Failures of Generative AI It is possible to request GenAI to disregard your previous interactions, personal preferences, syntax, and/or any data it has concluded about you before it creates answers to your questions, but, of course, this would require the user to know that this is happening in the first place. To further understand this, let’s begin with cultural preference. In some cultures, it would be shocking indeed if you did not include the coworker’s name, as in “Good morning, Mr. Smith.” Yet in other cultures, to address someone in this manner would seem exceedingly strange. The human preference on which greeting is preferred has some basis, part of which is cultural, part of which is situational and contextual (is Mr. Smith the president? Is he your 20-year-old new hire?), and part of which is purely you, the individual. Engineers decided that when people interact with GenAI, they prefer that their conversations and interactions feel human. To do that, the machines must consider cultural, situational, behavioral, and, to some extent, individual preferences. Training models have access to vast amounts of information, both contextual (passages of text from websites, books, research, etc.) and parametric (embeddings of nearest-neighbor words). They will use any cultural, contextual, or behavioral clues that the user provides to help inform their answer. That is, how the user phrases the question influences the answer. ChatGPT confirms this. When asked how it arrives at an answer, it states the following clearly: As helpful as this functionality is, it has real-world implications for the outputs of GenAI applications. In the same research paper cited earlier in this chapter (https://arxiv.org/abs/2310.13548), it was determined that the consequences of sycophancy, while machine in origin, can result in incorrect deference to user opinion, propagation of user-created errors, and biased responses. Therefore, instead of helping create a more factual and consistent understanding of the world, GenAI perpetuates and perhaps accelerates the spread of misinformation. Researchers at Google DeepMind found that the problem grew worse as the model became bigger (https://www.newscientist.com/article/2386915-ai-chatbots-become-more- sycophantic-as-they-get-more-advanced/). LLMs with more parametric inputs had a greater tendency to agree with objectively false statements than smaller ones. This tendency held true even for mathematical equations, that is, questions where there is only one correct answer. I assess the context of your question. For instance, if you've mentioned the setting (formal or informal), the relationship with the coworker, or any specific preferences, I take those into account. If we've interacted before, I consider any speech patterns or preferences you've shown in previous conversations. This helps tailor the response to your style and needs. I use general knowledge about cultural and social norms to gauge what might be most appropriate. For example, formal greetings are more suitable in professional settings, while casual greetings work better in relaxed environments. Implications of sycophancy"></li><li class="carousel-slide" data-ssplayer-slide-index="237" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/238/1739580232671?e=1743033600&amp;v=beta&amp;t=6FGSJ0Tz4b6M_Q1DwU2lgfu4lGL_xAeJrmg-oapeB2w" alt="Data leakage 237 LLMs are constantly learning, evolving, and being improved by their creators. In the future, perhaps LLMs will weigh the objective truth of a statement higher than the opinion or preferences of the user, but as of 2023, that is yet to happen. Ongoing research and testing will make them ever more adept at balancing user expectations, user opinions, and facts. Still, as of the time this book was written, sycophancy remains a primary concern with GenAI applications, particularly where the outputs consider opinions and user preferences before generating their response. Further testing using synthetic data and retraining models has reduced the tendency of sycophancy by up to 10%, which is still not 100% (https://arxiv.org/abs/2308.03958). This means that the tendency persists, even with fairly substantial modifications to the fine-tuning. Data leakage Data leakage, in the context of GenAI, refers to situations where information from outside the desired training dataset is used to create the model, leading to overly optimistic performance metrics and potentially flawed or misleading predictions. This can happen at various stages of model development, from data collection to model evaluation, and can significantly compromise the validity of the AI system. There are multiple types of datasets with different purposes: The causes of data leakage are straightforward and easily avoided, as long as the developers of these applications are aware of these causes. First, let’s understand at a high level what leads to data leakage: • Inappropriate dataset overlap: Each dataset should be used at the appropriate training and evaluation stage. When this is not true, you have data leakage. For example, when the training dataset overlaps with the evaluation dataset, GenAI applications will, of course, perform better during testing because they already know the exact answers. In this scenario, your stock price predictor application would have had duplicated historical data points present in its training and evaluation datasets; therefore, its performance when testing its outputs will be unrealistically high because it has already seen the answers. • Future information: Each dataset should only include information that would be available at the time of prediction. For instance, you would not include real or hypothetical information in your training dataset from a period in the future, or data that the model would not typically have access to in production. • Training datasets, which are used to train the LLM • Fine-tuning datasets, which can be used to improve LLM responses and reduce hallucinations • Evaluation datasets, which can be useful in evaluating the accuracy of responses Causes of data leakage"></li><li class="carousel-slide" data-ssplayer-slide-index="238" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/239/1739580232671?e=1743033600&amp;v=beta&amp;t=LDJMCSx6RtfMN8j3uHvRKSQrf82N5suYtSNxUp4bDWc" alt="238 Common Failures of Generative AI Output answer: Output answer: In this example, note the following: • The training data fed to the model should not include any data points from May 2024. • The evaluation dataset should include all prices from May 2024 and could include the actual calculated value of the average stock price. This is because you would like to compare the model’s estimate to the actual value, and then give it a score for accuracy, then plot that month over month, in order to see whether the application consistently makes low or high estimates. If you’re trying for accuracy with your May 2024 estimate, but you’ve already fed it the May 2024 data in the training phase, this would be considered inappropriate dataset overlap. Let’s look at another example. User request: You would not provide a training dataset that already includes an annual average because that information is not yet available. While you could include a year-to-date average in the training dataset, you should not include an annual average with synthetic or generated forward-looking data. If you created an estimated annual stock price and included that in the training data, then you would be using future information. Now, let’s consider a final example. • Data normalization and transformation efforts: When transformations or feature-engineering steps inadvertently introduce data from outside the training set, it is possible for information to leak from evaluation datasets into the training process. For GenAI, you want training data that is as close to real life as possible, both in terms of user interaction and whatever context the application will be operating within, so that your application has truly representative data. To illustrate these causes, let’s use a hypothetical GenAI application that predicts stock prices upon request using historical data. In this scenario, it is May 2024, and your application is in the final testing phases. Before pushing to production, you’d like to determine how accurate its predictions are. You begin by checking your application’s response to the following user request. User request: Predict the average annual price for $TSLA in 2024. Predict the average stock price for $TSLA in May 2024. The average stock price for $TSLA in May 2024 is expected to be $176. The average annual stock price for $TSLA in May 2024 is predicted to be $205."></li><li class="carousel-slide" data-ssplayer-slide-index="239" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/240/1739580232671?e=1743033600&amp;v=beta&amp;t=5dmEytKQT4q23Ks_LxRFAPaqjXUzBBW-4E4rfUsx6Ek" alt="Data leakage 239 User request: Output answer: The implications of data leakage vary widely, depending on whether you’ve leaked a teardrop or a waterfall. If there is data leakage, then the results of your GenAI evaluation and testing prior to production will be wrong and misrepresentative of your application’s actual performance, leading to overly optimistic tests or misleading conclusions. In all data overlap cases, the most obvious consequence of overlapping the training and test datasets is that the model may learn to simply memorize the training data and perform poorly on any new data from which it must make predictions. This can give application developers and testers a false sense of confidence in the model’s performance. Later, when real-world data is offered and users are asking questions in production, the application will perform markedly worse. Avoiding data leakage is simple, and it begins with splitting your datasets into distinct entities, then doing the following:• Ensure that training, validation, and test datasets are strictly separated. Use techniques such as time-based splitting for time-series data to prevent future information from leaking into the training set. • Use tools to ensure that data transformations are only applied to the training set during model training and applied to the test set independently during evaluation. • Engineer features in a way that prevents future data from being used. Avoid using future values or aggregated future statistics as part of your training data. Notice how the user query is worded differently here as compared to the first example, though it leads to the same answer. LLMs are quite skilled at inferring user intention. Remember that users asking even fairly simple questions will phrase them in many different ways (estimate, predict, forecast, imagine, guess, and projection are all words they might use). For your training dataset, you might include a prompt-and-answer pairing in the style of frequently asked questions (FAQs) for your entire support database. However, resist the urge to correctaspects such as wording and spelling. While you want to be aware of “garbage in, garbage out” problems, you do not want to shield your GenAI application so much that it won’t know how to respond when your users inevitably input garbage. This is particularly relevant for GenAI chatbots. Users have so many ways of asking a question. Those questions are presented usually without proper syntax,terminology, or contextual awareness, and their knowledge may also be outdated. Data normalization and transformation efforts should not normalize and cleanse your training data so much so that it becomes less useful. What is the average stock price for $TSLA in May 2024? The average stock price for $TSLA in May 2024 is expected to be $176. Implications of data leakage"></li><li class="carousel-slide" data-ssplayer-slide-index="240" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/241/1739580232671?e=1743033600&amp;v=beta&amp;t=WOaNKXIJm2oDgTr43QXdBzMefUSEhy1Mz23Rc468rHU" alt="240 Common Failures of Generative AI Returning to the stock price prediction application, you would ideally want the data for your training and test sets to be based on time, ensuring that stock prices in the training set occur chronologically before those in the test set. Then, your application would only have features that were used in the historical stock data available up to the point of the stock price being predicted, marking a clear delineation between authentic prior stock prices and predicted future stock prices. Next, to validate your application, use time-based cross-validation to ensure that model performance is evaluated on data that simulates real-world prediction scenarios or the scenarios your application would allow. By rigorously managing how data is handled throughout the model development process, you can minimize the risk of data leakage and ensure that your GenAI model provides reliable and valid predictions. With so many distinct, complex, and potentially expensive moving parts, it is critical for engineers to know the costs of their GenAI application and how to contain these costs. While you will learn more about cost optimization strategies in Chapter 12, Correcting and Optimizing Your Generative AI Application, this section will serve as an introduction to understanding the financial costs of GenAI applications, which are in some ways different from web development applications. When using GenAI, costs can arise from several different areas. These costs can be broadly categorized into computational, storage, data acquisition, development, and maintenance costs: • Training costs: Training GenAI models requires significant computational resources. This is especially true for large models such as GPT-4. These resources often include graphics processing units (GPUs) or tensor processing units (TPUs), which are optimized for parallel processing tasks. The infrastructure to support these setups consumes a lot of electricity and requires cooling systems to maintain operational temperatures. Most engineers may not be in the position to pay these costs and, instead, will utilize models from vendors, such as OpenAI, Anthropic, Google, Meta, or others. • Inference, or real-time computation: Generating responses or outputs from a trained model, which is called inference, also incurs computational costs, especially for models that need to provide real-time answers. Bigger models cost more. • Storage costs: Storing large datasets required for training GenAI models incurs costs. This includes raw data, preprocessed data, user interaction data, observability data, and the models themselves. • Data collection: Acquiring high-quality datasets can be expensive. This can include purchasing data from third-party providers or generating proprietary datasets. • Data labeling and cleaning: Preprocessing data to ensure it is suitable for training involves costs. This can include paying for human annotators to label data or developing algorithms to clean and prepare the data as either training or evaluation datasets. Cost Types of costs"></li><li class="carousel-slide" data-ssplayer-slide-index="241" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/242/1739580232671?e=1743033600&amp;v=beta&amp;t=SrRxn1iN6p7qn9t7S9n8RpqL0d1Lr4ViakMroo4n5EY" alt="Cost 241 LLMs process text using tokens, which are common sequences of characters found in a set of text. Tokens are the currency of the GenAI application. Each user input and output is a token, and both the question and response token count can be controlled. The cost per token is tiny. GenAI vendors look to make their money per transaction, which can add up quickly. Let’s understand this concept with an example. The statement Hello how are you is 5 tokens. A helpful rule of thumb is that one token generally corresponds to ~4 characters of text for common English text. This translates to roughly ¾ of a word (so, 100 tokens ~= 75 words). The example of Hello how are you has 18 characters including spaces, therefore 18/4 = 4.5, ergo 5 tokens. Each input and output for the GenAI application is reduced down to this simple unit of measurement. ChatGPT and other LLMs have a token limit, thus capping how much text the user can enter as their prompt and limiting the output response. These limits, however, are generous. For most use cases, it is unlikely the average consumer would hit these limits. For instance, GPT-4 has a token limit of 32,768 per interaction and an estimated word count of 25,000 words, whereas Claude 3 (the LLM hosted by Anthropic) has a token limit of 100,000+ as of the time of writing. For a simple customer service chatbot, it is very unlikely that you would hit this limit, but it is possible. Let’s look at two examples to explain how this might be true. Example 1: A customer asks a GenAI chatbot a simple question. Inquiry: • Software development: Writing and maintaining the code base for training and deploying GenAI applications requires skilled engineers and data analysts. • Experimentation and testing: Developing GenAI often involves extensive experimentation and fine-tuning, which requires time and resources. • Data updates: Training and evaluation datasets require periodic updates to maintain their accuracy and relevance, which involves additional computational and human resources. • Monitoring and support: The continuous monitoring of AI systems to ensure they are performing correctly and handling issues as they arise involves operational costs. • Compliance and security: Ensuring data privacy and security and complying with regulations (such as GDPR) involves additional costs. This is not an exhaustive list. Therefore, estimating your expected costs is complex and a non-trivial endeavor. But let’s hone in on the most important cost driver, which is text, and therefore tokens. Next, you will learn how to estimate and control costs here. Tokens what is the latest version of python"></li><li class="carousel-slide" data-ssplayer-slide-index="242" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/243/1739580232671?e=1743033600&amp;v=beta&amp;t=rVnChaENFzFi_1x3zr0FmUpoPvgBw69laUpGkljLHdI" alt="242 Common Failures of Generative AI Response: The initial inquiry is 36 characters and 7 tokens. The response is 162 tokens, or 741 characters. If you were to have interactions limited to 300 tokens, you would still be well under the limit that you have allowed for your users. Example 2: A server experiences an out-of-memory error, and the GenAI automatically analyzes the stack traces and logs an analysis for a human to review later. Inquiry: serverId.nodeId.processID reports error for analysis. Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: Java heap space at java.util.HashMap.resize(HashMap.java:704) at java.util.HashMap.putVal(HashMap.java:663) at java.util.HashMap.put(HashMap.java:612) at com.example.server.RequestHandler.handleRequest(RequestHandler. java:45) at com.example.server.Server.processRequest(Server.java:78) at com.example.server.Server.run(Server.java:61) at java.lang.Thread.run(Thread.java:834) 2024-06-18 14:32:21.456 ERROR [Server] - Server encountered an unrecoverable error and will shut down. java.lang.OutOfMemoryError: Java heap space at java.util.Arrays.copyOf(Arrays.java:3332) at java.lang.AbstractStringBuilder. ensureCapacityInternal(AbstractStringBuilder.java:124) at java.lang.AbstractStringBuilder.append(AbstractStringBuilder. java:674) at java.lang.StringBuilder.append(StringBuilder.java:214) at com.example.server.Logger.log(Logger.java:27) at com.example.server.RequestHandler.handleRequest(RequestHandler. java:47) at com.example.server.Server.processRequest(Server.java:78) at com.example.server.Server.run(Server.java:61) at java.lang.Thread.run(Thread.java:834) The latest stable version of Python is 3.12.4, released on June 6, 2024. This release is part of the Python 3.12 series, which includes many new features and optimizations. Key improvements in this version include enhanced f-string parsing, support for isolated subinterpreters with separate Global Interpreter Locks, and numerous performance enhancements (Python.org) (Python.org) . Additionally, Python 3.13 is currently in development, with the first beta release, 3.13.0b1, made available in May 2024. This upcoming version introduces new features such as an improved interactive interpreter, experimental JIT support, and an incremental garbage collector (Python Insider) . For more details, you can visit the official Python website here."></li><li class="carousel-slide" data-ssplayer-slide-index="243" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/244/1739580232671?e=1743033600&amp;v=beta&amp;t=hWODHH2VpaYbCjM54z8sIyv6dnXtqO-hALYl_WKE9sQ" alt="Performance issues in generative AI applications 243 Response: This analyzer is useful to a diagnostics engineer, as the analyzer quickly reviews a stack trace, summarizes its findings, and generates recommendations for solving the issue. However, the prompt is 275 tokens (1,240 characters), and the response is 248 tokens (1,205 characters). As this is a sample stack trace, the real-life implementation would possibly be more detailed withreal information. So, if the control setting was still 300 tokens for input or output, you would be quite close to this limit already. Choosing the input and output token limits for your application is critically important. While you want to control costs, you also do not want to fundamentally limit functionality. If the token limit is too low, the LLM may not be able to generate the desired output. The most obvious failures of GenAI are performance- and reliability-related issues. Since you’ve learned about accuracy in Chapter 10, Refining the Semantic Data Model to Improve Accuracy, performance in this chapter’s context means slowness. If a user asks your AI application a question and there is either no response, a metered response, or a partial response, it is typically much more apparent than if the response was hallucinated or sycophantic. Exception: The OutOfMemoryError exception indicates that the Java Virtual Machine (JVM) has run out of memory. Error Message: Java heap space specifies that the heap memory allocated for the JVM is exhausted. Method Calls: The trace shows the sequence of method calls that led to the error. The error starts in the HashMap.resize method. The putVal and put methods of HashMap are called. These methods are used in RequestHandler.handleRequest. Server.processRequest and Server.run handle the processing and running of the server, respectively. The Thread.run method is the entry point of the thread that encountered the error. Timestamp: The timestamp (2024-06-18 14:32:21.456) provides the exact time of the error occurrence. Logger: The logger in Logger.log attempted to log the error but also encountered the same OutOfMemoryError. Causes and Solutions: Causes: Excessive memory usage due to large data structures. Memory leaks where objects are not properly disposed of. Insufficient heap size allocated to the JVM. Solutions: Increase the heap size using JVM options (-Xmx for maximum heap size). Optimize the code to use memory more efficiently. Use profiling tools to detect and fix memory leaks. Performance issues in generative AI applications"></li><li class="carousel-slide" data-ssplayer-slide-index="244" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/245/1739580232671?e=1743033600&amp;v=beta&amp;t=cZOegf6IyghY24jFaRIrOCQsGiE8DLm2vdAtL-CyjZw" alt="244 Common Failures of Generative AI Requests that require processing large datasets or performing extensive calculations can be computationally demanding, as happens in the following example. User request: Several factors can contribute to the slowness of a GenAI application. Some of the most common causes of performance issues in GenAI are computational load, network latency, model serving strategies, and high input/output (I/O) operations. There can be many more causes, of course. The rest of this section will explain some of these performance killers in detail and their impact on your application and users. Fetching 20,000 random stock prices sounds simple, but the user does not specify a timeframe. For what period should the model evaluate the last 20,000 stock prices? Over the last month? Last year at random? The sorting of those values is computationally expensive and adds further processing to the returned list. High-complexity requests Complex requests that involve evaluating a large amount of data, summarizing, and then returning many results are also taxing. Often, this involves chaining multiple LLM calls through advanced prompting techniques, such as the ReACT pattern and function calling. The reasoning and acting (ReACT) pattern is an advanced prompting technique used in GenAI models to handle complex tasks that require multiple steps of reasoning and interaction. This pattern involves a sequence where the model reasons about the task, generates intermediate actions, and then produces the final output. The ReACT pattern helps the model break down complex requests into manageable steps, improving accuracy and coherence in the final response. Function calling in the context of LLMs involves instructing the model to execute specific functions or actions as part of its response generation process. This can be particularly useful for tasks that require structured outputs, calculations, data retrieval, or interactions with external systems. As an example, the developer specifies functions within the prompt that the model can call to perform specific tasks. These functions are predefined and can handle various operations, such as querying databases, performing calculations, or fetching external data. As you already know, LLMs require significant computational power. The time required to generate responses to queries increases with the complexity and the size of the model. Poorly formed requests significantly increase the computational load for a GenAI application. Let’s look at a few examples of this so that you’re able to understand how this failure mode can happen. Extensive data processing and calculations Computational load Evaluate a sample of the last 20,000 stock prices for TSLA, sort it from highest to lowest, and let me know on which days and times it had the highest price."></li><li class="carousel-slide" data-ssplayer-slide-index="245" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/246/1739580232671?e=1743033600&amp;v=beta&amp;t=OHXXJK8zhWWrml4_D8GmvKS-KyKBpLCGZD9YtLKIbR0" alt="Performance issues in generative AI applications 245 Let’s look at a high-complexity request to illustrate this. User request: In this scenario, the GenAI must first create the list of every US president, then seek information about each one, and then create a detailed summary of their policies and events during their terms in office. It must also retrieve content related to which things the presidents prioritized, identify consensus on what pieces of content were the top priorities, compile and summarize all that information, and then output it to the user. This is extensive knowledge retrieval, analysis, and text generation. Most likely, this information would require multiple LLM queries, and more queries equate to more spend. These examples illustrate how certain types of user requests can significantly increase the computational load for GenAI applications. Let’s now see how model serving strategies can impact GenAIperformance. Generating responses for every request individually can be inefficient, depending on volume. If the application is not designed to handle multiple requests concurrently, it will become slower the more users you have. If the application relies on cloud-based services, network latency can affect performance. Slow internet connections or high latency between the client and the server can cause delays. Frequent or complex API calls to external services can add to the response time, especially if those services are experiencing a high load or are geographically distant. Let’s return to the stock predictor application for an example. Because your GenAI application receives some news coverage, your website experiences a surge in traffic, and the number of customers interacting with the application increases dramatically. But, since your application handles each request individually and cannot process multiple requests concurrently, the response time for each user increases as the system becomes overwhelmed. Users experience slower response times, leading to frustration. The news coverage was from an influencer in Sydney, Australia, so the surge in users is from Asia. Your servers are in the US East region, and network latency due to the geographical distance between the server and the clients causes delays. Customers with slow internet connections experience even longer wait times, further degrading the user experience. Your application frequently calls external APIs to fetch real-time data for stock prices and financial market news. If these external services are experiencing high load, the API calls take longer tocomplete. Generate a detailed and historically accurate list of the top three priorities for every US president, but do not include their policies related to South America. Model serving strategies"></li><li class="carousel-slide" data-ssplayer-slide-index="246" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/247/1739580232671?e=1743033600&amp;v=beta&amp;t=UmPG-bUxmO-cHdng12CdB-UzoBsM7tWkAfKiQeqgaGQ" alt="246 Common Failures of Generative AI High I/O operations Poor data-handling practices, such as reading large datasets inefficiently or not using appropriate data structures, can slow down performance. Frequent read/write operations to disk can be a bottleneck, as can poorly optimized database interactions and malformed queries. The example stock price predictor application frequently reads large historical stock price datasets to make predictions. Let’s walk through some potential issues with data handling that result in high I/O operations: Now that you have navigated through these GenAI challenges, you can appreciate some of the complexities and nuances that accompany these powerful technologies. The issues of hallucinations, sycophancy, data leakage, cost, and performance present formidable obstacles that demand a critical eye and innovative solutions. Each challenge offers a unique perspective on the limitations and potential pitfalls inherent in GenAI applications. Despite these hurdles, GenAI remains unequivocally valuable. It continues to transform industries, enhance productivity, and open new avenues for creativity and innovation. By understanding and addressing these challenges, developers can harness the full potential of GenAI, delivering robust, reliable, and responsible applications. At the same time, it’s also important to note that applications can be useful even when they are not always correct. To take ChatGPT as an example: it has greatly improved the productivity of millions of users already, even though its deficiencies are well-known (and some not so easily worked around). Your GenAI application could be just as useful and popular but with similar caveats. In the next chapter, you’ll look at ways to optimize your GenAI application, improving its outputs and performance for a better user experience as well as combatting some of the issues discussedhere. • The application reads large datasets inefficiently, such as loading the entire dataset into memory even when only a subset is needed, which consumes excessive memory and processing power, slowing down performance. • The application saves intermediate prediction results and logs to disk after every prediction cycle. Frequent read/write operations to disk form a bottleneck, which significantly increases the time it takes to complete each prediction cycle. The application queries a database to fetch recent financial news and other relevant data before making predictions. However, a lack of indexes means that query results are slowly delivered. This increases response times, making the application slow to respond to user requests. Assuming you have a large dataset, you’ll want to avoid these practices as they will affect user experience and increase costs. • Summary"></li><li class="carousel-slide" data-ssplayer-slide-index="247" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/248/1739580232671?e=1743033600&amp;v=beta&amp;t=FZW64xuf3B0qrPGNKe8C4nSSEPNJKoGTfL70SbjXAF0" alt="12 Correcting and Optimizing Your Generative AI Application Until this point, you’ve read about how to build a generative AI (GenAI) application, its various components, and how they fit together. You've gained a solid understanding of what makes them work (and not work) well. You’re also aware of some of the challenges of GenAI applications and how to identify them. In this chapter, you’ll begin unraveling the mystery of how to improve your GenAI application once you’ve identified its shortcomings. You will also learn about optimizing and fine-tuning your GenAI application, so it’s a reliable, effective, and stable machine working in your favor, instead of a rogue actor bringing chaos. This chapter will discuss several well-known techniques to improve your GenAI application, so you can be confident in your finished product. Ideally, you will perform all of these techniques. Thechapter will define each of these and explain how they can improve your application. Then, you will complete a robust example of each of these as an activity. By the end of this chapter, you will have many ideas on how to improve your application."></li><li class="carousel-slide" data-ssplayer-slide-index="248" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/249/1739580232671?e=1743033600&amp;v=beta&amp;t=FDvxdt1YZKAYSeim3xrQZh3zl-Fd6gm1ulgB9g4pbno" alt="248 Correcting and Optimizing Your Generative AI Application This chapter will cover the following topics: • • Baselining Training and evaluation datasets • Few-shot prompting • • Retrieval and reranking Late interaction strategies, including in-application feedback and user feedback loops • Query rewriting • Testing and red teaming • Information post-processing This chapter does not contain any coding. However, it builds upon all the previous chapters to describe various methodologies for improving and optimizing your GenAI application output. To recreate some of the examples, you’ll simply need to use your favorite large language model (LLM) provider and recreate the attempts yourself. This chapter uses ChatGPT. Baselining Baselining, in the context of GenAI, refers to the process of defining a standard or a reference output for the AI model to compare future outputs. This standard serves as a crucial benchmark for evaluating the model’s performance, consistency, and improvements over time. By establishing a baseline, developers and stakeholders can objectively measure how the AI performs relative to a predefined set of expectations, ensuring that the model meets and maintains desired standards. In GenAI, baselining is essential for several reasons. Firstly, it provides a clear metric for assessing the quality and performance of the AI model. Secondly, it helps in tracking the model’s progress and improvements over time. Finally, baselining is a tool to help ensure consistency in the model’s outputs, via detection of output variability. All of these are vital for maintaining reliability and trust in the AI system. The aspects of the AI model that can be baselined are numerous and highly dependent on the specific application and its goals. Some common elements that might be baselined include the following: • Accuracy: This involves measuring the correctness of the model’s outputs. For instance, in a language model, accuracy can be gauged by how well the generated text matches the expected text or how often it provides the correct information. • Speed of response: This refers to the time it takes for the model to generate an output after receiving an input. Faster response times are generally preferred, especially in real-time applications. Technical requirements"></li><li class="carousel-slide" data-ssplayer-slide-index="249" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/250/1739580232671?e=1743033600&amp;v=beta&amp;t=8HVFrRpLI4tZSbBG-kVg_u7qI3nS8DpPl5ABAKpO4nE" alt="Baselining 249 To create your baseline, you will need to create an evaluation dataset. An evaluation dataset is a series of questions asked of your application to determine whether it meets the standards you have identified. Note that the evaluation dataset is not to be confused with the training dataset, which is the data that you used to train your model. The evaluation dataset should be a wholly different set of questions and answers. Effectively, the training dataset is akin to the notes and sources that you’d give to a student to learn, while the evaluation dataset is like the final exam. You don’t want to make that exam too easy! Training datasets As its name suggests, a training dataset is a collection of data used to teach or train a machine learning model. It contains input-output pairs where the input data is fed to the model, and the model learns to produce the correct output. This process involves adjusting the model’s parameters so that it can generalize well to new, unseen data. The quality and diversity of the training dataset directly impact the performance and accuracy of the trained model. High-quality training data ensures that the model can recognize patterns and make accurate predictions or generate appropriate responses. Therefore, your training dataset should be representative of the problem domain, covering a wide range of scenarios that the model would be expected to encounter in real-world applications. This helps in reducing biases and improving the model’s generalizability. • Effectiveness: This can be a measure of how well the AI meets its intended purpose. For example, in a recommendation system, effectiveness might be assessed by the relevance and personalization of the recommendations provided. • User satisfaction: This subjective metric can be gauged through user feedback and surveys, reflecting how satisfied users are with the AI’s performance and outputs. Establishing a baseline standard alongside your current performance also helps you—the engineer— determine whether you are improving results over time. This knowledge is crucial for ensuring that your application is not degrading in performance. In some industries, baseline performance indicators may be required to meet industry or regulatory standards and may be a reporting requirement for your application or organization. Once you evaluate the initial performance of your application, you’ll want to document these results. Subsequently, ensure that you consistently compare the model’s outputs to the baseline during each training and update cycle. Comprehensive documentation provides a reference that can be used to compare future outputs and identify trends or issues in the model’s performance. Regular evaluation of the model’s outputs against the baseline is also critical. During subsequent iterations of training and updates, these evaluations can help in detecting deviations from the expected (baseline) performance. If the model’s performance drops below the baseline, it can indicate a problem that needs to be addressed, such as data drift, changes in user behavior, or issues with the training dataset. Training and evaluation datasets"></li><li class="carousel-slide" data-ssplayer-slide-index="250" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/251/1739580232671?e=1743033600&amp;v=beta&amp;t=xZJTqGSHtDdZzRiJuDA502xy7CclIkJc7B0_1CsEkH4" alt="250 Correcting and Optimizing Your Generative AI Application The types of data in the training dataset might include the following: • Labeled data: This is the primary type of data used in supervised learning. Each data point consists of an input and a corresponding correct output, or label. For instance, in a text classification task, labeled data might include sentences paired with their respective categories. • Unlabeled data: Used in unsupervised learning, this data does not come with predefined labels. The model tries to find patterns and structures in the data. For example, clustering algorithms use unlabeled data to group similar data points together. • Mixed data: Semi-supervised learning uses a combination of labeled and unlabeled data. This approach leverages the large amounts of unlabeled data available while benefiting from the smaller labeled dataset to guide the learning process. • Diverse data: Including diverse data ensures that the model can handle various inputs. This might include different languages, dialects, formats, and contexts. For certain types of applications, this might include training data that is both human-readable documentation as well as code bases. Despite all that, you might wish to also include supplemental training data. Supplemental training data refers to additional data used to fine-tune or enhance the performance of an already trained model. There are many reasons to do this, but let’s talk about three that are particularly compelling: • Supplemental data can help adapt a general model to a specific domain. For example, a language model trained on general text might be fine-tuned with medical literature to perform better in healthcare applications. • Supplemental training data can be used to enhance the model’s ability in particular areas where it might be weak. For example, adding more data related to financial transactions can help a fraud detection model become more accurate. • As new information becomes available, supplemental training data can be used to update the model’s knowledge. This is especially relevant for applications requiring up-to-date information, such as news generation or where the industry is rapidly evolving (such as technology). In addition to your training data and supplemental data, you’ll also need an evaluation dataset. Evaluation datasets are crucial because they provide a controlled and consistent way to measure the performance of your AI model. They serve as a benchmark for comparison, ensuring that the model’s outputs can be objectively assessed against predefined criteria. By using a standard dataset, you can reliably track improvements, identify weaknesses, and maintain the quality of the model over time. It helps in validating that the model is not only performing well during the development phase but also generalizing effectively to new, unseen data. The content of an evaluation dataset depends on the specific application and its goals. Generally, it should include the following: Evaluation datasets"></li><li class="carousel-slide" data-ssplayer-slide-index="251" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/252/1739580232671?e=1743033600&amp;v=beta&amp;t=qmZ4YFJzaPxkaOA9ueYI5JorSq2bRZ32k5EilDsoPz4" alt="Baselining 251 • Representative queries: A variety of questions or inputs that the AI is likely to encounter in real-world usage. These should cover different scenarios and edge cases to ensure a comprehensive evaluation. Expected outputs: Corresponding correct or ideal responses for each query, against which the AI’s responses will be compared. • • Diverse data: Data that reflects the diversity of inputs the model will face, including variations in language, format, and context. This helps in assessing the model’s robustness and ability to handle different types of input. For example, the evaluation dataset for the MongoDB documentation chatbot includes questions and answers to the top 250 search terms, top 250 support questions by volume, and some of the most common questions asked about MongoDB. This can take the form of simple keywords or actual phrases in full-sentence format, like so: These terms and questions were retrieved from a combination of sources, which will vary depending on your infrastructure. For MongoDB, this infrastructure comes from the Google search console for mongodb.com as well as the support chat, community forums, and Stack Overflow. Determining the right amount of evaluation involves balancing thoroughness with practicality. You should have enough data to cover a wide range of scenarios and ensure the outputs of your GenAI application are consistently accurate and reliable. Typically, this involves hundreds or even thousands of data points, depending on the complexity of the application. That said, while more data can provide a more comprehensive assessment, there is a point of diminishing returns where additional data does not significantly improve the evaluation but adds to thecomplexity and resource requirements. Over-evaluation can also lead to overfitting of the evaluation dataset rather than improving overall performance. Returning to the earlier student/exam analogy, you don’t want your evaluation exam to be an exact replica of the training materials because all you would be testing then is whether the student was able to memorize a question and response. You would not be testing how well the student has learned the material. In summary, ensuring high-quality, accurate, and comprehensive training and evaluation datasets can reduce the likelihood of the model learning incorrect patterns. This requires significant effort upfront before deployment of the GenAI application but can dramatically improve your GenAI accuracy and depth of response as well as ensure its quality of responses to your users. Mongodb install Install mongodb ubuntu Mongodb connection string $in mongodb How to create a collection in mongodb What is an aggregation pipeline Cannot deserialize a 'String' from BsonType 'ObjectId' in C#"></li><li class="carousel-slide" data-ssplayer-slide-index="252" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/253/1739580232671?e=1743033600&amp;v=beta&amp;t=svirn7CJA1Nd6Qs_TfHcUIxvwT3DPNXy65cobrN32R0" alt="252 Correcting and Optimizing Your Generative AI Application Few-shot prompting In many cases, you will be asking your GenAI application to produce new content or summarize existing content, in which case what you need to do is provide the existing application with a properly engineered prompt. Most of the time, having the user simply request what they need is sufficient. But in cases where the outputs are complex, you will find that the quality and accuracy of the GenAI application’s response are improved by using a technique called few-shot prompting. Few-shot prompting is when you provide an example as part of the input to the LLM so that it can see exactly what type of syntax and response you need. You can also include a definition as part of the example in case you believe the input might be a term with which the LLM would not be familiar, or in case you’re using a business-specific term. Let’s try out an example using GPT-4. Example 1: Let’s see how the LLM responds to a request that does not use few-shot prompting. This is the user input: Here is the output: Please generate a linkedin post less than 500 words about the company OpenAI. 🚀 Pioneering the Future with OpenAI 🚀 In the dynamic world of technology, OpenAI stands as a beacon of innovation and progress. Their mission to advance artificial general intelligence (AGI) is not just envisioning the future; they are actively shaping it. OpenAI operates under the guidance of visionary leaders like CEO Sam Altman. Their cutting-edge research is pushing the boundaries of what's possible in AI, crafting a future where technology enhances human capabilities and opens new realms of possibility. From healthcare to finance, and education to entertainment, OpenAI's innovations are leaving an indelible mark. Their models, such as GPT-4, are revolutionizing how we interact with technology. Businesses are leveraging these tools to enhance customer experiences, streamline operations, and achieve unprecedented levels of productivity. OpenAI thrives on collaboration, partnering with organizations worldwide to accelerate innovation. Their open research culture demonstrates a belief in collective progress. By sharing insights, tools, and findings, OpenAI is democratizing access to AI, empowering developers, researchers, and businesses to push the boundaries of what's possible. Through initiatives like the OpenAI Scholars and Fellows programs, they are nurturing the next generation of AI talent. These programs provide mentorship, resources, and opportunities to individuals from diverse backgrounds, ensuring a wealth of perspectives and ideas in AI development. Join the conversation and be part of the future OpenAI is creating. #AI #OpenAI #Innovation #Technology #Future #Collaboration #Leadership"></li><li class="carousel-slide" data-ssplayer-slide-index="253" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/254/1739580232671?e=1743033600&amp;v=beta&amp;t=Ho-RsThUpItR5uAeW7YfdakYqazpJlg3LmTrrdyibJM" alt="Baselining 253 Here is the output: Example 2: Now, let’s try this example with few-shot prompting. This is the user input: You can see how, by providing the example text, you can enhance the output to make it match whatever syntax you prefer. The additional prompt information need not be terribly difficult to produce either. If you can provide an example output to your GenAI application, its results will be much nearer to what you desire. Please generate a linkedin post focusing on the OpenAI company. It should be less than 500 words in length. Also, please match the tone, syntax, word choices, and style of this passage: &quot;There's a way to do it better. Find it.&quot; — Thomas Edison &quot;Lies. We are being lied to. We are told that technology takes our jobs, reduces our wages, increases inequality, threatens our health, ruins the environment, degrades our society, corrupts our children, impairs our humanity, threatens our future, and is ever on the verge of ruining everything. We are told to be angry, bitter, and resentful about technology. We are told to be pessimistic. The myth of Prometheus – in various updated forms like Frankenstein, Oppenheimer, and Terminator – haunts our nightmares. We are told to denounce our birthright – our intelligence, our control over nature, our ability to build a better world. We are told to be miserable about the future.&quot; –Marc Andreessen We are told that artificial intelligence is a threat, that it steals our jobs, that it widens the chasm of inequality, that it's an invisible force eroding our humanity. We are told that AI is an uncontrollable beast, ready to plunge us into dystopia. OpenAI is here to shatter these myths. Founded on the principle of advancing digital intelligence for the collective good, OpenAI isn't just another tech company. It's a bold proclamation that the future is ours to shape. OpenAI stands as a beacon against the darkness of misinformation and fearmongering. It's a rallying cry for innovation, for the relentless pursuit of better. Imagine a world where AI enhances every aspect of our lives—where it amplifies human potential rather than diminishes it. OpenAI's groundbreaking models, like GPT-4, are not instruments of destruction but tools of empowerment. From revolutionizing healthcare to transforming education, from pioneering new frontiers in entertainment to optimizing complex financial systems, OpenAI's innovations are the keystones of a new era. OpenAI is driven by visionaries who refuse to bow to the pessimism that pervades our discourse. That we are a testament to what can be achieved when we refuse to accept the status quo. We are the architects of tomorrow, wielding AI not as a weapon, but as a bridge."></li><li class="carousel-slide" data-ssplayer-slide-index="254" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/255/1739580232671?e=1743033600&amp;v=beta&amp;t=d2tL_PZPlPRBdoWDtqmdEuO2N9cteS8CzGJMqBhknw4" alt="254 Correcting and Optimizing Your Generative AI Application Retrieval and reranking Retrieval and reranking are key techniques used to enhance the performance and accuracy of LLMs. First, understand that by retrieving relevant context or documents, an LLM provides more accurate and contextually relevant responses. This is particularly useful when the model’s training data does not cover the specifics of the query or when up-to-date information is required. In the context of LLMs, retrieval can involve searching through a vast collection of documents, knowledge bases, or other data sources to find pieces of information that are pertinent to a given query or task. Let’s have a look at the two different types of retrieval: • Keyword-based retrieval: This uses keywords from the query to find matching documents. For instance, if you use the word cars in your query, it returns documents that contain the word cars. • Embedding-based retrieval: This uses vector embeddings to find matching documents. Both the query and documents are transformed into vectors in a high-dimensional space. Retrieval then involves finding vectors (that is, documents) that are close to the query vector. Reranking is the process of reordering the retrieved documents or pieces of information to prioritize the most relevant ones. After the initial retrieval, the documents are ranked based on their relevance to the query. Retrieved documents are initially ranked based on their similarity to the query using methods such as cosine similarity in embedding space. However, a more sophisticated model can rerank these initially retrieved documents by considering additional features and context. Let’s look at the following examples. Example 1: Recommending restaurants with a GenAI application. You have built a GenAI application that provides restaurant recommendations. A user requests restaurants currently open near them. When examining the potential restaurants to provide to the user, the application looks at the distance from the user’s current location or provided address and the current local time and opening hours. It will then rank the results so that the closest restaurant is the first one shown to the user. This is a perfectly fine solution. But you may want to have smarter results that are dynamically reranked based on other criteria, such as user ratings for the restaurants. You may want to show a higher-rated restaurant that is three miles away first, rather than a one-star restaurant that is one mile away. As the user gives feedback on the results, you may want to rerank dynamically, expanding your pool of restaurants as you get more information about what the user would prefer (including, say, the type of cuisine or ambiance). By reranking the results, the most relevant and useful information is prioritized, improving the overall quality of the LLM’s output. It helps in filtering out less relevant or redundant information, ensuring the response is precise and useful."></li><li class="carousel-slide" data-ssplayer-slide-index="255" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/256/1739580232671?e=1743033600&amp;v=beta&amp;t=qVYgfbBcSmCPbgUrhQDIYi8FEhGG2T48tthRA6kDKLI" alt="Baselining 255 When combined, retrieval and reranking significantly enhance LLM outputs with the following: • The model can access and utilize relevant information that might not be present in its training data, providing more accurate and contextually appropriate answers. • By focusing on the most relevant information through reranking, the model’s responses become more precise, reducing errors and irrelevant content. • Retrieval can pull in the latest information from updated sources, making the model’s responses more current. • These techniques allow the model to handle specific, detailed queries efficiently without needing to retrain the entire model frequently. Example 2: Summarizing the latest research on quantum computing. Here’s another practical example. Suppose you ask an LLM about the latest research on quantum computing. The steps of the output would be as follows: 1. Retrieval: The model searches through a large database of scientific papers and articles to find relevant documents on quantum computing. Reranking: The initially retrieved documents are then reranked, with the most recent and pertinent studies placed at the top. 2. 3. Response generation: The LLM uses the top-ranked documents to generate a detailed and accurate response about the latest research trends in quantum computing. By incorporating retrieval and reranking, the LLM can provide a well-informed, up-to-date, and contextually accurate answer, vastly improving the user experience. Now that you’re ready to take your application into production, there are still a few more things you can do to help improve the user experience and create a feedback loop in order to get a better signal as to the behavior of your GenAI application. This next set of recommendations focuses on late interaction strategies, sometimes referred to as contextualized late interaction over BERT (ColBERT). First, let’s define interaction. Interaction refers to the process of evaluating the relevance between a query and a document by comparing their representations. A late processing strategy is one where the interaction between the query and document representations occurs later in the process, typically after both have been independently encoded. Early interaction models are where query anddocument embeddings interact at earlier stages, typically before or during their encoding by the model. Second, let’s dig a little bit into the internal workings. When a user interacts with a GenAI application, they input a query that is encoded into a dense vector representation. Potential responses, usually documents or passages, are also encoded into dense vector representations. The system performs similarity matching between the query and document embeddings, returning the documents with the highest similarity scores as the best matches. Late interaction strategies"></li><li class="carousel-slide" data-ssplayer-slide-index="256" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/257/1739580232671?e=1743033600&amp;v=beta&amp;t=hB3JPqahkZD6pqYVG2iiR8lawP84ktAWDR7pSfwpwNI" alt="256 Correcting and Optimizing Your Generative AI Application Here, for each button style, you can add the terms that augment the user query and then apply them before submission. To enhance relevance, you don’t return all matching results to the user. Instead, you aim to provide the most relevant results or a summarized version of the result set. Late interaction models such as ColBERT improve efficiency by focusing on the most promising query-document pairs rather than considering all possible pairs, yielding more precise results and a better user experience. This selective approach allows for more precise and relevant results, enhancing the user experience. If you need to focus on improving search results, consider implementing ColBERT or similar techniques to enhance retrieval performance and provide more relevant results for user queries. Query rewriting, or query reformulation, is a technique used to improve the quality of the answers provided by LLMs. This process involves modifying the original query to make it clearer, more specific, or more detailed, which can help the model generate better responses. LLMs do not explicitly rewrite queries in the background, so this effort is manual unless you have implemented a workflow that will evaluate and rewrite the user’s query before it’s processed. Rewriting a query can make it clearer and more precise, reducing ambiguity and ensuring the model understands exactly what is being asked. Adding relevant context or details to the query can help the model provide more accurate and contextually appropriate answers and can help disambiguate terms that have multiple meanings, ensuring the response aligns with the intended meaning. In addition, reformulating the query to include additional relevant details can lead to more comprehensive answers. How does query rewriting work? It’s important to understand user intent for your GenAI application. What is the purpose of your application, and what kinds of questions will your application attempt to answer? Understanding what sort of response users expect versus what your application might deliver is key. After that, you can do the following activities, which are not mutually exclusive, meaning that you can perform some, just one, or none of these. For instance, based on the intent, the user query can be augmented with additional context and details. This activity substantially expands the user query (and increases the token count per query) but will typically yield much better results. To take an easy example, imagine that your application generates images. The user requests of a kitten, a quite simple query that could have endless results. To help the user get better results, you can add three buttons in the UI so that the user can select a style—a realistic photograph style, a Renaissance painting style, or an anime cartoon style. When the user clicks the style button and then submits their inquiry, instead of relaying kitten, the query is modified to the following: Query rewriting a picture of a a picture An image of a kitten, in anime style, large eyes, chikai, chibi-style, pixel-style, anime illustration, cute, in the style of Akira Toriyama."></li><li class="carousel-slide" data-ssplayer-slide-index="257" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/258/1739580232671?e=1743033600&amp;v=beta&amp;t=OUV6S2UsG6pMsDtvjIZrcgSLjQPM0WSIEZv2kdzapaU" alt="Testing and red teaming 257 A meaningful rewrite could be as follows: As another example, consider this user query: Just like your typical software engineering features, you’ll want to include the phases of unit testing, integration testing, performance testing, and user acceptance into your test plan. However, the specifics of how this is done vary from one use case to another. In the context of GenAI applications, unit testing still has the same basic tenets and involves testing individual components or modules of the application to ensure they function correctly. However, in the case of GenAI applications, your unit tests will need to also include steps such as the following: This rewritten query with additional context helps the system understand that the user is asking for a specific product and time period, leading to a more accurate and useful response. Ultimately, when conducting query rewrites, you’ll want to simplify the language. Complex queries can be simplified or broken down into simpler parts, making it easier for the model to process and respond accurately. This method involves taking a large query and breaking it into constituent parts (which typically is achieved via a series of input fields/forms) and then unifying each data entry into a single submitted query. This guides your user into constructing a well-formed query without specialized knowledge. As an example, imagine your user has only a single-entry field to input their query. In such a case, they may leave out relevant information or provide irrelevant information that could impact accuracy or increase the possibility of hallucination. Instead, if you were to provide the user with a series of fields, each with clear instructions, and then assemble the inputted information into a query that was fed into the GenAI application, you would get a better outcome than a free-form text entry. For practical implementation, you could consider a workflow in which the system itself analyzes the query for intent and context, reviews the query’s complexity, and then rewrites the query to be clearer, more specific, or more detailed. The reformulated query can then be used to generate the response. Testing and red teaming Testing AI systems is critical to ensure their accuracy, reliability, and overall performance. Typically, in software engineering, automated testing is used as part of the software development process. GenAI applications are no different. You’ll want to routinely and regularly test the outputs to ensure there are no radical shifts in output quality. &quot;What's the average revenue?&quot; &quot;What's the average revenue for [May 2024] for [sales sku 123]?&quot; Testing"></li><li class="carousel-slide" data-ssplayer-slide-index="258" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/259/1739580232671?e=1743033600&amp;v=beta&amp;t=R7_fCDQuO1ersc_JCzDUYlRDtTbUaIOSYooXpJwU1YA" alt="258 Correcting and Optimizing Your Generative AI Application • Input validation: Ensure that the application correctly handles and validates various input types, formats, and ranges. Test for edge cases, such as empty inputs, excessively large inputs, or malformed data. • Pre-processing: Verify that any pre-processing steps, such as tokenization, normalization, or feature extraction, are performed correctly. • Model loading: Test that the model is correctly loaded from its storage location, and verify that the correct version is being used. • Model inference: Ensure that the model generates outputs without errors given valid inputs. Test the inference function with controlled inputs to verify expected behavior, such as deterministic responses for certain prompts or scenarios. • Output format: Validate that the generated outputs meet the expected format and structure. This includes checking that outputs are complete, correctly formatted, and adhere to any length or content constraints. • Post-processing: Test any post-processing steps that modify or enhance the model’s output, such as cleaning up text, converting formats, or applying additional business logic. Proper functioning: The outputs should work. If your GenAI application outputs code, you will need to test that the code itself compiles and behaves as intended. • These are just a few of the items that you should include for unit testing your GenAI application. Integration testing focuses on verifying that the components of your GenAI system work together as needed. This means you’ll be testing the interactions between components to check the following: • Whether your data ingestion pipeline pulls the correct data • How recommendations are presented to the user (formatting, for instance, if this is done by another library or tool) • API load testing, if you’re using another LLM such as OpenAI or Anthropic You’ll want to evaluate processing time, efficiency, and scalability via performance testing. This might include activities such as the following: • Load testing your application for how it handles a large volume of simultaneous queries. • Assessing the inference time of self-hosted models on various hardware configurations. • Measuring how many token limits should be set for input and output to control costs and processing time. • Measuring the time taken for the model to generate outputs and ensuring it meets performance requirements. This can be especially important for applications with real-time constraints."></li><li class="carousel-slide" data-ssplayer-slide-index="259" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/260/1739580232671?e=1743033600&amp;v=beta&amp;t=INSwuR9YoXNcvSIR79cfTr70jn1_pIrz-Dw2wls5R6U" alt="Testing and red teaming 259 If your GenAI application will accept natural language prompts and inputs from human beings, then the practice of red teaming cannot be recommended enough. Red teaming involves simulating real- world, challenging, or adversarial situations to identify vulnerabilities and weaknesses in your GenAI application. This approach is borrowed from cybersecurity practices and is particularly important for ensuring your GenAI application meets user expectations. This involves having a large pool of users who will ask real-world questions, but they are not limited by scripts as to what they may ask. The reason for red teaming is that GenAI applications can, and often do, produce different outputs that vary widely, even with similar or the same input. Not only that but the quality of the generated output is often subjective and depends on human judgment. So, while traditional software applications produce predictable and consistent results, the same is not true of GenAI. Let’s take an example to see how this works. For a chatbot application, you might have routine automated testing that would ask your GenAI application the top 200 most common user questions and then evaluate them for correctness. With a red team, you would have 50 users ask whatever questions they wanted, and then record both the questions asked and the responses. This might yield insights such as the following: • If a user asks a question in a similar way but not with the exact same wording, they receive incorrect or less correct answers. • Some users will ask malicious questions and the GenAI application will respond poorly. • Other users ask questions that are not part of the training data, and the GenAI application hallucinates answers (or gives no answer at all), thus identifying the need to expand your training data. • When users ask many questions in a row, the application stalls. • When a user asks specific question types, they are dissatisfied with the output because the application lacks high-quality training data or the formatting of the reply is undesirable. • When properly prompted, the GenAI application will share details of other users’ sessions, thus identifying a security issue. In addition to this routine testing, you have more to add to your test suite. In general, it is also recommended that GenAI applications go through additional testing for the following: • Bias and fairness: If your model is making recommendations that affect lives and livelihoods, you’ll want to carefully consider training data biases for different demographic groups. Robustness: To ensure your GenAI application is resilient to variations and noise, you’ll want to test with adversarial examples and edge cases to evaluate its ability to handle unexpected inputs. • Once you’ve gotten through all of that, you’ll want to think about user acceptance testing, which is one of the most exciting parts of the process, as you will see in the next section. Red teaming"></li><li class="carousel-slide" data-ssplayer-slide-index="260" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/261/1739580232671?e=1743033600&amp;v=beta&amp;t=11vxk-sgnKr96Tslbzswg3Zd6aT1MQ4Iww-ctnBEEho" alt="260 Correcting and Optimizing Your Generative AI Application You might know that the main way in which GenAI differs from previous forms of AI or analytics is that it generates new content efficiently. But did you know that that content is often in unstructured forms, for example, written text or images? When you see outputs that are nicely formatted, in bulleted lists, multiple fonts, and so on, it is a form of information post-processing. Information post-processing refers to the series of steps taken after an AI model generates an initial response, but before that response is sent to the user. This crucial step enhances the output of GenAI models, refining raw responses to make them more useful, accurate, and contextually appropriate. It can take many forms, so this chapter will only discuss some of the most useful ones along with information on how to implement them: • Fact-checking: Verifying the accuracy of the information provided. This can involve checking facts against reliable sources or databases. • Formatting: Structuring the information in a clear and readable format, such as bullet points, paragraphs, or tables. This may also include style changes such as bold, text color, or font to enhance readability and emphasis. To enable the red-teaming phase, it is recommended that you record every question asked by every user, as well as every response given, and then ask testers to rate the response with notes. While this level of detailed user testing is strenuous and uncommon in software development, it is incredibly valuable to see how your application performs in real-world scenarios, with real human beings, before production. Due to the scale and scope of some AI systems, fully testing each component is impossible. Effective testing and red teaming rely on using judgment in terms of which parts of the system are most risky.It may be true that giving occasionally not-quite-accurate advice is a non-impactful event. However, the potential harm of a single hallucination could be quite high. You will want to consider the severity of harm, the likelihood of inaccuracy, and the ability to retract or rectify the inaccuracy as your standard measures of risk. Using those simple, albeit subjective, measures can assist you in determining towhat extent you test each aspect of the system, and the size of your red team. To give yourself a sense of what sorts of harms and incidents you will be testing for—which are too many to enumerate—you will find it helpful to review the AI Incident Database at https:// incidentdatabase.ai/. Upon review of this tool, you may find your specific use case (or ones like it) and what incidents have already been reported, so that you can test and think through the repercussions of inaccuracies. As an example, one incident that is detailed here involved an application that made staffing-level recommendations. However, the algorithm-based recommendations left facilities understaffed,leading to critical incidents of neglect, injury, and death. Those incidents then prompted lawsuits and even legislation against healthcare providers using AI. Information post-processing"></li><li class="carousel-slide" data-ssplayer-slide-index="261" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/262/1739580232671?e=1743033600&amp;v=beta&amp;t=mmcqMo6-RXHi4eJogLCJT226aOx-ut9daR9we7BMeJo" alt="Other remedies 261 • Grammar, style, and tone checking: At times, the resulting text provided by GenAI applications is not up to par or consistent with the exact messaging, tone, and style that one would expect a human being to write. Post-processing tools and vendors can take generated text outputs and markedly improve them for readability, making them match reader expectations. Information post-processing is a vital component in the lifecycle of GenAI outputs. It bridges the gap between raw model outputs and polished, user-ready responses, enhancing accuracy,readability, relevance, and overall user satisfaction. By implementing effective post-processing strategies, AI systems can deliver higher-quality and more reliable results. There are entire services springing up around this valuable step in the GenAI process, so engineers do not have to build it themselves. Other remedies Some other technical remedies can be employed even more easily than the ones detailed in this chapter. Some of these may improve the accuracy and performance of your GenAI application, though the level of effort involved varies. As an example, during MongoDB’s testing of GPT, it was discovered that the accuracy rate for the same set of questions was improved by 7% between GPT- 3.5 and GPT-4. Getting such a level of improvement in accuracy via prompting, retrieval augmentation, or late interaction strategies is certainly possible but would have been difficult. So, it is worth investigating every avenue of potential improvement, including areas such as hardware upgrades, code optimization, concurrency management, database query optimization, and even just upgrading your software. All of these can improve the results of your GenAI application and should be independently investigated: • Hardware and software upgrades: Upgrade computational resources, such as using more powerful GPUs, scaling horizontally with more servers, or updating to the latest version of the software, to outsize impacts on both accuracy and performance. • Code optimization: Refactor and optimize code to improve efficiency, reduce computational load, and handle data more effectively. • Network optimization: Reduce network latency by optimizing data transfer, caching responses, and minimizing API call overheads. • Concurrency management: Implement concurrency and parallel processing techniques to handle multiple requests efficiently. • Database optimization: Optimize database queries and interactions to reduce I/O overhead."></li><li class="carousel-slide" data-ssplayer-slide-index="262" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/263/1739580232671?e=1743033600&amp;v=beta&amp;t=owkzb-OLl1FdVqSiqwcePUjnM83glyE00O_8udusrdU" alt="262 Correcting and Optimizing Your Generative AI Application Summary Implementing mechanisms to correct and optimize your GenAI application can have many forms and can be implemented before, during, and after answers are generated. For optimal performance, you’ll want to train your GenAI model with high-quality data, supplement existing models with your specific use case data, and have thorough evaluation datasets and record the model’s performance to establish a baseline of accuracy. Once you have that baseline, however, you can immediately begin improving upon it with the techniques discussed in this chapter. Among these techniques is one- or few-shot prompting. It involvesproviding the GenAI model with a single example or prompt to guide its response, enabling the model to generate relevant and contextually appropriate outputs with minimal training data. You can also try retrieving and reranking relevant documents or data points based on the user’s query, and then reordering these results to prioritize the most relevant and useful information before generating a final response.Query rewriting is another technique that can improve clarity, specificity, or context, helping the AI model understand and respond more accurately to the user’s requests. Formatting GenAI responses via structuring and presenting the AI-generated content in a clear, organized, and readable manner can enhance the overall user experience and ensure the information is easily digestible. Similarly, implementing late interaction strategies such as ColBERT can improve the relevance and accuracy of the retrieved information. By testing, red teaming, and recording your results, you can track your progress in improving the performance, security, and quality of responses over time. GenAI technologies are changing (and will continue to change) the face of the software industry. With these optimization strategies in place, your GenAI application will be well equipped to adapt and excel in an ever-evolving landscape."></li><li class="carousel-slide" data-ssplayer-slide-index="263" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/264/1739580232671?e=1743033600&amp;v=beta&amp;t=RuPsl9WKGDNrX1jFffhIdDNL3j3n5iixbGasX8eGhGE" alt="Appendix: Further Reading In addition to the links provided within the chapters, here are some resources to take your learning journey forward. Chapter 1, Getting Started with Generative AI • Naveed et al. “A Comprehensive Overview of Large Language Models.” arXiv, July 12, 2023. https://arxiv.org/abs/2307.06435. • Gryka, Maciej. “Invest in RAG” in “Building reliable systems out of unreliable agents.” The Rainforest Blog, April 3, 2024. https://www.rainforestqa.com/blog/building- reliable-systems-out-of-unreliable-agents#Invest_in_RAG. “The Black Box: Even AI’s creators don’t understand it.” July 2023. Unexplainable. Produced by Vox Creative. Podcast, Spotify, 36:15. https://open.spotify.com/episode/3npjXNCt USGRUjVR4EYb4Y?si=-XpudYVzSEKfhD0-2NBjEQ. • • “Speech and Language Processing,” n.d., https://web.stanford.edu/~jurafsky/slp3/. • Hochreiter, Sepp, and Jürgen Schmidhuber. “Long Short-Term Memory.” Neural Computation 9, no. 8 (November 1, 1997): 1735–80. https://doi.org/10.1162/neco.1997.9.8.1735. • Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. “Attention Is All You Need.” arXiv (Cornell University), January 1, 2017. https://doi.org/10.48550/arxiv.1706.03762. • “Prompt Engineering Guide – Nextra,” n.d., https://www.promptingguide.ai/. • A. Aruna Gladys and V. Vetriselvi, “Survey on multimodal approaches to emotion recognition,” Neurocomputing 556 (November 1, 2023): 126693, https://doi.org/10.1016/j. neucom.2023.126693. • Sumit Kumar, “Positive and Negative Sampling Strategies for Representation Learning in Semantic Search,” Sumit’s Diary, March 22, 2023, https://blog.reachsumit.com/ posts/2023/03/pairing-for-representation. • Tomas Mikolov et al., “Efficient Estimation of Word Representations in Vector Space,” arXiv. org, January 16, 2013, https://arxiv.org/abs/1301.3781. Chapter 4, Embedding Models Chapter 3, Large Language Models Chapter 2, Building Blocks of Intelligent Applications"></li><li class="carousel-slide" data-ssplayer-slide-index="264" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/265/1739580232671?e=1743033600&amp;v=beta&amp;t=VH8fTYBsIxou37aW6zlyF9nX16xmtXqZfHXI_Nsk6aE" alt="264 Appendix: Further Reading • • Yu. A. Malkov and D. A. Yashunin, “Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs,” arXiv.org, March 30, 2016, http://arxiv. org/abs/1603.09320. Yikun Han, Chunjiang Liu, and Pengfei Wang, “A Comprehensive Survey on Vector Database: Storage and Retrieval Technique, Challenge,” arXiv.org, October 18, 2023, http://arxiv. org/abs/2310.11703. • OpenAI, “GPT-4”. GPT-4 Research, March 14, 2023. https://openai.com/index/gpt-4- research. • Jeffrey Pennington, “GloVe: Global Vectors for Word Representation,” n.d., https://nlp. stanford.edu/projects/glove. • Jacob Devlin et al., “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,” arXiv.org, October 11, 2018, https://arxiv.org/abs/1810.04805. “fastText,” n.d., https://fasttext.cc/.• • Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. “Deep contextualized word representations,” arXiv:1802.05365, March 22, 2018. https:// arxiv.org/pdf/1802.05365. • Karen Simonyan and Andrew Zisserman, “Very Deep Convolutional Networks for Large-Scale Image Recognition,” arXiv.org, September 4, 2014, https://arxiv.org/abs/1409.1556v6. • Kaiming He et al., “Deep Residual Learning for Image Recognition,” arXiv.org, December 10, 2015, https://arxiv.org/abs/1512.03385. • Aurora Cramer, Ho-Hsiang Wu, Justin Salamon, and Juan Pablo Bello, “OpenL3 — OpenL3 0.4.2 documentation,” n.d., https://openl3.readthedocs.io/en/latest/#. “Google | vggish | Kaggle,” n.d., https://www.kaggle.com/models/google/vggish.• • Tran, D., Bourdev, L., Fergus, R., Torresani, L., and Paluri, M., “Learning Spatiotemporal Features with 3D Convolutional Networks.” arXiv:1412.0767, October 7, 2015. https:// arxiv.org/pdf/1412.0767. • Grover, A., and Leskovec, J. “Node2Vec: Scalable Feature Learning for Networks.” Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016. https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf. • Bryan Perozzi, Rami Al-Rfou, and Steven Skiena, “DeepWalk,” August 24, 2014, https:// doi.org/10.1145/2623330.2623732. • Zhang, S., and Xu, Y. “Json2Vec: A Representation Learning Method for JSON Data.” arXiv:2002.05707, February 13, 2020. https://arxiv.org/pdf/2002.05707. • Alec Radford et al., “Learning Transferable Visual Models From Natural Language Supervision,” arXiv.org, February 26, 2021, https://arxiv.org/abs/2103.00020. Chapter 5, Vector Databases"></li><li class="carousel-slide" data-ssplayer-slide-index="265" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/266/1739580232671?e=1743033600&amp;v=beta&amp;t=eqOr3pCgG8SGdu_49knEuOSzR-8ALnS_RdEKcAE5go8" alt="265 • • • • • • “Customize Cluster Storage - MongoDB Atlas,” n.d., atlas/customize-storage/. . • Lauren Schaefer Daniel Coupal, “Bloated Documents | MongoDB,” May 31, 2022, . • Daniel Coupal, “Building with Patterns: The Extended Reference Pattern,” MongoDB, March 19, 2019, https://www.mongodb.com/blog/post/building-with-patterns-the- extended-reference-pattern. • “Atlas Cluster Sizing and Tier Selection - MongoDB Atlas,” n.d., https://www.mongodb. com/docs/atlas/sizing-tier-selection/. • “Customize Cluster Storage - MongoDB Atlas,” n.d., https://www.mongodb.com/docs/ atlas/customize-storage/. • “Amazon EBS volume types - Amazon EBS,” n.d., . • Zhi Jing et al., “When Large Language Models Meet Vector Databases: A Survey,” arXiv.org, January 30, 2024, http://arxiv.org/abs/2402.01763. • Doug Turnbull, “What Is a Judgment List?,” Doug Turnbull’s Blog, February 21, 2021, . • “Building RAG-based LLM Applications for Production,” Anyscale, n.d., . • “How to Perform Hybrid Search - MongoDB Atlas,” n.d., . https://docs.aws.amazon.com/ebs/ latest/userguide/ebs-volume-types.html#gp3-ebs-volume-type https://www.mongodb.com/docs/ https:// softwaredoug.com/blog/2021/02/21/what-is-a-judgment-list https://www. anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm- applications-part-1 https://www.mongodb.com/ docs/atlas/atlas-vector-search/tutorials/reciprocal-rank-fusion/ “Review Deployment Options - MongoDB Atlas,” n.d., https://www.mongodb.com/docs/ atlas/atlas-vector-search/deployment-options/. “MongoDB Atlas,” LangChain, n.d., https://python.langchain.com/v0.2/docs/ integrations/vectorstores/mongodb_atlas/. “How to Index Fields for Vector Search - MongoDB Atlas,” n.d., https://www.mongodb. com/docs/atlas/atlas-vector-search/manage-indexes/. “Get Started with the LangChain Integration - MongoDB Atlas,” n.d., https://www.mongodb. com/docs/atlas/atlas-vector-search/ai-integrations/langchain/. “How to Index Fields for Vector Search - MongoDB Atlas,” n.d., https://www.mongodb. com/docs/atlas/atlas-vector-search/vector-search-type/#considerations https:// www.mongodb.com/developer/products/mongodb/schema-design-anti-pattern- bloated-documents/ Chapter 6, AI/ML Application Design Chapter 7, Useful Frameworks, Libraries, and APIs"></li><li class="carousel-slide" data-ssplayer-slide-index="266" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/267/1739580232671?e=1743033600&amp;v=beta&amp;t=vHUgdNiNi9rK0bLp_P_hVViDaryl2SvfTx9LAyGZNg4" alt="266 Appendix: Further Reading • • • • • • • • • • • “MongoDB with Python - MongoDB Documentation,” n.d., . “Transformers,” n.d., “OpenAI developer platform,” OpenAI Platform, n.d., docs/overview. “SentenceTransformers Documentation — Sentence Transformers documentation,” n.d., sbert.net/. “Train and Fine-Tune Sentence Transformers Models,” n.d., . “Run Vector Search Queries - MongoDB Atlas,” n.d., . “Knowledge Graph RAG Query Engine - LlamaIndex,” n.d., “Papers with Code - Measuring Massive Multitask Language Understanding,” September 7, 2020, https://paperswithcode.com/paper/measuring-massive-multitask- language. “Papers with Code - HellaSwag: Can a Machine Really Finish Your Sentence?,” May 19, 2019, . “Papers with Code - Evaluating Large Language Models Trained on Code,” July 7, 2021, . • Yunfan Gao et al., “Retrieval-Augmented Generation for Large Language Models: A Survey,” arXiv.org, December 18, 2023, https://arxiv.org/abs/2312.10997. • Rupak Roy, “Harness LLM Output-parsers like CommaSeparatedListOutputParser, PydanticOutputParser and more for a Structured Ai | by Rupak (Bob) Roy - II | Medium | Medium,” Medium, August 14, 2024, https://bobrupakroy.medium.com/harness- llm-output-parsers-for-a-structured-ai-7b456d231834. • Mirjam Minor and Eduard Kaucher, “Retrieval Augmented Generation with LLMs for Explaining Business Process Models,” in Lecture Notes in Computer Science, 2024, 175–90, https://doi. org/10.1007/978-3-031-63646-2_12. https://www.mongodb.com/ docs/languages/python/#integrations https://huggingface.co/docs/transformers/en/index. https://platform.openai.com/ https://huggingface.co/ blog/how-to-train-sentence-transformers https://www.mongodb.com/docs/ atlas/atlas-vector-search/vector-search-stage/#atlas-vector-search- pre-filter https://docs.llamaindex. ai/en/stable/examples/query_engine/knowledge_graph_rag_query_engine/. https://paperswithcode.com/paper/hellaswag-can-a-machine-really- finish-your https:// paperswithcode.com/paper/evaluating-large-language-models-trained-on “Introduction | Ragas,” n.d., https://docs.ragas.io/en/stable/index.html. https:// Chapter 9, LLM Output Evaluation Chapter 8, Implementing Vector Search in AI Applications Chapter 10, Refining the Semantic Data Model to Improve Accuracy"></li><li class="carousel-slide" data-ssplayer-slide-index="267" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/268/1739580232671?e=1743033600&amp;v=beta&amp;t=XFvCPLBvHmVd_3HyCQQ5OlG7T-_4u9rIt0hi9GEn1Hs" alt="267 Chapter 11, Common Failures of Generative AI Chapter 12, Correcting and Optimizing Your Generative AI Application • Chui, Michael, Roger Roberts, Tanya Rodchenko, et al. “What every CEO should know about generative AI.” McKinsey Digital, May 12, 2023. https://www.mckinsey.com/ capabilities/mckinsey-digital/our-insights/what-every-ceo-should-know- about-generative-ai. • Xiao, Han. “What is ColBERT and Late Interaction and Why They Matter in Search?,” Jina AI, February 20, 2024. https://jina.ai/news/what-is-colbert-and-late-interaction- and-why-they-matter-in-search/. • Lance Eliot, “Doctors Relying On Generative AI To Summarize Medical Notes Might Unknowingly Be Taking Big Risks,” Forbes, July 2, 2024, https://www.forbes.com/sites/ lanceeliot/2024/02/05/doctors-relying-on-generative-ai-to-summarize- medical-notes-might-unknowingly-be-taking-big-risks/. • Markman, Ofer. “Time to Strategize: 85% of Data is Garbage or Siloed.” Filo Focus, February 11, 2024. https://www.filo.systems/blog/85-percent-of-data-is-not-actionable- time-to-restrategize. • Neeman, Ella, Roee Aharoni, Or Honovich, et al. “DisentQA: Disentangling Parametric and Contextual Knowledge with Counterfactual Question Answering.” arXiv.org, November 10, 2022. https://arxiv.org/pdf/2211.05655. • Sharma, Mrinank, Meg Tong, Tomasz Korbak, et al. “Towards Understanding Sycophancy in Language Models.” arXiv.org, October 20, 2023. https://arxiv.org/abs/2310.13548. • Sparkes, Matthew. “AI chatbots become more sycophantic as they get more advanced.” New Scientist, August 17, 2023. https://www.newscientist.com/article/2386915-ai- chatbots-become-more-sycophantic-as-they-get-more-advanced/. • Wei, Jerry, Da Huang, Yifeng Lu, et al. “Simple synthetic data reduces sycophancy in large language models.” arXiv.org, August 7, 2023. https://arxiv.org/abs/2308.03958."></li><li class="carousel-slide" data-ssplayer-slide-index="268" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/269/1739580232671?e=1743033600&amp;v=beta&amp;t=UnH3EDzBCuEu0_4498Lv0MjcK0ZXCvCKVRwdzAk7FNU" alt="269"></li><li class="carousel-slide" data-ssplayer-slide-index="269" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/270/1739580232671?e=1743033600&amp;v=beta&amp;t=NyXttfx2726mWpiD814ST3UurCL33K1FUIENPxNRDa0" alt="Index A accelerate: 81, 89, 118, 134, 252 accuracy: 34, 53, 59, 79-80, 103, 124-125, 141, 143, 147-148, 150-152, 156, 175, 188, 201, 203-204, 223, 230-234, 237-238, 241, 243-244, 248- 249, 251-252, 254, 257, 260-262 ACID: 108 aggregation: 80, 83, 105, 133, 143, 146, 224, 251 AI-generated: 12, 262 AI-powered: 12-13, 16, 37, 118, 167 algorithms: 3, 5, 17, 27-28, 32, 38, 50, 65, 68- 74, 86, 118-119, 151-152, 167, 181, 184, 240, 250 Amazon: 79, 92, 99-100, 134 ANNs: 27, 32, 36, 40-41, 45 Apache: 3, 105, 109, 151 api_key: 189, 195, 198, 220, 225 APIs: 12, 19, 28, 37, 44, 113, 117-118, 132, 134, 137, 140, 149, 154-155, 245 B bucket: 142-144, 158 build: 3, 9, 12, 19, 22, 28, 30, 70, 76, 78-80, 89, 103, 117, 119-121, 126, 139, 141-142, 146-147, 149-150, 153-154, 157-159, 167, 247, 253, 261 architecture: 5, 19-20, 22, 27, 32-33, 40-43, 51, 53-55, 59, 64-65, 72, 75, 89, 102, 140- 141, 150-152, 154, 167, 175, 233 array: 6, 14, 58, 94, 131, 133, 136, 138 assistant: 135, 162, 172-175, 177-179, 182, 184-185, 187-188, 201, 226 Atlas: 2, 20, 48, 60, 62, 64, 76-79, 83, 85, 88- 89, 91-92, 94, 96, 98-103, 105-106, 109-113,118, 120-128, 132, 140-147, 149-155, 157-160, 164-165, 167, 224 authentication: 68, 105, 109, 111, 127 automation: 15, 76, 150 Azure: 19, 134"></li><li class="carousel-slide" data-ssplayer-slide-index="270" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/271/1739580232671?e=1743033600&amp;v=beta&amp;t=c7iOHhNUHQBtNZJM6wkbyXMRC-L-4eYRCZu5azCBy8o" alt="270 Index D database: 2-3, 6, 12-13, 17-18, 20-22, 27, 45, 47, 58-59, 65, 74-75, 78-79, 83, 88, 98-99, 104-106, 109, 111-112, 121-125, 127-128, 131, 133, 143, 211, 224, 239, 246, 255, 260- 261 DataFrame: 129-131, 133, 144, 158 debugging: 23, 119 dependencies: 40, 42, 53-54, 56, 120, 196, 205, 209, 220, 224 deploy: 5, 18, 88, 101, 103, 112, 119, 128, 137, 139, 241 C cache: 89, 101, 164 chatbot: 3, 7, 12, 15, 17, 19-22, 51, 58, 77-78, 135, 150, 157, 166, 171-175, 177-180, 182, 184-185, 187-189, 201, 211, 213, 218, 222, 224, 239, 241, 251, 259 ChatGPT: 7, 30, 79, 180, 232, 234-236, 241, 246, 248 CNNs: 54 collaborate: 86, 136, 152, 179, 252 configuration: 69, 89, 91, 99, 102, 113, 223, 258 consistent: 22, 109, 173, 180, 188, 191, 200, 205, 218, 227, 234-236, 238, 249-251, 259, 261 conversation: 5, 15, 20-21, 27, 51, 135, 157, 161, 163-165, 187, 236, 252 cost-effective: 44, 88 customize: 44, 75, 153 cybersecurity: 259 G GDPR: 241 Gemini: 134, 180 GenAI: 1-9, 75, 80, 117, 119, 121, 124, 126, 128, 131, 134-135, 137, 139-140, 231-248, 251-262 E encrypted: 111, 113, 127 execute: 18, 35, 48, 103, 105, 182, 184, 189, 220, 244 F FastAPI: 5 feature: 2, 7, 17, 19, 47, 53, 55, 57, 74, 84, 93, 112, 127, 133, 140-141, 157-159, 163, 235, 239-240, 242, 254, 257-258 filter: 17, 22, 44-45, 69, 74, 79-81, 86, 95-96, 105-107, 111-112, 123-125, 129-130, 133,146-148, 164-165, 223-224, 229, 254 fine-tuning: 14, 18-19, 44-45, 50, 56, 74-75, 119, 134, 203-204, 208, 210, 220, 230, 237, 241,247 flexibility: 6, 18, 29, 44, 83, 87, 117, 119, 133- 134, 146, 208 foundation: 2, 7, 15, 74, 112-113, 135, 156, 203-204 framework: 3-5, 28, 44, 53, 82-83, 105, 113, 117-120, 128, 133, 137, 139-140, 146, 149, 156-157, 167, 199, 200, 205, 208 function: 4, 11, 17, 23, 33-35, 39, 69, 80, 84, 87, 105-106, 109-110, 112, 121, 124, 137, 139,143, 147, 156, 160, 164, 176, 209, 224-225, 244, 257-258"></li><li class="carousel-slide" data-ssplayer-slide-index="271" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/272/1739580232671?e=1743033600&amp;v=beta&amp;t=QiG31g9bzRXSLRyhCJ8pGQwd0fjIX877GRyAdEi7LbM" alt="Index 271 hallucination: 187, 232, 234, 257, 260 hierarchical: 17, 54, 72-73, 79, 88, 101, 151, 153 identify: 6, 8, 17, 49, 57, 68, 106, 111, 143, 152, 157, 175, 179, 193, 204, 218, 233, 245, 247, 249-250, 259 implementation: 27, 54, 85, 120, 167, 243, 256-257, 262 import: 37, 61, 103-104, 106, 120, 129, 132, 135, 137, 144-146, 149, 154, 158-160, 182, 184,189, 194, 197, 206, 209, 215, 220, 225 index: 32, 38-39, 45, 48, 69-72, 74, 78-79, 81, 83-85, 88-89, 95, 97-98, 101-102, 105, 110,121, 123-125, 136, 146-147, 159, 193, 224, 229 infrastructure: 5, 11, 13, 18-19, 23, 135, 240, 251 generate: 3, 7, 12-18, 22-23, 27, 45, 51-52, 56, 75, 78, 106, 120, 127, 134, 137, 148, 155-157, 165, 173, 176, 180, 196, 199, 219, 233, 243- 245, 248-249, 252-253, 255-258, 262 generative: 1, 3, 5, 7-8, 23, 42, 44, 75, 117, 134, 179, 187, 231-233, 240, 243, 247 governance: 23, 100 GPT-4: 5, 32, 37, 51, 56, 176, 214, 228, 232- 233, 240-241, 252-253, 261 GPT-4o: 44, 154, 176, 189, 191, 193, 196, 220, 224 gpt-4o-mini: 135, 154, 195, 198, 207, 221, 225 GPUs: 18, 36, 240, 261 ingested: 22, 88, 103, 146, 203, 221, 223-224, 227, 229 innovation: 136, 140, 246, 252-253 inquiry: 128, 241-242, 256 integrate: 4, 55, 76, 113, 119, 149 intelligence: 2-3, 11, 49, 92, 135, 200, 252-253 JavaScript: 4, 105, 119 JSON: 53-54, 92, 101, 103, 131, 146, 149, 164-165, 178, 189-191, 213-214, 220-221, 224-226 latency: 69-70, 80, 101, 103, 105, 109, 112, 244-245, 261 leverage: 14, 16, 50-51, 80, 82, 87, 112, 140, 142, 167, 179 lexical: 47, 83-85, 87, 95, 101-102, 110, 112 library: 28, 36-37, 48-49, 61, 117-118, 121, 128-131, 133-137, 139, 153, 193, 215, 258 lifecycle: 23, 91, 108-111, 113, 120, 261 LLMs: 3, 5, 11, 13-19, 23, 27-28, 30-32, 36-39, 42, 44-45, 48, 50-51, 79, 103, 117-118, 120, 150, 154, 156, 167, 171-173, 175-177, 180-181, 187- 188, 192, 199-201, 213, 219, 221, 224, 232, 235- 237, 239, 241, 244, 254, 256 H I L J"></li><li class="carousel-slide" data-ssplayer-slide-index="272" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/273/1739580232671?e=1743033600&amp;v=beta&amp;t=YlADIZBgP5rTTHEiE4hrsl9-bCjnw4dRsMktnzzcfEk" alt="272 Index M malicious: 259 mapping: 32, 36-38, 50, 77, 82- 83, 132, 159 Markdown: 6, 153, 215, 218, 227- 229 message: 20-21, 105, 108, 135, 155, 162, 165, 174, 191, 221, 226, 243 metadata: 17, 63-64, 74, 79-80, 87, 89, 93, 122, 125, 128, 149, 151, 173, 178-179, 203, 208, 210-224, 227, 229-230 misinformation: 143, 234, 236, 253 modeling: 7, 17, 27-28, 32, 40, 42, 78-79, 81, 83, 91-92, 112-113 MTEB: 59-60, 204-205 O Okta: 76-77 OpenAI: 2, 5, 9, 19-20, 36, 44, 48, 51, 55, 60, 64, 92, 105-106, 110, 120, 124, 127, 134-135,142, 144-145, 147, 149, 154, 157, 159-160, 164, 167, 172, 189, 191, 193, 196, 204-206, 214, 220,224-225, 240, 252-253, 258 N nested: 54, 112, 133, 136 network: 5, 23, 27, 32-36, 38-39, 41-42, 45, 50, 52-55, 57-58, 69-70, 72-73, 109, 244-245, 261 NLKT: 153, 220 NoSQL: 3, 79 NumPy: 5, 117, 128, 131, 133, 142, 145 R RBAC: 91-92, 111-113 real-time: 15, 56, 59, 105, 108, 111, 134, 233, 240, 245, 248, 258 real-world: 66, 76, 89, 113, 141, 175, 233, 236, 239-240, 249, 251, 259-260 recommendation: 12, 17, 47, 49, 54, 86, 157-158, 161, 163-165, 167, 189-192, 204, 249 redundant: 232-233, 254 reliability: 14, 19, 109, 171-172, 175, 234, 248, 257 P privacy: 23, 111, 171, 241 prompt: 3, 14, 16, 19, 21, 23, 44-45, 79, 81, 112, 120, 126-127, 138, 151, 154-156, 172, 188,190-191, 201, 221, 232, 241, 243-244, 252-253, 259- 260, 262 pydantic: 220, 224-225 PyMongoArrow: 128, 131-133 S scalability: 5,13, 19, 22-23, 76, 79, 88, 91, 105, 112-113, 119, 135, 258 schema: 92-94, 96, 105, 110, 132-133, 153, 167 security: 19, 23, 44, 76, 90-92, 111-113, 124-125, 127-128, 171-172, 179, 241, 259, 262 operation: 52, 66-67, 69, 74-75, 87, 104-105, 107, 109, 113, 128, 133, 150, 244, 246, 252 optimization: 19, 22-23, 80, 208, 223, 230, 232, 240, 242, 261-262"></li><li class="carousel-slide" data-ssplayer-slide-index="273" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/274/1739580232671?e=1743033600&amp;v=beta&amp;t=T5bsDW_OUDZN8QIDpBm1LaVcAl_MP2iEmrcesu28aTc" alt="Index 273 semantic: 11, 13, 16-17, 22-23, 38, 47, 50-54, 56, 58-60, 62-68, 74, 76-77, 80-83, 89, 93, 95, 99, 101, 110, 112, 119-120, 124-126, 143, 146, 149, 151-152, 201, 203-204, 210, 212, 215, 217-218, 223-224, 227, 229-230, 243 sequential: 36, 40-42, 43, 45, 54 strategies: 44-45, 70, 81-82, 108, 110-113, 151-153, 178, 180, 199, 221, 223, 240, 244- 245, 248, 255, 261-262 structured: 15, 47, 60, 64, 66, 74-75, 79, 92, 128, 131, 152-153, 155, 180, 188, 213, 222-223,244 summarization: 5, 14, 23, 27, 51-52, 131, 135- 136, 152, 184, 187 syntax: 4, 52, 105, 118, 131, 218, 235-236, 239, 252-253 update: 14, 23, 35, 38-39, 57, 94, 96-97, 103-104, 107-108, 110, 223, 249-250, 253, 255 upgrade: 110, 261 target: 14, 34-35, 42, 45, 72, 152, 162 technique: 14, 18, 22-23, 30-31, 44-45, 47, 50, 60, 75, 82-83, 151, 153-154, 157, 192, 203, 218,220, 223-224, 235, 244, 252, 256, 262 terminal: 135-136, 182-184, 187, 189, 192-193, 195-196, 199, 205, 207, 209-210, 215, 217,220-221, 224, 227 time-indexed: 129 time-series: 239 transparency: 44, 59, 60, 110 YAML: 213-214 yields: 207 XML-based: 227 zero-downtime: 89 zstd: 99 WiredTiger: 99, 101 workflow: 16, 58-59, 77, 79, 118, 150-152, 256-257 writeConcern: 109 validate: 79, 179, 233, 240, 258 vector: 2-3, 6, 9, 11, 13, 16-23, 37-39, 45, 48, 50, 52-55, 57-60, 62, 64-81, 83-85, 87-89, 92-99, 101-102, 104-105, 108-112, 117, 121, 123- 127, 136, 140-143, 146-151, 154-157, 159-160,164-165, 167, 173, 192, 196, 203-204, 206-207, 210, 212, 221-224, 227, 229-230, 254-255 vulnerability: 234 U T W V X Y Z"></li><li class="carousel-slide" data-ssplayer-slide-index="274" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/275/1739580232671?e=1743033600&amp;v=beta&amp;t=VLENF5WeFYd754J08_Nj2-2cr18vd-jIVFY0hI4mBWo" alt="275"></li><li class="carousel-slide" data-ssplayer-slide-index="275" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/276/1739580232671?e=1743033600&amp;v=beta&amp;t=NYLlIVqeqck-TL_ZCHE39eU_ULI-6CHOXvM3ty9TTxU" alt="www.packtpub.com Subscribe to our online digital library for full access to over 7,000 books and videos, as well as industry leading tools to help you plan your personal development and advance your career. For more information, please visit our website. Why subscribe? • Spend less time learning and more time coding with practical eBooks and Videos from over 4,000 industry professionals • Improve your learning with Skill Plans built especially for you • Get a free eBook or video every month • Fully searchable for easy access to vital information • Copy and paste, print, and bookmark content Did you know that Packt offers eBook versions of every book published, with PDF and ePub files available? You can upgrade to the eBook version at packtpub.com and as a print book customer, you are entitled to a discount on the eBook copy. Get in touch with us at customercare@packtpub.com for more details. At www.packtpub.com, you can also read a collection of free technical articles, sign up for a range of free newsletters, and receive exclusive discounts and offers on Packt books and eBooks."></li><li class="carousel-slide" data-ssplayer-slide-index="276" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/277/1739580232671?e=1743033600&amp;v=beta&amp;t=dctpN40HB_IvvspneaFZznxlwUlTOHmZ9pKf-PeEZRY" alt="Other Books You May Enjoy If you enjoyed this book, you may be interested in these other books by Packt: Full Stack FastAPI, React, and MongoDB Marko Aleksendrić, Ph.D., Shrey Batra, Rachelle Palmer, Shubham Ranjan ISBN: 978-1-83588-676-2 • Model data and create collections for efficient data handling. • Utilize pure Python to build powerful backend solutions. • Use React and Next.js 14 to develop and launch production-ready applications. • Integrate Large Language Models, handle email functionalities, and more."></li><li class="carousel-slide" data-ssplayer-slide-index="277" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/278/1739580232671?e=1743033600&amp;v=beta&amp;t=4PPtwOEp9MsMmGJTAaTaL-opEzvtsGQdPs0i2ADncdw" alt="277 Mastering MongoDB 7.0 Marko Aleksendrić, Ph.D., Arek Borucki, Leandro Domingues, Malak Abu Hammad, Elie Hannouch, Rajesh Nair, Rachelle Palmer ISBN: 978-1-83546-047-4 • Execute complex queries for deep insights • Transform data with aggregation pipelines • Guarantee data integrity with ACID transactions • Optimize query speed with strategic indexing • Manage and monitor with MongoDB Atlas • Empower search with Atlas Search • Safeguard data with RBAC, user management, and encryption • Maintain transparency with auditing practices"></li><li class="carousel-slide" data-ssplayer-slide-index="278" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/279/1739580232671?e=1743033600&amp;v=beta&amp;t=omb2t8m_a-Q42cLBP5Ftoya7Me0nMUbycoS64UbEzaw" alt="278 Other Books You May Enjoy Practical MongoDB Aggregations Paul Done ISBN: 978-1-83588-436-2 • Build adaptable pipelines for evolving business needs • Optimize pipelines for lightning-fast data processing • Maximize efficiency with sharding for large datasets • Process data directly in the database for performance gains • Enhance data security through pipeline-based access control"></li><li class="carousel-slide" data-ssplayer-slide-index="279" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/280/1739580232671?e=1743033600&amp;v=beta&amp;t=nk3YoEoPBM4MrnqK1cMlIE8WqthimKOgAiAevigkysw" alt="Packt is searching for authors like you 279 Packt is searching for authors like you If you’re interested in becoming an author for Packt, please visit authors.packtpub.com and apply today. We have worked with thousands of developers and tech professionals, just like you, to help them share their insight with the global tech community. You can make a general application, apply for a specific hot topic that we are recruiting an author for, or submit your own idea. Download a free PDF copy of this book Thanks for purchasing this book! Do you like to read on the go but are unable to carry your print books everywhere? Is your eBook purchase not compatible with the device of your choice? Don’t worry, now with every Packt book you get a DRM-free PDF version of that book at no cost. Read anywhere, any place, on any device. Search, copy, and paste code from your favorite technical books directly into your application. The perks don’t stop there, you can get exclusive access to discounts, newsletters, and great free content in your inbox daily Follow these simple steps to get the benefits: 2. 3. Submit your proof of purchase That’s it! We’ll send your free PDF and other benefits to your email directly 1. Scan the QR code or visit the link below https://packt.link/free-ebook/9781836207252"></li><li class="carousel-slide" data-ssplayer-slide-index="280" tabindex="-1" aria-hidden="true" style="width: 556px; height: 684px;"><img class="carousel-lazy-element carousel-slide-is-portrait" data-src="https://media.licdn.com/dms/image/v2/D561FAQHeShGubujyxg/feedshare-document-images_1280/B56ZUHDvNDGoA4-/281/1739580232671?e=1743033600&amp;v=beta&amp;t=9l4k0TYCx9UOuRYs0bmZgpryKzFKq48n8QUsW_B283o" alt="281"></li></ul></div>
